{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the last notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, torch, math, numpy as np, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load the mnist training images and labels**<br>\n",
    "This data is downloaded in notebook 001a_nn_basics so make sure you have run through that first\n",
    "We print out the min and max of the features to get a feel for the range of feature values.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(0.9961))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'mnist'\n",
    "\n",
    "with gzip.open(PATH/'mnist.pkl.gz', 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "\n",
    "x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\n",
    "x_train.min(),x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** now set the batch size, number of epochs and learning rate..**\n",
    "\n",
    "handy reference for abbreviations:<br>\n",
    "https://github.com/fastai/fastai_v1/blob/master/docs/abbr.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "epochs = 2\n",
    "lr=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load the training and validation datasets**\n",
    "\n",
    "TensorDataset is a pytorch utility class that implements the pytorch Dataset api for a given list of tensors<br>\n",
    "<br>\n",
    "The Dataset api allows iteration, indexing and slicing along the first dimension of each tensor passed in.\n",
    "In this case we are passing in two tensors: training features and classification targets.<br>\n",
    "so each iteration of of the TensorDataset will return a tuple of length two that looks like (x_features, y_target)\n",
    "\n",
    "In case you aren't already familiar with the pytorch Dataset class, this tutorial talks more about the Dataset abstract class, how to use it, and how implement custom datasets for new kinds of data:<br>\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl: loss_batch(model, xb, yb, loss_fn, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify nn.Sequential layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function composition** is a great way to capture and parameterize common operations as a single concept.<br> \n",
    "You can see this with the *PoolFlatten* function below.<br>\n",
    "The *Lambda* layer we created in the last notebook wakes it easy to quickly create pytorch layers for the same purpose.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResizeBatch(*size): return Lambda(lambda x: x.view((-1,)+size))\n",
    "def Flatten(): return Lambda(lambda x: x.view((x.size(0), -1)))\n",
    "def PoolFlatten(): return nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define the model**<br>\n",
    "Thanks to our named pytorch nn.Modules above, the meaning and intention of each of the layers in our model is clearer and less prone to error when we make changes<br>\n",
    "This is an example a stripped down CNN. \n",
    "You will nearly always use a kernel size of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    ResizeBatch(1,28,28),\n",
    "    nn.Conv2d(1,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    PoolFlatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a *get_data* function**<br>\n",
    "It's often conventient in your notebook to define a *get_data* function that encapsulates the work of setting up the training, validation and sometimes test data.<br>\n",
    "Parameterizing *get_data* makes it easy to do things like change the batch size, etc.<br>\n",
    "<br>\n",
    "Notice in in this scenario that we shuffle the training dataloader but not the validation dataloader. We want the validation loss to be calculated the same way every time so that we can tell if we are still learning and not overfitting.<br>\n",
    "Shuffling the training data helps prevent overfitting when calculating the gradients to be applied after each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "            DataLoader(valid_ds, batch_size=bs*2))\n",
    "\n",
    "train_dl,valid_dl = get_data(train_ds, valid_ds, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use cross entropy for our loss function**<br>\n",
    "We are doing classification so cross_entropy is the correct loss function.<br>\n",
    "Here is tutorial explaining why we use cross entropy for classification tasks:<br>\n",
    "https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the optimizer**<br>\n",
    "We stick with stochastic gradient descent as our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test our loss function**<br>\n",
    "Try out our loss function on one batch of X features and y targets to make sure it's working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3003, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(x_valid[0:bs]), y_valid[0:bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit**<br>\n",
    "everything looks ready, lets call the fit function we developed earlier for two epochs and confirm we are learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0272689867973328\n",
      "1 0.8396794363021851\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations; refactor network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to refactor some of the data transformations out of the network and into a pipeline that is applied to the data coming out of the Dataloaders.<br>\n",
    "This is more flexible, simplifies the model, and will be useful later when we want to apply additional transformations for things like data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a transformation**\n",
    "*mnist2image* is a utility function to reshape our features into 28x28 arrays<br>\n",
    "The pytorch Dataloaders are iterables that on each iteration return a tuple like (X, y)<br>\n",
    "<br>\n",
    "X is a batch of features where the first dimension is the number of samples in the batch and the remaining dimensions define the shape of the features.\n",
    "y is the target variable to be learned - in this case it an integer representing one of 10 image classes\n",
    "\n",
    "With mnist data, the X features start out as a 1x784 vector. We want to conver the features to 1x28x28 images. This helper function does that for an entire batch work of features. It passes the target variable through as is\n",
    "\n",
    "(need to finish writing this / pick up here when back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist2image(b): return b[0].view(-1,1,28,28), b[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Iterable\n",
    "from functools import reduce\n",
    "\n",
    "def is_listy(x): return isinstance(x, (list,tuple))\n",
    "\n",
    "def listify(p=None, q=None):\n",
    "    if p is None: p=[]\n",
    "    elif not isinstance(p, Iterable): p=[p]\n",
    "    n = q if type(q)==int else 1 if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    return p\n",
    "\n",
    "def compose(funcs):\n",
    "    return reduce(lambda f, g: lambda z: f(g(z)), listify(funcs), lambda o: o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterPipe():\n",
    "    def __init__(self, iterator, funcs): self.iter,self.func = iterator,compose(funcs)\n",
    "    def __len__(self): return len(self.iter)\n",
    "    def __iter__(self): return map(self.func, self.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dl(ds, bs, shuffle, tfms=None):\n",
    "    return IterPipe(DataLoader(ds, batch_size=bs, shuffle=shuffle), tfms)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs, train_tfms=None, valid_tfms=None):\n",
    "    return (get_dl(train_ds, bs, shuffle=True, tfms=train_tfms),\n",
    "            get_dl(valid_ds, bs*2, shuffle=False, tfms=valid_tfms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl,valid_dl = get_data(train_ds, valid_ds, bs, mnist2image, mnist2image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([1, 28, 28]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds[0][0].shape, x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(valid_ds[0][0], x[0].view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2_relu(nif, nof, ks, stride):\n",
    "    return nn.Sequential(nn.Conv2d(nif, nof, ks, stride, padding=ks//2), nn.ReLU())\n",
    "\n",
    "def simple_cnn(actns, kernel_szs, strides):\n",
    "    layers = [conv2_relu(actns[i], actns[i+1], kernel_szs[i], stride=strides[i])\n",
    "        for i in range(len(strides))]\n",
    "    layers.append(PoolFlatten())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9104695863723755\n",
      "1 0.7454537752151489\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle non-lists (e.g. single tensor)\n",
    "def to_device(device, b): return [o.to(device) for o in b]\n",
    "\n",
    "default_device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "tfms = [partial(to_device, default_device), mnist2image]\n",
    "train_dl,valid_dl = get_data(train_ds, valid_ds, bs, tfms, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2]).to(default_device)\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7942686936378479\n",
      "1 0.48481653938293456\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_fn, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook, trange, tnrange\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        it = tqdm_notebook(train_dl, leave=False)\n",
    "        for xb,yb in it:\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            it.set_postfix_str(loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, train_ds, valid_ds, bs=64, device=None, train_tfms=None, valid_tfms=None):\n",
    "        self.device = default_device if device is None else device\n",
    "        dev_tfm = [partial(to_device, self.device)]\n",
    "        self.train_dl = get_dl(train_ds, bs,   shuffle=True,  tfms=dev_tfm + listify(train_tfms))\n",
    "        self.valid_dl = get_dl(valid_ds, bs*2, shuffle=False, tfms=dev_tfm + listify(valid_tfms))\n",
    "\n",
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD):\n",
    "        opt = opt_fn(self.model.parameters(), lr=lr)\n",
    "        loss_fn = F.cross_entropy\n",
    "        fit(epochs, self.model, loss_fn, opt, self.data.train_dl, self.data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs, train_tfms=mnist2image, valid_tfms=mnist2image)\n",
    "model = simple_cnn([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learner = Learner(data, model)\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811b3df0efa441c78091ed0fea83d082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.457903608417511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.33018289823532104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.29600199661254883\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.2516591844558716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(4, lr/5, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(data, simple_cnn([1,16,16,10], [3,3,3], [2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87532689eeeb4aae99b623808e02f58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4901185554504395\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b7cdf2c6eb4d2081e8184de51fd591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2518716604232788\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.21359226384162902\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aaf993555245d98d39b388c4a8dc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=782), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14774560313224794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learner.fit(1, lr/5, opt_fn=opt_fn)\n",
    "learner.fit(2, lr, opt_fn=opt_fn)\n",
    "learner.fit(1, lr/5, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
