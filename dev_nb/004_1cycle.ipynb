{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nb_002b import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "PATH = DATA_PATH/'cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm = normalize_tfm(mean=data_mean,std=data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5),\n",
    "        pad_tfm(padding=4),\n",
    "        crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset(PATH/'train')\n",
    "valid_ds = FilesDataset(PATH/'test')\n",
    "data = DataBunch(train_ds, valid_ds, bs=bs, train_tfm=tfms+[cifar_norm], valid_tfm=[cifar_norm], num_workers=0)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl):\n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            if train_dl.progress_func is not None: train_dl.gen.set_postfix_str(loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD):\n",
    "        opt = opt_fn(self.model.parameters(), lr=lr)\n",
    "        loss_fn = F.cross_entropy\n",
    "        fit(epochs, self.model, loss_fn, opt, self.data.train_dl, self.data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_layer(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_layer(ni//2, ni, ks=3)\n",
    "        \n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def make_group_layer(self, ch_in, num_blocks, stride=1):\n",
    "        return [conv_layer(ch_in, ch_in*2,stride=stride)\n",
    "               ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes, nf=32):\n",
    "        super().__init__()\n",
    "        layers = [conv_layer(3, nf, ks=3, stride=1)]\n",
    "        for i,nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, num_classes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want an optimizer with an easy way to set hyperparameters: they're all properties and we define custom setters to handle the different names in pytorch optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOptimizer():\n",
    "    \n",
    "    def __init__(self, params, opt_fn, init_lr):\n",
    "        self.opt = opt_fn(params, init_lr)\n",
    "        self._lr = init_lr\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "    \n",
    "    #Pytorch optimizer methods\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "    \n",
    "    #Hyperparameters as properties\n",
    "    @property\n",
    "    def lr(self): return self._lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, val):\n",
    "        self.set_val('lr', val)\n",
    "        self._lr = val\n",
    "    \n",
    "    @property\n",
    "    def mom(self): return self._mom\n",
    "\n",
    "    @mom.setter\n",
    "    def mom(self, val):\n",
    "        if 'momentum' in self.opt_keys: self.set_val('momentum', val)\n",
    "        elif 'betas' in self.opt_keys:  self.set_val('betas', (val, self._beta))\n",
    "        self._mom = val\n",
    "    \n",
    "    @property\n",
    "    def beta(self): return self._beta\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val):\n",
    "        if 'betas' in self.opt_keys:    self.set_val('betas', (self._mom,val))\n",
    "        elif 'alpha' in self.opt_keys:  self.set_val('alpha', val)\n",
    "        self._beta = val\n",
    "    \n",
    "    @property\n",
    "    def wd(self): return self._wd\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val):\n",
    "        self.set_val('weight_decay', val)\n",
    "        self._wd = val\n",
    "    \n",
    "    #Helper functions\n",
    "    def read_defaults(self):\n",
    "        if 'momentum' in self.opt_keys: self._mom = self.opt.param_groups[0]['momentum']\n",
    "        if 'alpha' in self.opt_keys: self._beta = self.opt.param_groups[0]['alpha']\n",
    "        if 'betas' in self.opt_keys: self._mom,self._beta = self.opt.param_groups[0]['betas']\n",
    "        if 'weight_decay' in self.opt_keys: self._wd = self.opt.param_groups[0]['weight_decay']\n",
    "    \n",
    "    def set_val(self, key, val):\n",
    "        for pg in self.opt.param_groups: pg[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = HPOptimizer(model.parameters(), opt_fn, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr, opt.mom, opt.wd, opt.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr, opt.mom, opt.wd, opt.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's easy to set and change the HP in the optimizer, we need a scheduler to change it. To keep the training loop as readable as possible we don't want to handle all of this stuff inside it so we'll use callbacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def on_train_begin(self): pass         \n",
    "        #To initiliaze constants in the callback.\n",
    "    def on_batch_begin(self, xb, yb): return xb, yb \n",
    "        #To set HP before the step is done. A look at the input can be useful (set the lr depending on the seq_len in RNNs, \n",
    "        #or for reg_functions called in on_backward_begin)\n",
    "        #Returns xb, yb (which can allow us to modify the input at that step if needed)\n",
    "    def on_backward_begin(self, loss): return loss\n",
    "        #Called after the forward pass and the loss has been computed, but before the back propagation.\n",
    "        #Returns the loss (which can allow us to modify it, for instance for reg functions)\n",
    "    def on_backward_end(self): pass\n",
    "        #Called after the back propagation had been done (and the gradients computed) but before the step of the optimizer.\n",
    "        #Useful for true weight decay in AdamW\n",
    "    def on_step_end(self): pass\n",
    "        #Called after the step of the optimizer but before the gradients are zeroed (not sure this one is useful)\n",
    "    def on_batch_end(self, loss): pass\n",
    "        #Called at the end of the batch. Can stop the current epoch by returning True\n",
    "    def on_epoch_end(self, val_loss): pass\n",
    "        #Called at the end of an epoch. Can stop the training by returning True.\n",
    "\n",
    "#on_epoch_begin isn't really useful since we have on_train_begin for epoch 0 and on_epoch_end for the others\n",
    "#does on_train_end make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to have a callback between every line of the training loop, that way everything we need to add will be treated there and not inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None, callbacks=[]):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        for cb in callbacks: loss = cb.on_backward_begin(loss)\n",
    "        loss.backward()\n",
    "        for cb in callbacks: cb.on_backward_end()\n",
    "        opt.step()\n",
    "        for cb in callbacks: cb.on_step_end()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl=None,callbacks=[]):\n",
    "    for cb in callbacks: cb.on_train_begin()\n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            for cb in callbacks: xb, yb = cb.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            if train_dl.progress_func is not None: train_dl.gen.set_postfix_str(loss)\n",
    "            stop = False\n",
    "            for cb in callbacks: stop = stop or cb.on_batch_end(loss)\n",
    "            if stop: break\n",
    "        \n",
    "        if valid_dl is not None:    \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                    for xb,yb in valid_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            print(epoch, val_loss)\n",
    "        else: val_loss = None\n",
    "            \n",
    "        stop = False\n",
    "        for cb in callbacks: stop = stop or cb.on_epoch_end(val_loss)\n",
    "        if stop: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a 1cycle scheduler pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealing_no(start, end, pct): return start\n",
    "def annealing_linear(start, end, pct): return start + pct * (end-start)\n",
    "def annealing_exponential(start, end, pct): return start * (end / start) ** pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stepper():\n",
    "    \n",
    "    def __init__(self, start, end, num_it, ft):\n",
    "        self.start,self.end,self.num_it,self.ft = start,end,num_it,ft\n",
    "        self.n = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        return self.ft(self.start, self.end, self.n/self.num_it)\n",
    "    \n",
    "    def is_done(self):\n",
    "        return self.n >= self.num_it\n",
    "    \n",
    "    def init_val(self):\n",
    "        return self.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(Callback):\n",
    "    \n",
    "    def __init__(self, learn, lr_max, epochs, moms=(0.95,0.85), div_factor=10, pct_end=0.1):\n",
    "        self.learn = learn\n",
    "        a = int(len(learn.data.train_dl) * epochs * (1 - pct_end) / 2)\n",
    "        b = int(len(learn.data.train_dl) * epochs * pct_end)\n",
    "        self.lr_scheds = [Stepper(lr_max/div_factor, lr_max, a, annealing_linear),\n",
    "                          Stepper(lr_max, lr_max/div_factor, a, annealing_linear),\n",
    "                          Stepper(lr_max/div_factor, lr_max/(div_factor*100), b, annealing_linear)]\n",
    "        self.mom_scheds = [Stepper(moms[0], moms[1], a, annealing_linear),\n",
    "                          Stepper(moms[1], moms[0], a, annealing_linear),\n",
    "                          Stepper(moms[0], None, b, annealing_no)]\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr, self.opt.mom = self.lr_scheds[0].init_val(), self.mom_scheds[0].init_val()\n",
    "        self.idx_s = 0\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        self.opt.lr = self.lr_scheds[self.idx_s].step()\n",
    "        self.opt.mom = self.mom_scheds[self.idx_s].step()\n",
    "        if self.lr_scheds[self.idx_s].is_done():\n",
    "            self.idx_s += 1\n",
    "            if self.idx_s >= len(self.lr_scheds): return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD, callbacks = []):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), opt_fn, lr)\n",
    "        loss_fn = F.cross_entropy\n",
    "        fit(epochs, self.model, loss_fn, self.opt, self.data.train_dl, self.data.valid_dl, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model)\n",
    "sched = OneCycleScheduler(learn, 0.1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(5,0.1,callbacks=[sched])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LRFinder we need two different callbacks: one will keep track of the losses and lrs, the other will schedule the LR exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    beta = 0.98\n",
    "    \n",
    "    def __init__(self, learn):\n",
    "        self.learn = learn\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.n,self.avg_loss,self.losses,self.lrs = 0,0.,[],[]\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        self.n += 1\n",
    "        self.avg_loss =  self.beta * self.avg_loss + (1-self.beta) * loss\n",
    "        self.losses.append(self.avg_loss / (1-self.beta ** self.n))\n",
    "        self.lrs.append(self.opt.lr)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        lrs = self.lrs[skip_start:-skip_end] if skip_end != 0 else self.lrs[skip_start:]\n",
    "        losses = self.losses[skip_start:-skip_end] if skip_end != 0 else self.losses[skip_start:]\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder(Callback):\n",
    "    \n",
    "    def __init__(self, learn, num_it=200, start_lr=1e-5, end_lr=10):\n",
    "        self.learn = learn\n",
    "        self.lr_sched = Stepper(start_lr, end_lr, num_it, annealing_exponential)\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr = self.lr_sched.init_val()\n",
    "        self.first, self.best_loss = True, 0\n",
    "    \n",
    "    def on_batch_end(self, loss):\n",
    "        if self.first or self.best_loss > loss:\n",
    "            self.first, self.best_loss = False, loss\n",
    "        self.opt.lr = self.lr_sched.step()\n",
    "        if self.lr_sched.is_done() or loss > 4 * self.best_loss:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.opt_fn, self.loss_fn = optim.SGD, F.cross_entropy\n",
    "\n",
    "    def fit(self, epochs, lr, callbacks = []):\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, lr)\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data.train_dl, self.data.valid_dl, callbacks)\n",
    "    \n",
    "    def lr_find(self, num_it=200, start_lr=1e-5, end_lr=10):\n",
    "        scheds = [Recorder(self), LRFinder(self, num_it, start_lr, end_lr)]\n",
    "        self.recorder = scheds[0]\n",
    "        epochs = int(np.ceil(num_it/len(self.data.train_dl)))\n",
    "        self.opt = HPOptimizer(self.model.parameters(), self.opt_fn, start_lr)\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data.train_dl, callbacks=scheds)\n",
    "        \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        self.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=16)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find(end_lr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=16)\n",
    "learn = Learner(data, model)\n",
    "sched = OneCycleScheduler(learn, 1e-2, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(30,1e-2,callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(mode, dl):\n",
    "    match, count = 0, 0\n",
    "    with torch.no_grad(): \n",
    "        for x,y in dl:\n",
    "            z = model(x)\n",
    "            preds = z.argmax(1)\n",
    "            count += y.size(0)\n",
    "            match += (preds==y).long().sum().item()\n",
    "    return match/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(learn.model, learn.data.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrueWD(Callback):\n",
    "    \n",
    "    def __init__(self, learn, wd):\n",
    "        self.learn,self.wd = learn,wd\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.wd = 0.\n",
    "    \n",
    "    def on_backward_end(self):\n",
    "        for pg in self.opt.param_groups():\n",
    "            lr = pg['lr']\n",
    "            for p in pg['params']:\n",
    "                p.data.mul_(1 - lr * self.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=16)\n",
    "learn = Learner(data, model)\n",
    "scheds = [OneCycleScheduler(learn, 1e-2, 30), TrueWD(learn,0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(30,1e-2,callbacks=scheds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(learn.model, learn.data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
