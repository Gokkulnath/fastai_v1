{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.conv_learner import *\n",
    "from fastai.models.cifar10.wideresnet import wrn_22\n",
    "torch.backends.cudnn.benchmark = True\n",
    "PATH = Path(\"../data/cifar10/\")\n",
    "os.makedirs(PATH,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n",
    "\n",
    "bs=128\n",
    "sz=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 32, aug_tfms=[RandomCrop(32), RandomFlip()], pad=4)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84482bccf52a47c0aba248b0526dedfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.070115   1.196505   0.5752    \n",
      "    1      0.825004   0.973065   0.6554                      \n",
      "    2      0.653242   0.771395   0.7448                      \n",
      "    3      0.584217   0.681873   0.7707                      \n",
      "    4      0.532069   0.591126   0.8027                      \n",
      "    5      0.489163   0.569924   0.8081                      \n",
      "    6      0.45033    0.489344   0.8385                      \n",
      "    7      0.418057   0.505793   0.8317                      \n",
      "    8      0.394136   0.514943   0.8263                      \n",
      "    9      0.390115   0.622177   0.7885                      \n",
      "    10     0.384691   0.473768   0.8427                      \n",
      "    11     0.356405   0.432304   0.8512                      \n",
      "    12     0.344455   0.414629   0.8612                      \n",
      "    13     0.335668   0.428897   0.8523                      \n",
      "    14     0.302873   0.376021   0.8756                      \n",
      "    15     0.284885   0.366886   0.8773                      \n",
      "    16     0.254075   0.41661    0.8638                      \n",
      "    17     0.234941   0.362723   0.8808                      \n",
      "    18     0.20991    0.32523    0.8927                      \n",
      "    19     0.190377   0.296267   0.9038                      \n",
      "    20     0.174052   0.309467   0.8989                      \n",
      "    21     0.147387   0.294149   0.9068                      \n",
      "    22     0.133616   0.248515   0.9203                      \n",
      "    23     0.10681    0.285099   0.9125                      \n",
      "    24     0.089988   0.248651   0.9253                       \n",
      "    25     0.062843   0.227796   0.9304                       \n",
      "    26     0.045598   0.224104   0.9364                       \n",
      "    27     0.029226   0.226274   0.9376                       \n",
      "    28     0.019049   0.214894   0.9423                       \n",
      "    29     0.014146   0.207299   0.9451                       \n",
      "\n",
      "CPU times: user 24min 22s, sys: 13min 27s, total: 37min 49s\n",
      "Wall time: 22min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20729929401874542, 0.9451]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard DawnBench result with one GPU: 94% accuracy in 22min47s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline + openCV (like in old fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(folder):\n",
    "    classes = [d for d in folder.iterdir()\n",
    "               if d.is_dir() and not d.name.startswith('.')]\n",
    "    classes.sort(key=lambda d: d.name)\n",
    "    return classes\n",
    "\n",
    "def get_image_files(c):\n",
    "    return [o for o in list(c.iterdir())\n",
    "            if not o.name.startswith('.') and not o.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as Dataset1\n",
    "\n",
    "class FilesDataset1(Dataset1):#Renamed to avoid conflict with fastai FilesDataset\n",
    "    def __init__(self, folder, tfms):\n",
    "        cls_dirs = find_classes(folder)\n",
    "        self.fns, self.y = [], []\n",
    "        self.classes = [cls.name for cls in cls_dirs]\n",
    "        for i, cls_dir in enumerate(cls_dirs):\n",
    "            fnames = get_image_files(cls_dir)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = open_image(self.fns[i])\n",
    "        for tfm in self.tfms: x,_ = tfm(x, None) \n",
    "        return x,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device, stats):\n",
    "        self.dl,self.device = dl,device\n",
    "        self.m, self.s = map(lambda x:torch.tensor(x, dtype=torch.float32, device=device), stats)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            x, y = b[0].to(self.device),b[1].to(self.device)\n",
    "            x = (x - self.m[None,:,None,None]) / self.s[None,:,None,None]\n",
    "            yield x,y\n",
    "    \n",
    "    def __len__(self): return (len(self.dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the DataLoader from pytorch since fastai replaced the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader as DataLoader1\n",
    "def get_dataloader(ds, bs, shuffle, device, stats):\n",
    "    return DeviceDataLoader(DataLoader1(ds, batch_size=bs, shuffle=shuffle,num_workers=8), device, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, trn_ds, val_ds, stats, bs=64, device=None):\n",
    "        self.device = default_device if device is None else device\n",
    "        if hasattr(trn_ds, 'classes'): self.classes = trn_ds.classes\n",
    "        self.trn_dl = get_dataloader(trn_ds, bs,   shuffle=True,  device=self.device, stats=stats)\n",
    "        self.val_dl = get_dataloader(val_ds, bs*2, shuffle=False, device=self.device, stats=stats)\n",
    "\n",
    "    @classmethod\n",
    "    def from_files(cls, Path, trn_tfms, val_tfms, stats, trn_name='train', val_name='valid', bs=64, device=None):\n",
    "        trn_ds, val_ds = FilesDataset1(Path/trn_name, trn_tfms), FilesDataset1(Path/val_name, val_tfms)\n",
    "        return cls(trn_ds, val_ds, stats, bs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 32, aug_tfms=[RandomCrop(32), RandomFlip()], pad=4)\n",
    "tfms[0].tfms.pop(-2)\n",
    "tfms[1].tfms.pop(-2)\n",
    "data = DataBunch.from_files(PATH, tfms[0].tfms, tfms[1].tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms1 = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms1, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba556aa0e4c4e6b97c467cfe5532011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.095266   1.14763    0.5863    \n",
      "    1      0.824504   0.937468   0.6746                      \n",
      "    2      0.698435   0.730168   0.7521                      \n",
      "    3      0.609296   0.611863   0.7971                      \n",
      "    4      0.532544   0.606716   0.7964                      \n",
      "    5      0.493457   0.503308   0.83                        \n",
      "    6      0.455336   0.611184   0.8036                      \n",
      "    7      0.419502   0.50613    0.8269                      \n",
      "    8      0.407886   0.544323   0.8193                      \n",
      "    9      0.383891   0.484687   0.8349                      \n",
      "    10     0.37561    0.524561   0.8323                      \n",
      "    11     0.346088   0.507868   0.8352                      \n",
      "    12     0.343822   0.496445   0.8368                      \n",
      "    13     0.335781   0.614847   0.8121                      \n",
      "    14     0.307608   0.473802   0.8476                      \n",
      "    15     0.278937   0.445718   0.853                       \n",
      "    16     0.259902   0.351999   0.8826                      \n",
      "    17     0.235303   0.380551   0.8777                      \n",
      "    18     0.209758   0.295435   0.9006                      \n",
      "    19     0.185055   0.327301   0.8937                      \n",
      "    20     0.172416   0.325397   0.8965                      \n",
      "    21     0.13705    0.30302    0.904                       \n",
      "    22     0.126511   0.287199   0.9101                      \n",
      "    23     0.105721   0.255392   0.9232                       \n",
      "    24     0.08476    0.25194    0.9259                       \n",
      "    25     0.062034   0.262352   0.9262                       \n",
      "    26     0.042956   0.227027   0.9378                       \n",
      "    27     0.02793    0.230478   0.9396                       \n",
      "    28     0.017105   0.221831   0.9436                       \n",
      "    29     0.013412   0.222246   0.9447                       \n",
      "\n",
      "CPU times: user 10min 1s, sys: 3min 45s, total: 13min 47s\n",
      "Wall time: 13min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22224570198059082, 0.9447]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataloader in pytorch is fast! It only takes 13min47s to reach the 94%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline + torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as Dataset1\n",
    "\n",
    "class FilesDataset1(Dataset1):#Renamed to avoid conflict with fastai FilesDataset\n",
    "    def __init__(self, folder, tfms):\n",
    "        cls_dirs = find_classes(folder)\n",
    "        self.fns, self.y = [], []\n",
    "        self.classes = [cls.name for cls in cls_dirs]\n",
    "        for i, cls_dir in enumerate(cls_dirs):\n",
    "            fnames = get_image_files(cls_dir)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        self.tfms = torchvision.transforms.Compose(tfms) if tfms != [] else None\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = Image.open(self.fns[i])\n",
    "        if self.tfms is not None: x = self.tfms(x) \n",
    "        return np.array(x, dtype=np.float32).transpose(2,0,1)/255,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = [torchvision.transforms.Pad(4, padding_mode='symmetric'),\n",
    "            torchvision.transforms.RandomCrop(32),\n",
    "            torchvision.transforms.RandomHorizontalFlip()]\n",
    "val_tfms = []\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5722c15d72104c17962df3fbe8117d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.106797   1.20077    0.5803    \n",
      "    1      0.841253   1.137082   0.6228                      \n",
      "    2      0.67957    0.898744   0.6969                      \n",
      "    3      0.604853   0.590829   0.7994                      \n",
      "    4      0.538944   0.692684   0.7748                      \n",
      "    5      0.496735   0.55795    0.8137                      \n",
      "    6      0.468676   0.520246   0.8232                      \n",
      "    7      0.432442   0.53066    0.8241                      \n",
      "    8      0.409816   0.496635   0.8319                      \n",
      "    9      0.384066   0.454232   0.846                       \n",
      "    10     0.365821   0.537091   0.8284                      \n",
      "    11     0.352648   0.431229   0.8565                      \n",
      "    12     0.338223   0.44415    0.8516                      \n",
      "    13     0.333257   0.459053   0.8375                      \n",
      "    14     0.317837   0.388367   0.8719                      \n",
      "    15     0.287709   0.377607   0.8742                      \n",
      "    16     0.251055   0.339506   0.8867                      \n",
      "    17     0.239938   0.357924   0.8812                      \n",
      "    18     0.216934   0.337228   0.8891                      \n",
      "    19     0.194689   0.313683   0.9019                      \n",
      "    20     0.166916   0.309344   0.8977                      \n",
      "    21     0.153332   0.26909    0.9141                      \n",
      "    22     0.125993   0.275879   0.9162                      \n",
      "    23     0.099443   0.264777   0.9201                       \n",
      "    24     0.085024   0.239538   0.9265                       \n",
      "    25     0.060834   0.253492   0.929                        \n",
      "    26     0.046201   0.242794   0.934                        \n",
      "    27     0.030591   0.243549   0.9361                       \n",
      "    28     0.020873   0.235312   0.9402                       \n",
      "    29     0.014911   0.223151   0.9445                       \n",
      "\n",
      "CPU times: user 9min 59s, sys: 3min 49s, total: 13min 48s\n",
      "Wall time: 13min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22315136890411377, 0.9445]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a tiny bit slower but nothing remarkable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New pipeline + data aug on tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do all the data aug on the torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as Dataset1\n",
    "\n",
    "class FilesDataset1(Dataset1):#Renamed to avoid conflict with fastai FilesDataset\n",
    "    def __init__(self, folder, tfms=None):\n",
    "        cls_dirs = find_classes(folder)\n",
    "        self.fns, self.y = [], []\n",
    "        self.classes = [cls.name for cls in cls_dirs]\n",
    "        for i, cls_dir in enumerate(cls_dirs):\n",
    "            fnames = get_image_files(cls_dir)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        self.tfms = tfms\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = Image.open(self.fns[i])\n",
    "        x = torch.tensor(np.array(x, dtype=np.float32).transpose(2,0,1)/255)\n",
    "        if self.tfms is not None: x = self.tfms(x)[0]\n",
    "        return x,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size):\n",
    "        self.p_flip,self.pad,self.size = p_flip,pad,size\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        x = x[:,:,a:a+self.size,b:b+self.size]\n",
    "        return do_random_flip(x, self.p_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_random_flip(x, prob):\n",
    "    if np.random.rand() < prob:\n",
    "        idx = [i for i in range(x.size(3)-1, -1, -1)]\n",
    "        idx = torch.LongTensor(idx)\n",
    "        return x.index_select(3, idx)\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb5373438e4bdb8197711023eea058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.103617   1.091981   0.6097    \n",
      "    1      0.831675   0.844796   0.7023                      \n",
      "    2      0.691814   0.839972   0.7116                      \n",
      "    3      0.593204   0.780473   0.741                       \n",
      "    4      0.559518   0.632671   0.7869                      \n",
      "    5      0.492613   0.471881   0.8396                      \n",
      "    6      0.456501   0.492801   0.8319                      \n",
      "    7      0.43044    0.437216   0.8522                      \n",
      "    8      0.407928   0.604274   0.7974                      \n",
      "    9      0.379541   0.569592   0.816                       \n",
      "    10     0.359945   0.457494   0.8454                      \n",
      "    11     0.353988   0.499036   0.8367                      \n",
      "    12     0.335698   0.494482   0.8376                      \n",
      "    13     0.338825   0.537266   0.831                       \n",
      "    14     0.305629   0.375439   0.8749                      \n",
      "    15     0.273885   0.438265   0.8595                      \n",
      "    16     0.261163   0.336748   0.8863                      \n",
      "    17     0.239979   0.299561   0.9031                      \n",
      "    18     0.210969   0.301144   0.9027                      \n",
      "    19     0.192596   0.35536    0.8906                      \n",
      "    20     0.165607   0.292314   0.9031                      \n",
      "    21     0.149376   0.300748   0.9077                      \n",
      "    22     0.12648    0.262754   0.9173                      \n",
      "    23     0.104606   0.287464   0.9139                      \n",
      "    24     0.082829   0.246321   0.9262                       \n",
      "    25     0.059893   0.258575   0.929                        \n",
      "    26     0.044219   0.228895   0.9383                       \n",
      "    27     0.031717   0.22635    0.9372                       \n",
      "    28     0.020474   0.223564   0.9426                       \n",
      "    29     0.013075   0.223093   0.9437                       \n",
      "\n",
      "CPU times: user 9min 48s, sys: 3min 54s, total: 13min 42s\n",
      "Wall time: 13min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22309259892702102, 0.9437]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit faster than opencv. And reflect padding instead of symmetric doesn't seem to hurt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same but with an interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(x, coords, padding='reflect'):\n",
    "    if padding=='reflect':#Reflect padding isn't implemented in grid_sample yet\n",
    "        coords[coords < -1] = coords[coords < -1].mul_(-1).add_(-2)\n",
    "        coords[coords > 1] = coords[coords > 1].mul_(-1).add_(2)\n",
    "        padding='zeros'\n",
    "    return F.grid_sample(x, coords, padding_mode=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size):\n",
    "        self.p_flip,self.pad,self.size = p_flip,pad,size\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        matrix = torch.eye(3)\n",
    "        matrix = matrix[:2,:]\n",
    "        img_size = torch.Size([1,3,h+2*self.pad,w+2*self.pad])\n",
    "        coords = F.affine_grid(matrix[None], img_size)\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        coords = coords[:,a:a+self.size,b:b+self.size,:]\n",
    "        return do_random_flip(interpolate(x, coords), self.p_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 159/391 [00:30<00:43,  5.30it/s, loss=0.291]"
     ]
    }
   ],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85959bd301734d618cd22f12a4aaae70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 145/391 [00:09<00:16, 14.60it/s, loss=1.5] \n",
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.096365   1.221584   0.5711    \n",
      "    1      0.828955   0.880446   0.6967                      \n",
      "    2      0.704505   0.847956   0.7117                      \n",
      "    3      0.612732   0.632858   0.7827                      \n",
      "    4      0.530568   0.601534   0.8001                      \n",
      "    5      0.489203   0.567136   0.8061                      \n",
      "    6      0.449854   0.480488   0.8403                      \n",
      "    7      0.435406   0.54834    0.8205                      \n",
      "    8      0.409922   0.456254   0.8476                      \n",
      "    9      0.407223   0.480855   0.8425                      \n",
      "    10     0.388129   0.561264   0.8225                      \n",
      "    11     0.363649   0.694288   0.7856                      \n",
      "    12     0.344785   0.47669    0.8455                      \n",
      "    13     0.332264   0.409744   0.8596                      \n",
      "    14     0.33043    0.398632   0.8645                      \n",
      "    15     0.289064   0.359781   0.8813                      \n",
      "    16     0.273721   0.384478   0.8729                      \n",
      "    17     0.2419     0.323606   0.8923                      \n",
      "    18     0.217356   0.310729   0.895                       \n",
      "    19     0.198375   0.323632   0.899                       \n",
      "    20     0.168788   0.300755   0.904                       \n",
      "    21     0.149459   0.304372   0.9049                      \n",
      "    22     0.11687    0.28526    0.913                       \n",
      "    23     0.110042   0.27576    0.9185                       \n",
      "    24     0.085793   0.257592   0.9277                       \n",
      "    25     0.065682   0.244674   0.9318                       \n",
      "    26     0.044053   0.243751   0.9371                       \n",
      "    27     0.0373     0.242295   0.9341                       \n",
      "    28     0.02109    0.223149   0.9417                       \n",
      "    29     0.015444   0.227427   0.9434                       \n",
      "\n",
      "CPU times: user 9min 53s, sys: 3min 48s, total: 13min 42s\n",
      "Wall time: 13min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22742701032161713, 0.9434]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't lose time and it's still as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same with random flip as an affine transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_transform(img, matrix, interpol=True, padding='reflect'):\n",
    "    \"\"\"\n",
    "    Applies an affine transformation to an image.\n",
    "    \n",
    "    Optional: only computes the new coordinates without doing the interpolation to create the new images.\n",
    "    Args:\n",
    "    x: a batch of images\n",
    "    matrix: a matrix of size 2 by 3 describing the transformation.\n",
    "            if the transformation is Ax + b, the matrix is (A|b)\n",
    "    interpol: if False, returns only the new coordinates\n",
    "    padding: padding to apply during the interpolation. Supports zeros, border, reflect\n",
    "    \n",
    "    \"\"\"\n",
    "    coords = F.affine_grid(matrix[None], img[None].size())\n",
    "    return interpolate(img[None],coords,padding) if interpol else coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_rot_matrix(degrees):\n",
    "    theta = random.uniform(-degrees,degrees) * math.pi / 180\n",
    "    return torch.tensor([[math.cos(theta), -math.sin(theta), 0],\n",
    "                         [math.sin(theta), math.cos(theta),  0],\n",
    "                         [0,               0,                1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_scale_matrix(zoom_range):\n",
    "    scale = random.uniform(*zoom_range)\n",
    "    return torch.tensor([[scale, 0, 0],\n",
    "                         [0, scale, 0],\n",
    "                         [0,  0,    1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_flip(prob):\n",
    "    if np.random.rand() < prob:\n",
    "        return torch.tensor([[-1, 0, 0],\n",
    "                             [0,  1, 0],\n",
    "                             [0,  0, 1]]).float()\n",
    "    else: return torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size):\n",
    "        self.p_flip,self.pad,self.size = p_flip,pad,size\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        matrix = get_random_flip(self.p_flip)\n",
    "        matrix = matrix[:2,:]\n",
    "        img_size = torch.Size([1,3,h+2*self.pad,w+2*self.pad])\n",
    "        coords = F.affine_grid(matrix[None], img_size)\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        coords = coords[:,a:a+self.size,b:b+self.size,:]\n",
    "        return interpolate(x, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71319bb2ea9c48c1ae62a5a6afd8b5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.090638   1.110851   0.6083    \n",
      "    1      0.868387   0.895112   0.6872                      \n",
      "    2      0.717526   0.729544   0.7502                      \n",
      "    3      0.607997   0.657253   0.7799                      \n",
      "    4      0.560143   0.625082   0.7863                      \n",
      "    5      0.491189   0.487409   0.8326                      \n",
      "    6      0.472838   0.578546   0.812                       \n",
      "    7      0.43529    0.514291   0.8297                      \n",
      "    8      0.41678    0.454186   0.8473                      \n",
      "    9      0.400172   0.439173   0.8569                      \n",
      "    10     0.372496   0.507483   0.8303                      \n",
      "    11     0.385859   0.419538   0.8588                      \n",
      "    12     0.359593   0.440407   0.8571                      \n",
      "    13     0.350565   0.524155   0.8191                      \n",
      "    14     0.321838   0.432839   0.853                       \n",
      "    15     0.297971   0.442856   0.8563                      \n",
      "    16     0.262823   0.356882   0.8824                      \n",
      "    17     0.240723   0.344589   0.8883                      \n",
      "    18     0.214819   0.390743   0.876                       \n",
      "    19     0.207647   0.361031   0.8859                      \n",
      "    20     0.174888   0.27826    0.9076                      \n",
      "    21     0.14067    0.310478   0.9051                      \n",
      "    22     0.133276   0.300791   0.9108                      \n",
      "    23     0.112153   0.276574   0.9148                      \n",
      "    24     0.082846   0.275415   0.9202                       \n",
      "    25     0.063018   0.243244   0.932                        \n",
      "    26     0.051963   0.243038   0.9335                       \n",
      "    27     0.032741   0.242727   0.9382                       \n",
      "    28     0.022344   0.245564   0.9407                       \n",
      "    29     0.013198   0.242242   0.942                        \n",
      "\n",
      "CPU times: user 9min 51s, sys: 3min 50s, total: 13min 42s\n",
      "Wall time: 13min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24224200434684753, 0.942]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still seems fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfm():\n",
    "    \n",
    "    def __init__(self, p_flip, pad, size, size_mult):\n",
    "        self.p_flip,self.pad,self.size,self.size_mult = p_flip,pad,size,size_mult\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        _, h, w = x.size()\n",
    "        #Resize the image so that the lower dimension is size * size_mult\n",
    "        ratio = (self.size * self.size_mult) / min(h,w)\n",
    "        h,w = int(h * ratio), int(w*ratio)\n",
    "        #Pads\n",
    "        x = F.pad(x[None], (self.pad,self.pad,self.pad,self.pad), 'reflect') #Symmetric not implemented in F.pad\n",
    "        #Affine transforms\n",
    "        matrix = get_random_flip(self.p_flip)\n",
    "        matrix = matrix[:2,:]\n",
    "        img_size = torch.Size([1,3,h+2*self.pad,w+2*self.pad])\n",
    "        coords = F.affine_grid(matrix[None], img_size)\n",
    "        #Coords transforms then crop\n",
    "        a = random.randint(0, h+2*self.pad-self.size) if h + 2*self.pad>= self.size else 0\n",
    "        b = random.randint(0, w+2*self.pad-self.size) if w + 2*self.pad>= self.size else 0\n",
    "        coords = coords[:,a:a+self.size,b:b+self.size,:]\n",
    "        #Interpolation\n",
    "        return interpolate(x, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_tfms = CustomTfm(0.5, 4, 32, 1)\n",
    "val_tfms = None\n",
    "data = DataBunch.from_files(PATH, trn_tfms, val_tfms, stats, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfms = tfms_from_stats(stats, 28, aug_tfms=[RandomCrop(28)], pad=0)\n",
    "data1 = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, val_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = wrn_22()\n",
    "opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn = ConvLearner.from_model_data(m, data1, opt_fn=opt_fn)\n",
    "learn.crit = nn.CrossEntropyLoss()\n",
    "learn.metrics = [accuracy]\n",
    "wd=0.1\n",
    "learn.data.trn_dl, learn.data.val_dl = data.trn_dl, data.val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2263a1b8e48a993f6d62609316dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      1.120263   1.128053   0.599     \n",
      "    1      0.870778   0.898527   0.6884                      \n",
      "    2      0.709498   0.687441   0.7642                      \n",
      "    3      0.614241   0.77149    0.7289                      \n",
      "    4      0.539517   0.730055   0.756                       \n",
      "    5      0.510432   0.524482   0.8198                      \n",
      "    6      0.477814   0.474085   0.8392                      \n",
      "    7      0.428758   0.543345   0.8174                      \n",
      "    8      0.431321   0.560641   0.8089                      \n",
      "    9      0.378077   0.436421   0.8542                      \n",
      "    10     0.378665   0.503413   0.8335                      \n",
      "    11     0.371681   0.47223    0.8381                      \n",
      "    12     0.354042   0.475374   0.8439                      \n",
      "    13     0.343062   0.415416   0.8606                      \n",
      "    14     0.314198   0.395102   0.8664                      \n",
      "    15     0.286189   0.411435   0.8634                      \n",
      "    16     0.263579   0.415265   0.863                       \n",
      "    17     0.234589   0.314385   0.8979                      \n",
      "    18     0.215581   0.335343   0.8956                      \n",
      "    19     0.201432   0.320588   0.8962                      \n",
      "    20     0.171908   0.259517   0.9164                      \n",
      "    21     0.145872   0.296372   0.9055                      \n",
      "    22     0.126011   0.262663   0.9175                      \n",
      "    23     0.105878   0.272667   0.9189                       \n",
      "    24     0.087653   0.252812   0.9237                       \n",
      "    25     0.058819   0.247293   0.9295                       \n",
      "    26     0.045004   0.256887   0.9323                       \n",
      "    27     0.033787   0.24148    0.9369                       \n",
      "    28     0.022081   0.238518   0.939                        \n",
      "    29     0.015321   0.238955   0.9407                       \n",
      "\n",
      "CPU times: user 9min 47s, sys: 3min 55s, total: 13min 43s\n",
      "Wall time: 13min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23895504400730133, 0.9407]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time learn.fit(3e-3, 1, cycle_len=30, use_clr_beta=(10,7.5,0.95,0.85), wds=wd, use_wd_sched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "266px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
