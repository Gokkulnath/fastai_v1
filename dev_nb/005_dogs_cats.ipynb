{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_004b import *\n",
    "import torchvision.models as tvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs and cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/dogscats')\n",
    "\n",
    "train_ds = FilesDataset.from_folder(PATH/'train')\n",
    "valid_ds = FilesDataset.from_folder(PATH/'valid')\n",
    "\n",
    "arch = tvm.resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform_int(low, high, size=None):\n",
    "    return random.randint(low,high) if size is None else torch.randint(low,high,size)\n",
    "\n",
    "@TfmPixel\n",
    "def dihedral(x, k:partial(uniform_int,0,8)):\n",
    "    flips=[]\n",
    "    if k&1: flips.append(1)\n",
    "    if k&2: flips.append(2)\n",
    "    if flips: x = torch.flip(x,flips)\n",
    "    if k&4: x = x.transpose(1,2)\n",
    "    return x.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=valid_ds[2][0]\n",
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axes.flat): dihedral(x,i).show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_transforms(do_flip=False, flip_vert=False, max_rotate=0., max_zoom=1., max_lighting=0., max_warp=0.,\n",
    "                   p_affine=0.75, p_lighting=0.5, xtra_tfms=None):\n",
    "    res = [rand_crop()]\n",
    "    if do_flip:    res.append(dihedral() if flip_vert else flip_lr(p=0.5))\n",
    "    if max_warp:   res.append(symmetric_warp(magnitude=(-max_warp,max_warp), p=p_affine))\n",
    "    if max_rotate: res.append(rotate(degrees=(-max_rotate,max_rotate), p=p_affine))\n",
    "    if max_zoom>1: res.append(rand_zoom(scale=(1.,max_zoom), p=p_affine))\n",
    "    if max_lighting:\n",
    "        res.append(brightness(change=(0.5*(1-max_lighting), 0.5*(1+max_lighting)), p=p_lighting))\n",
    "        res.append(contrast(scale=(1-max_lighting, 1/(1-max_lighting)), p=p_lighting))\n",
    "    #       train                   , valid\n",
    "    return (res + listify(xtra_tfms), [crop_pad()])  \n",
    "\n",
    "def transform_datasets(train_ds, valid_ds, tfms, **kwargs):\n",
    "    return (DatasetTfm(train_ds, tfms[0], **kwargs),\n",
    "            DatasetTfm(valid_ds, tfms[1], **kwargs),\n",
    "            DatasetTfm(valid_ds, tfms[0], **kwargs))\n",
    "\n",
    "imagenet_stats = tensor([0.485, 0.456, 0.406]), tensor([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm,data_denorm = normalize_funcs(*imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl:DataLoader, valid_dl:DataLoader, augm_dl:DataLoader=None,\n",
    "                 device:torch.device=None, tfms=None):\n",
    "        self.device = default_device if device is None else device\n",
    "        self.train_dl = DeviceDataLoader(train_dl, self.device, tfms=tfms)\n",
    "        self.valid_dl = DeviceDataLoader(valid_dl, self.device, tfms=tfms)\n",
    "        if augm_dl: self.augm_dl = DeviceDataLoader(augm_dl,  self.device, tfms=tfms)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, augm_ds=None, bs=64, train_tfm=None, valid_tfm=None, num_workers=4,\n",
    "               tfms=None, device=None, **kwargs):\n",
    "        if train_tfm or not isinstance(train_ds, DatasetTfm): train_ds = DatasetTfm(train_ds,train_tfm, **kwargs)\n",
    "        if valid_tfm or not isinstance(valid_ds, DatasetTfm): valid_ds = DatasetTfm(valid_ds,valid_tfm, **kwargs)\n",
    "        if not augm_ds: augm_ds = DatasetTfm(valid_ds, train_tfm, **kwargs)\n",
    "        return cls(DataLoader(train_ds, bs,   shuffle=True,  num_workers=num_workers),\n",
    "                   DataLoader(valid_ds, bs*2, shuffle=False, num_workers=num_workers),\n",
    "                   DataLoader(augm_ds,  bs*2, shuffle=False, num_workers=num_workers),\n",
    "                   device=device, tfms=tfms)\n",
    "\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dl.dataset\n",
    "    @property\n",
    "    def c(self): return self.train_ds.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=224\n",
    "\n",
    "tfms = get_transforms(do_flip=True, max_rotate=10, max_zoom=1.2, max_lighting=0.3, max_warp=0.15)\n",
    "tds = transform_datasets(train_ds, valid_ds, tfms, size=size)\n",
    "data = DataBunch.create(*tds, bs=64, num_workers=8, tfms=data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = next(iter(data.valid_dl))\n",
    "\n",
    "_,axs = plt.subplots(4,4,figsize=(12,12))\n",
    "for i,ax in enumerate(axs.flatten()): show_image(data_denorm(x[i].cpu()), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(4,4,figsize=(12,12))\n",
    "for ax in axs.flat: tds[0][2][0].show(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_epoch(model, dl, opt, loss_func):\n",
    "    \"Simple training of `model` for 1 epoch of `dl` using optim `opt` and loss function `loss_func`\"\n",
    "    model.train()\n",
    "    for xb,yb in dl:\n",
    "        loss = loss_func(model(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    def __init__(self, sz=None):\n",
    "        super().__init__()\n",
    "        sz = sz or 1\n",
    "        self.ap,self.mp = nn.AdaptiveAvgPool2d(sz), nn.AdaptiveMaxPool2d(sz)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "def create_body(model, cut=None, body_fn=None):\n",
    "    return (nn.Sequential(*list(model.children())[:-cut]) if cut\n",
    "            else body_fn(model) if body_fn else model)\n",
    "\n",
    "def num_features(m):\n",
    "    for l in reversed(flatten_model(m)):\n",
    "        if hasattr(l, 'num_features'): return l.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_body(arch(), 2)\n",
    "num_features(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers\n",
    "\n",
    "def create_head(nf, nc, lin_ftrs=None, ps=0.2):\n",
    "    lin_ftrs = [nf, 512, nc] if lin_ftrs is None else [nf] + lin_ftrs + [nc]\n",
    "    ps = listify(ps)\n",
    "    if len(ps)==1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    layers = [AdaptiveConcatPool2d(), Flatten()]\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1],lin_ftrs[1:],ps,actns): \n",
    "        layers += bn_drop_lin(ni,no,True,p,actn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_head(512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cond_init(m, init_fn):\n",
    "    if (not isinstance(m, bn_types)) and requires_grad(m):\n",
    "        if hasattr(m, 'weight'): init_fn(m.weight)\n",
    "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
    "\n",
    "def apply_leaf(m, f):\n",
    "    c = children(m)\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)\n",
    "\n",
    "def apply_init(m, init_fn): apply_leaf(m, partial(cond_init, init_fn=init_fn))\n",
    "\n",
    "def _init(learn, init): apply_init(learn.model, init)\n",
    "Learner.init = _init\n",
    "\n",
    "class ConvLearner(Learner):\n",
    "    def __init__(self, data, arch, cut, pretrained=True, lin_ftrs=None, ps=0.2, custom_head=None, **kwargs):\n",
    "        body = create_body(arch(pretrained), cut)\n",
    "        nf = num_features(body) * 2\n",
    "        head = custom_head or create_head(nf, data.c, lin_ftrs, ps)\n",
    "        model = nn.Sequential(body, head)\n",
    "        super().__init__(data, model, **kwargs)\n",
    "        self.split([model[1]])\n",
    "        if pretrained: self.freeze()\n",
    "        apply_init(model[1], nn.init.kaiming_normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, arch, 2, wd=1e-2, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=6e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(6, slice(lr/25,lr), pct_start=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Hook():\n",
    "    def __init__(self, m, hook_func, is_forward=True):\n",
    "        self.hook_func,self.stored = hook_func,None\n",
    "        f = m.register_forward_hook if is_forward else m.register_backward_hook\n",
    "        self.hook = f(self.hook_fn)\n",
    "        self.removed = False\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        input  = (o.detach() for o in input ) if is_listy(input ) else input.detach()\n",
    "        output = (o.detach() for o in output) if is_listy(output) else output.detach()\n",
    "        self.stored = self.hook_func(module, input, output)\n",
    "\n",
    "    def remove(self):\n",
    "        if not self.removed:\n",
    "            self.hook.remove()\n",
    "            self.removed=True\n",
    "\n",
    "class Hooks():\n",
    "    def __init__(self, ms, hook_func, is_forward=True):\n",
    "        self.hooks = [Hook(m, hook_func, is_forward) for m in ms]\n",
    "        \n",
    "    def __getitem__(self,i): return self.hooks[i]\n",
    "    def __len__(self): return len(self.hooks)\n",
    "    def __iter__(self): return iter(self.hooks)\n",
    "    @property\n",
    "    def stored(self): return [o.stored for o in self]\n",
    "    \n",
    "    def remove(self):\n",
    "        for h in self.hooks: h.remove()\n",
    "\n",
    "def hook_output (module):  return Hook (module,  lambda m,i,o: o)\n",
    "def hook_outputs(modules): return Hooks(modules, lambda m,i,o: o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HookCallback(LearnerCallback):\n",
    "    def __init__(self, learn, modules=None, do_remove=True):\n",
    "        super().__init__(learn)\n",
    "        self.modules,self.do_remove = modules,do_remove\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        if not self.modules:\n",
    "            self.modules = [m for m in flatten_model(self.learn.model)\n",
    "                            if hasattr(m, 'weight')]\n",
    "        self.hooks = Hooks(self.modules, self.hook)\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        if self.do_remove: self.remove()\n",
    "\n",
    "    def remove(self): self.hooks.remove\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "class ActivationStats(HookCallback):\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        super().on_train_begin(**kwargs)\n",
    "        self.stats = []\n",
    "        \n",
    "    def hook(self, m,i,o): return o.mean().item(),o.std().item()\n",
    "    def on_batch_end(self, **kwargs): self.stats.append(self.hooks.stored)\n",
    "    def on_train_end(self, **kwargs): self.stats = tensor(self.stats).permute(2,1,0)\n",
    "\n",
    "def idx_dict(a): return {v:k for k,v in enumerate(a)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, arch, 2, wd=1e-2, metrics=accuracy,\n",
    "                    callback_fns=ActivationStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = learn.activation_stats.modules\n",
    "d = idx_dict(ms)\n",
    "ln = d[learn.model[1][8]]; ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learn.activation_stats.stats[1][ln].numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(2,4,figsize=(12,6))\n",
    "for ax in axs.flat: tds[2][1][0].show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_preds(model, dl, pbar=None):\n",
    "    return [torch.cat(o).cpu() for o in validate(model, dl, pbar=pbar)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,y = get_preds(model, data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = master_bar(range(4))\n",
    "all_preds = torch.stack([get_preds(model, data.augm_dl, pbar=pbar)[0] for _ in pbar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_preds = all_preds.mean(0)\n",
    "avg_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(avg_preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=0.5\n",
    "accuracy(preds*beta + avg_preds*(1-beta), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TTA(model, valid_dl, augm_dl, n=4, beta=0.5):\n",
    "    preds,y = get_preds(model, valid_dl)\n",
    "    pbar = master_bar(range(n))\n",
    "    all_preds = torch.stack([get_preds(model, augm_dl, pbar=pbar)[0]\n",
    "                             for _ in pbar]).mean(0)\n",
    "    return preds*beta + avg_preds*(1-beta)\n",
    "\n",
    "def _learn_TTA(learn, n=4, beta=0.5):\n",
    "    return TTA(learn.model, learn.data.valid_dl, learn.data.augm_dl, n=n, beta=beta)\n",
    "\n",
    "Learner.TTA = _learn_TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, arch, 2, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_preds = learn.TTA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(tta_preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
