{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get everything in a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_007 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/cifar10/')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func=func\n",
    "        \n",
    "    def forward(self, x): return self.func(x)\n",
    "\n",
    "def ResizeBatch(*size): return Lambda(lambda x: x.view((-1,)+size))\n",
    "def Flatten(): return Lambda(lambda x: x.view((x.size(0), -1)))\n",
    "def PoolFlatten(): return nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_2d(ni, nf, ks, stride): return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=False)\n",
    "\n",
    "def bn(ni, init_zero=False):\n",
    "    m = nn.BatchNorm2d(ni)\n",
    "    m.weight.data.fill_(0 if init_zero else 1)\n",
    "    m.bias.data.zero_()\n",
    "    return m\n",
    "\n",
    "def bn_relu_conv(ni, nf, ks, stride, init_zero=False):\n",
    "    bn_initzero = bn(ni, init_zero=init_zero)\n",
    "    return nn.Sequential(bn_initzero, nn.ReLU(inplace=True), conv_2d(ni, nf, ks, stride))\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, stride, drop_p=0.0):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(ni)\n",
    "        self.conv1 = conv_2d(ni, nf, 3, stride)\n",
    "        self.conv2 = bn_relu_conv(nf, nf, 3, 1)\n",
    "        self.drop = nn.Dropout(drop_p, inplace=True) if drop_p else None\n",
    "        self.shortcut = conv_2d(ni, nf, 1, stride) if ni != nf else noop\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = F.relu(self.bn(x), inplace=True)\n",
    "        r = self.shortcut(x2)\n",
    "        x = self.conv1(x2)\n",
    "        if self.drop: x = self.drop(x)\n",
    "        x = self.conv2(x) * 0.2\n",
    "        return x.add_(r)\n",
    "\n",
    "\n",
    "def _make_group(N, ni, nf, block, stride, drop_p):\n",
    "    return [block(ni if i == 0 else nf, nf, stride if i == 0 else 1, drop_p) for i in range(N)]\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, num_groups, N, num_classes, k=1, drop_p=0.0, start_nf=16):\n",
    "        super().__init__()\n",
    "        n_channels = [start_nf]\n",
    "        for i in range(num_groups): n_channels.append(start_nf*(2**i)*k)\n",
    "\n",
    "        layers = [conv_2d(3, n_channels[0], 3, 1)]  # conv1\n",
    "        for i in range(num_groups):\n",
    "            layers += _make_group(N, n_channels[i], n_channels[i+1], BasicBlock, (1 if i==0 else 2), drop_p)\n",
    "\n",
    "        layers += [nn.BatchNorm2d(n_channels[3]), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d(1),\n",
    "                   Flatten(), nn.Linear(n_channels[3], num_classes)]\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x): return self.features(x)\n",
    "\n",
    "\n",
    "def wrn_22(): return WideResNet(num_groups=3, N=3, num_classes=10, k=6, drop_p=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrn_22()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way to create datasets in fastai_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = FilesDataset.from_folder(PATH/'train'), FilesDataset.from_folder(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mean,cifar_std = map(tensor, ([0.4914, 0.48216, 0.44653], [0.24703, 0.24349, 0.26159]))\n",
    "cifar_norm,cifar_denorm = normalize_funcs(cifar_mean,cifar_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = [pad(padding=4), \n",
    "              crop(size=32, row_pct=(0,1), col_pct=(0,1)), \n",
    "              flip_lr(p=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=512, train_tfm=train_tfms, tfms=cifar_norm, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learner wraps together the data and the model, like in fastai. Here we test the usual training to 94% accuracy with AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrn_22()\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 3e-3, wd=0.4, div_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same but in mixed-precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wrn_22()\n",
    "model = model2half(model)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.callbacks.append(MixedPrecision(learn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(25, 3e-3, wd=0.4, div_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time learn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
