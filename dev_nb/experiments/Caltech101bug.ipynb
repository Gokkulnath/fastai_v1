{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") \n",
    "\n",
    "from nb_004 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data')\n",
    "PATH = DATA_PATH/'caltech101'\n",
    "\n",
    "data_mean,data_std = map(tensor, ([0.5355,0.5430,0.5280], [0.2909,0.2788,0.2979]))\n",
    "\n",
    "classes = ['airplanes','Motorbikes','Faces','watch','Leopards']\n",
    "np.random.seed(42)\n",
    "train_ds,valid_ds = FilesDataset.from_folder(PATH, test_pct=0.2, classes=classes)\n",
    "classes = train_ds.classes\n",
    "c = len(classes); c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 224\n",
    "norm = normalize_tfm(mean=data_mean,std=data_std)\n",
    "denorm = partial(denormalize,mean=data_mean,std=data_std)\n",
    "\n",
    "trn_tfms = [squish_tfm(scale=(0.75,1.33), row_pct=(0,1.), col_pct=(0,1.)),\n",
    "            zoom_tfm(scale=(0.8,1.), row_pct=(0,1.), col_pct=(0,1.)),\n",
    "            flip_lr_tfm(p=0.5),\n",
    "            crop_tfm(size=sz),\n",
    "            norm]\n",
    "val_tfms = [crop_tfm(size=sz), norm]\n",
    "\n",
    "train_ds = TfmDataset(train_ds, trn_tfms, size=224, do_crop=True)\n",
    "valid_ds = TfmDataset(valid_ds, val_tfms, size=224, do_crop=True)\n",
    "\n",
    "data = DataBunch(train_ds, valid_ds, bs=64, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_cnn(n_classes, actns, kernel_szs, strides, bn=False):\n",
    "    kernel_szs = listify(kernel_szs, len(actns)-1)\n",
    "    strides    = listify(strides   , len(actns)-1)\n",
    "    layers = [conv2_relu(actns[i], actns[i+1], kernel_szs[i], stride=strides[i], bn=bn)\n",
    "        for i in range(len(strides))]\n",
    "    layers += [PoolFlatten(), nn.Linear(actns[-1], n_classes)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv2_relu(nif, nof, ks, stride, bn=False):\n",
    "    layers = [nn.Conv2d(nif, nof, ks, stride, padding=ks//2), nn.ReLU()]\n",
    "    if bn: layers.append(nn.BatchNorm2d(nof))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(): return simple_cnn(c, [3,16,16,16], 3, 2, bn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_cycle(learn:Learner, max_lr:float, cyc_len:int, moms=(0.95,0.85), div_factor:float=10.,\n",
    "                 pct_end:float=0.1, wd:float=0.):\n",
    "    \"Fits a model following the 1cycle policy\"\n",
    "    cbs = [OneCycleScheduler(learn, max_lr, cyc_len, moms, div_factor, pct_end)]\n",
    "    learn.fit(cyc_len, max_lr/div_factor, wd=wd, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = Learner(data, model)\n",
    "learn.loss_fn = F.cross_entropy\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn.true_wd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, 1e-3, 1, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0][2].running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    *val_metrics,nums = zip(*[loss_batch(model, xb, yb, learn.loss_fn, metrics=learn.metrics)\n",
    "                                for xb,yb in learn.data.valid_dl])\n",
    "    val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),PATH/'model1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = get_model()\n",
    "learn1 = Learner(data, model1)\n",
    "learn1.loss_fn = F.cross_entropy\n",
    "learn1.metrics = [accuracy]\n",
    "learn1.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn1.true_wd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load(PATH/'model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    *val_metrics,nums = zip(*[loss_batch(model1, xb, yb, learn1.loss_fn, metrics=learn1.metrics)\n",
    "                                for xb,yb in learn1.data.valid_dl])\n",
    "    val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1[0][2].running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1[0][2].running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
