{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_004 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../../data')\n",
    "PATH = DATA_PATH/'caltech101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std = map(tensor, ([0.5355,0.5430,0.5280], [0.2909,0.2788,0.2979]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilesDataset(Dataset):\n",
    "    def __init__(self, fns, labels, classes=None):\n",
    "        if classes is None: classes = list(set(labels))\n",
    "        self.classes = classes\n",
    "        self.class2idx = {v:k for k,v in enumerate(classes)}\n",
    "        self.fns = np.array(fns)\n",
    "        self.y = [self.class2idx[o] for o in labels]\n",
    "        \n",
    "    @classmethod\n",
    "    def from_folder(cls, folder, classes=None, test_pct=0., tfms=None):\n",
    "        if classes is None: classes = [cls.name for cls in find_classes(folder)]\n",
    "            \n",
    "        fns,labels = [],[]\n",
    "        for cl in classes:\n",
    "            fnames = get_image_files(folder/cl)\n",
    "            fns += fnames\n",
    "            labels += [cl] * len(fnames)\n",
    "            \n",
    "        if test_pct==0.: return cls(fns, labels)\n",
    "        fns,labels = np.array(fns),np.array(labels)\n",
    "        is_test = np.random.uniform(size=(len(fns),)) < test_pct\n",
    "        return cls(fns[~is_test], labels[~is_test]), cls(fns[is_test], labels[is_test])\n",
    "\n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = PIL.Image.open(self.fns[i]).convert('RGB')\n",
    "        x = pil2tensor(x)\n",
    "        return x,self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes = ['airplanes','Motorbikes','Faces','watch','Leopards']\n",
    "np.random.seed(42)\n",
    "train_ds,valid_ds = FilesDataset.from_folder(PATH, test_pct=0.2)\n",
    "classes = train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_affine\n",
    "def zoom1(scale: uniform = 1.0, row_pct:uniform = 0.5, col_pct:uniform = 0.5) -> TfmType.Affine:\n",
    "    s = 1-math.sqrt(scale)\n",
    "    col_c = s * (2*col_pct - 1)\n",
    "    row_c = s * (2*row_pct - 1)\n",
    "    return get_zoom_mat(math.sqrt(scale), math.sqrt(scale), col_c, row_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_affine\n",
    "def squish(scale: uniform = 1.0, row_pct:uniform = 0.5, col_pct:uniform = 0.5) -> TfmType.Affine:\n",
    "    if scale <= 1: \n",
    "        col_c = (1-scale) * (2*col_pct - 1)\n",
    "        return get_zoom_mat(scale, 1, col_c, 0.)\n",
    "    else:          \n",
    "        row_c = (1-1/scale) * (2*row_pct - 1)\n",
    "        return get_zoom_mat(1, 1/scale, 0., row_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 224\n",
    "trn_tfms = [squish_tfm(scale=(0.75,1.33), row_pct=(0,1.), col_pct=(0,1.)),\n",
    "            zoom1_tfm(scale=(0.8,1.), row_pct=(0,1.), col_pct=(0,1.)),\n",
    "            flip_lr_tfm(p=0.5),\n",
    "            crop_tfm(size=sz),\n",
    "            normalize_tfm(mean=data_mean,std=data_std)] #torchvision.transforms.RandomRotation(10),\n",
    "val_tfms = [crop_tfm(size=sz),\n",
    "            normalize_tfm(mean=data_mean,std=data_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes = ['airplanes','Motorbikes','Faces','watch','Leopards']\n",
    "np.random.seed(42)\n",
    "train_ds,valid_ds = FilesDataset.from_folder(PATH, test_pct=0.2)\n",
    "classes = train_ds.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TfmDataset(train_ds, trn_tfms, size=224, do_crop=True)\n",
    "valid_ds = TfmDataset(valid_ds, val_tfms, size=224, do_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_ds[0]\n",
    "x,y = valid_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x.numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    \"Data object that regroups training and validation data\"\n",
    "    \n",
    "    def __init__(self, train_ds:Dataset, valid_ds:Dataset, bs:int=64, device:torch.device=None, num_workers:int=4):\n",
    "        self.device,self.bs = default_device if device is None else device,bs\n",
    "        self.train_dl = DeviceDataLoader.create(train_ds, bs, shuffle=True, num_workers=num_workers, device=self.device)\n",
    "        self.valid_dl = DeviceDataLoader.create(valid_ds, bs*2, shuffle=False, num_workers=num_workers, device=self.device)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        res = f'DataBunch, batch_size={self.bs} on {self.device}.\\n  train dataloader: {len(self.train_dl)} batches'\n",
    "        if self.valid_dl is not None: res += f'\\n  validation dataloader: {len(self.valid_dl)} batches'\n",
    "        return res\n",
    "\n",
    "    #TODO: uncomment when transforms are available\n",
    "    #@classmethod\n",
    "    #def create(cls, train_ds, valid_ds, train_tfm=None, valid_tfm=None, **kwargs):\n",
    "    #    return cls(TfmDataset(train_ds, train_tfm), TfmDataset(valid_ds, valid_tfm))\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self) -> Dataset: return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self) -> Dataset: return self.valid_dl.dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs=64, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.valid_dl))\n",
    "\n",
    "_,axes = plt.subplots(2,4, figsize=(9,3))\n",
    "for i,ax in enumerate(axes.flat): show_image(x[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Learner():\n",
    "    \"Object that wraps together some data, a model, a loss function and an optimizer\"\n",
    "    \n",
    "    data:DataBunch\n",
    "    model:nn.Module\n",
    "    opt_fn:Callable=optim.SGD\n",
    "    loss_fn:Callable=F.cross_entropy\n",
    "    metrics:Collection[Callable]=None\n",
    "    true_wd:bool=False\n",
    "    def __post_init__(self): \n",
    "        self.model = self.model.to(self.data.device)\n",
    "        self.callbacks = []\n",
    "\n",
    "    def fit(self, epochs:int, lr:float, wd:float=0., callbacks:Collection[Callback]=None):\n",
    "        if not hasattr(self, 'opt'): self.create_opt(lr, wd)\n",
    "        if callbacks is None: callbacks = []\n",
    "        callbacks = self.callbacks + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics)\n",
    "    \n",
    "    def create_opt(self, lr:float, wd:float=0.):\n",
    "        self.opt = OptimWrapper(self.opt_fn(self.model.parameters(), lr), wd=wd, true_wd=self.true_wd)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        self.callbacks = [self.recorder] + self.callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 6, 3], num_classes=len(classes), nf=16).cuda()\n",
    "learn = Learner(data, model)\n",
    "learn.loss_fn = F.cross_entropy\n",
    "learn.metrics = [accuracy]\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn.true_wd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_cycle(learn:Learner, max_lr:float, cyc_len:int, moms=(0.95,0.85), div_factor:float=10.,\n",
    "                 pct_end:float=0.1, wd:float=0.):\n",
    "    \"Fits a model following the 1cycle policy\"\n",
    "    cbs = [OneCycleScheduler(learn, max_lr, cyc_len, moms, div_factor, pct_end)]\n",
    "    learn.fit(cyc_len, max_lr/div_factor, wd=wd, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0][1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_one_cycle(learn, 4e-3, 5, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0][1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model.state_dict(),PATH/'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),PATH/'model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb,yb in learn.data.valid_dl:\n",
    "        print(loss_batch(model, xb, yb, learn.loss_fn, metrics=learn.metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Darknet([1, 2, 4, 6, 3], num_classes=len(classes), nf=16).cuda()\n",
    "learn1 = Learner(data, model1)\n",
    "learn1.loss_fn = F.cross_entropy\n",
    "learn1.metrics = [accuracy]\n",
    "learn1.opt_fn = partial(optim.Adam, betas=(0.95,0.99))\n",
    "learn1.true_wd = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.layers[0][1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load(PATH/'model1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.layers[0][1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    *val_metrics,nums = zip(*[loss_batch(learn1.model, xb, yb, learn1.loss_fn, metrics=learn1.metrics)\n",
    "                                for xb,yb in learn1.data.valid_dl])\n",
    "    val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "with torch.no_grad():\n",
    "    for xb,yb in learn1.data.valid_dl:\n",
    "        print(loss_batch(model1, xb, yb, learn1.loss_fn, metrics=learn1.metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
