{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_001b import *\n",
    "from PIL import Image\n",
    "import PIL, matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from operator import itemgetter, attrgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temp storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_listy(x): return isinstance(x, (list,tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/carvana')\n",
    "PATH_PNG = PATH/'train_masks_png'\n",
    "PATH_X = PATH/'train-128'\n",
    "PATH_Y = PATH/'train_masks-128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and resize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PNG.mkdir(exist_ok=True)\n",
    "PATH_X.mkdir(exist_ok=True)\n",
    "PATH_Y.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img(fn): Image.open(fn).save(PATH_PNG/f'{fn.name[:-4]}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list((PATH/'train_masks').iterdir())\n",
    "with ThreadPoolExecutor(8) as e: e.map(convert_img, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(fn, dirname):\n",
    "    Image.open(fn).resize((128,128)).save((fn.parent.parent)/dirname/fn.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(PATH_PNG).iterdir())\n",
    "with ThreadPoolExecutor(8) as e: e.map(partial(resize_img, dirname='train_masks-128'), files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list((PATH/'train').iterdir())\n",
    "with ThreadPoolExecutor(8) as e: e.map(partial(resize_img, dirname='train-128'), files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = next(PATH_X.iterdir())\n",
    "img_x = open_image(img_f)\n",
    "show_image(img_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_fn(x_fn): return f'{x_fn[:-4]}_mask.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_y_f = PATH_Y/get_y_fn(img_f.name)\n",
    "img_y = open_image(img_y_f)\n",
    "show_image(img_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(): return open_image(img_f)\n",
    "def y(): return open_image(img_y_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5),\n",
    "        rotate_tfm(degrees=(-10,10.), p=0.25),\n",
    "        zoom_tfm(scale=(0.8,1.2), p=0.25),\n",
    "        contrast_tfm(scale=(0.8,1.2)),\n",
    "        brightness_tfm(change=(0.4,0.6))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_pipeline(x(), tfms), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_pipeline(y(), tfms), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependent var transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy(): return x(),y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_args(brightness, change=(0.4,0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_rand(x, y=None, smooth_y=True):\n",
    "    args = resolve_args(rotate, degrees=(-45,45.))\n",
    "    m = rotate(**args)\n",
    "    x = do_affine(x, m)\n",
    "    if y is None: return x\n",
    "    \n",
    "    y = do_affine(y, m)\n",
    "    if not smooth_y: torch.round_(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgx,imgy = rotate_rand(*xy(), smooth_y=False)\n",
    "assert(torch.any((imgy>0.) & (imgy<1.)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    imgx,imgy = rotate_rand(*xy(), smooth_y=False)\n",
    "    show_image(imgx, axes[0][i])\n",
    "    show_image(imgy, axes[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    imgx,imgy = rotate_rand(x(),x())\n",
    "    show_image(imgx, axes[0][i])\n",
    "    show_image(imgy, axes[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,6))\n",
    "for ax in axes: show_image(rotate_rand(x()), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_affine(img_x, img_y=None, m=None, funcs=None, smooth_y=True):\n",
    "    if m is None: m=eye_new(img_x, 3)\n",
    "    c = affine_grid(img_x,  img_x.new_tensor(m))\n",
    "    c = compose(funcs)(c)\n",
    "    img_x = grid_sample(img_x, c, padding='zeros')\n",
    "    if img_y is None: return img_x\n",
    "\n",
    "    img_y = grid_sample(img_y, c, padding='zeros')\n",
    "    if not smooth_y: torch.round_(img_y)\n",
    "    return img_x, img_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pixel_tfm(func): \n",
    "    def _inner(x,y=None):\n",
    "        logit_(x)\n",
    "        if y is None: return func(x).sigmoid()\n",
    "        logit_(y)\n",
    "        x,y = func(x,y)\n",
    "        return x.sigmoid(),y.sigmoid()\n",
    "    \n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pipeline(tfms, x, y=None, smooth_y=True):\n",
    "    tfms = listify(tfms)\n",
    "    if len(tfms)==0: return x\n",
    "    grouped_tfms = dict_groupby(tfms, lambda o: o.__annotations__['return'])\n",
    "    pixel_tfms,coord_tfms,affine_tfms = map(grouped_tfms.get, TfmType)\n",
    "    x = apply_pixel_tfm(compose(pixel_tfms))(x,y)\n",
    "    if isinstance(x,tuple): x,y = x\n",
    "    matrices = [f() for f in listify(affine_tfms)]\n",
    "    return do_affine(x, y, affines_mat(x, matrices), funcs=coord_tfms, smooth_y=smooth_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [rotate_tfm(degrees=(-45,45.)), brightness_tfm(change=(0.3,0.7))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgx,imgy = apply_pipeline(tfms, *xy(), smooth_y=False)\n",
    "assert(torch.any((imgy>0.) & (imgy<1.)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    imgx,imgy = apply_pipeline(tfms, *xy(), smooth_y=False)\n",
    "    show_image(imgx, axes[0][i])\n",
    "    show_image(imgy, axes[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    imgx,imgy = apply_pipeline(tfms, x(),x())\n",
    "    show_image(imgx, axes[0][i])\n",
    "    show_image(imgy, axes[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,6))\n",
    "for ax in axes: show_image(apply_pipeline(tfms, x()), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms2 = [jitter_tfm(magnitude=(-0.1,0.1))]\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,6))\n",
    "for ax in axes: show_image(apply_pipeline(tfms2, x()), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def annealing_no(start, end, pct): return start\n",
    "def annealing_linear(start, end, pct): return start + pct * (end-start)\n",
    "def annealing_exp(start, end, pct): return start * ((end/start) ** pct)\n",
    "def annealing_cos(start, end, pct):\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out\n",
    "    \n",
    "def do_annealing_poly(start, end, pct, degree): return end + (start-end) * (1-pct)**degree\n",
    "def annealing_poly(degree): return functools.partial(do_annealing_poly, degree=degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealings = \"NO LINEAR COS EXP POLY\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(0, 100)\n",
    "p = np.linspace(0.01,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fns = [annealing_no, annealing_linear, annealing_cos, annealing_exp, annealing_poly(0.8)]\n",
    "for fn, t in zip(fns, annealings):\n",
    "    plt.plot(a, [fn(2, 1e-2, o) for o in p], label=t)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_val_in_group(val, group, key):\n",
    "    if key == 'momentum' and 'betas' in group:  group['betas'] = (val, group['betas'][1])\n",
    "    elif key == 'beta' and 'betas' in group:    group['betas'] = (group['betas'][0], val)\n",
    "    elif key == 'beta' and 'alpha' in group:    group['alpha'] = val\n",
    "    else:                                       group[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParamScheduler():\n",
    "    def __init__(self, opt, hyper_param, n_iter, annealing, start, end=None, extra=None):\n",
    "        self.opt,self.hyper_param,self.n_iter,self.annealing = opt,hyper_param,n_iter,annealing\n",
    "        self.start,self.end,self.extra = start,end,extra\n",
    "        # TODO: what's this?\n",
    "        #if None, default end is 0 (only useful for cosine annealing, so we can decide to remove this)\n",
    "        #at the same time, defaut arg can't be 0 since annealing_exp wouldn't like it. So this is\n",
    "        #is a messy way around.\n",
    "        if self.end is None and not (self.annealing in [annealing_no, annealing_exp]):\n",
    "            self.end = 0\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.n = 0\n",
    "        self.vals = []\n",
    "        self.set_val(self.start)\n",
    "        \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        # TODO: what's this?\n",
    "        #This is the class telling anyone listening it's finished: useful for when you want to go the next phase.\n",
    "        if self.n >= self.n_iter: return True\n",
    "        lr = self.annealing(self.start, self.end, self.n/(self.n_iter-1))\n",
    "        self.set_val(lr)\n",
    "        return False\n",
    "    \n",
    "    def set_val(self, val):\n",
    "        if not is_iter(val): val = [val]\n",
    "        if len(val) == 1: val = val * len(self.opt.param_groups)\n",
    "        self.vals.append(val[-1])\n",
    "        for v, g in zip(val,self.opt.param_groups): set_val_in_group(v, g, self.hyper_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = PATH/'models'\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "TEMP_MODEL_NAME = 'tmp.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname): torch.save(model.state_dict(), fname)\n",
    "def load_model(model, fname): model.load_state_dict(torch.load(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(model, loss_fn, opt_fn, train_dl, n_iter=200, start_lr=1e-5, end_lr=10):\n",
    "    opt = opt_fn(model.parameters(), lr=start_lr)\n",
    "    \n",
    "    save_model(model, MODEL_PATH/'tmp.pt')\n",
    "    model.train()\n",
    "    lrs, losses= [], []\n",
    "    msa_loss, beta = 0, 0.98\n",
    "    min_loss = 0\n",
    "    lr_sched = HyperParamScheduler(opt, 'lr', n_iter, annealing_exp, start_lr, end_lr)\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        for xb,yb in train_dl: \n",
    "            #pdb.set_trace()\n",
    "            raw_loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            # smoothes the loss with a mean average\n",
    "            msa_loss = beta * msa_loss + (1-beta) * raw_loss\n",
    "            losses.append(msa_loss / (1 - beta ** (len(losses)+1)))\n",
    "            finished = lr_sched.step()\n",
    "            if len(losses) == 1 or msa_loss < min_loss: min_loss = msa_loss\n",
    "            if finished or msa_loss > 4 * min_loss: break\n",
    "\n",
    "    load_model(model, MDL_PATH/'tmp.pt')\n",
    "    return lr_sched.vals, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr(lrs, losses, skip_start=10, skip_end=5):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    lrs = lrs[skip_start:-skip_end] if skip_end !=0 else lrs[skip_start:]\n",
    "    losses = losses[skip_start:-skip_end] if skip_end !=0 else losses[skip_start:]\n",
    "    ax.plot(lrs, losses)\n",
    "    ax.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs, losses = lr_find(model, loss_fn, optim.SGD, data.train_dl, end_lr=100)\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactors inside to use the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def on_train_begin(self): pass\n",
    "    def on_train_end(self): pass\n",
    "    def on_batch_begin(self): pass\n",
    "    def on_batch_end(self, raw_loss): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossRecorder(Callback):\n",
    "    beta = 0.98\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.msa_loss,self.losses = 0,[]\n",
    "        \n",
    "    def on_batch_end(self, raw_loss):\n",
    "        self.msa_loss = self.beta * self.msa_loss + (1-self.beta) * raw_loss\n",
    "        self.losses.append(self.msa_loss / (1 - self.beta ** (len(self.losses)+1)))\n",
    "        stop = False\n",
    "        return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinderCallback(LossRecorder):\n",
    "    def __init__(self, opt, n_iter, start_lr, end_lr):\n",
    "        self.sched = HyperParamScheduler(opt, 'lr', n_iter, annealing_exp, start_lr, end_lr)\n",
    "        super().__init__()\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.min_loss = 0\n",
    "        super().on_train_begin()\n",
    "    \n",
    "    def on_batch_end(self, raw_loss):\n",
    "        super().on_batch_end(raw_loss)\n",
    "        loss = self.losses[-1]\n",
    "        if len(self.losses) == 1 or loss < self.min_loss: self.min_loss = loss\n",
    "        stop = self.sched.step()\n",
    "        if loss > 4 * self.min_loss: return True\n",
    "        return stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None):\n",
    "    loss = loss_fn(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl=None, callbacks=[]):\n",
    "    for cb in callbacks: cb.on_train_begin()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            for cb in callbacks: cb.on_batch_begin()\n",
    "            raw_loss,_ = loss_batch(model, xb, yb, loss_fn, opt)\n",
    "            if np.any([cb.on_batch_end(raw_loss) for cb in callbacks]): break  \n",
    "                \n",
    "        model.eval()\n",
    "        if valid_dl is not None:\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, xb, yb, loss_fn)\n",
    "                                for xb,yb in valid_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "\n",
    "            print(epoch, val_loss)\n",
    "            \n",
    "    for cb in callbacks: cb.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn = F.nll_loss\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD):\n",
    "        opt = opt_fn(self.model.parameters(), lr=lr)\n",
    "        fit(epochs, self.model, self.loss_fn, opt, self.data.train_dl, self.data.valid_dl)\n",
    "        \n",
    "    def lr_find(self, opt_fn=optim.SGD, n_iter=200, start_lr=1e-5, end_lr=10):\n",
    "        opt = opt_fn(self.model.parameters(), lr=start_lr)\n",
    "        save_model(self.model, MODEL_PATH/'tmp.pt')\n",
    "    \n",
    "        cb = LRFinderCallback(opt, n_iter, start_lr, end_lr)\n",
    "        epochs = int(np.ceil(n_iter/len(train_dl)))\n",
    "        fit(epochs, self.model, self.loss_fn, opt, self.data.train_dl, callbacks=[cb])\n",
    "    \n",
    "        load_model(self.model, MODEL_PATH/'tmp.pt')\n",
    "        return cb.sched.vals, cb.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.from_arrays(x_train,y_train,x_valid,y_valid, x_tfms=mnist2image)\n",
    "model = Simple_CNN([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learner = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs, losses = learner.lr_find()\n",
    "plot_lr(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training phases and SGDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few helper functions. The second one is there to map the names of the hyper-parameters in pytorch optimizers to 'momentum' and 'beta' (whether it's SGD, Adam, RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tuple(x): return isinstance(x, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_val_in_group(val, group, key):\n",
    "    if key == 'momentum' and 'betas' in group:  group['betas'] = (val, group['betas'][1])\n",
    "    elif key == 'beta' and 'betas' in group:    group['betas'] = (group['betas'][0], val)\n",
    "    elif key == 'beta' and 'alpha' in group:    group['alpha'] = val\n",
    "    else:                                       group[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_val_in_groups(val, groups, key):\n",
    "    for g in groups: set_val_in_group(val, g, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the use of training phase, the args lr and mom can either be:\n",
    "- a single value, then it's assumed to have it constant during the phase\n",
    "- a tuple (lr1, lr2), then the default is to go linearly from one to the other\n",
    "- a tuple (lr1, lr2, VarType), then we go from one to the other using the VarType given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPhase():\n",
    "    \n",
    "    def __init__(self, epochs, opt_fn, hp_dict):\n",
    "        self.epochs,self.opt_fn,self.beta,self.wds = epochs,opt_fn,beta,wds\n",
    "        self.start_lr, self.end_lr, self.lr_vType, self.extra_lr = self.decode(lr)\n",
    "        self.start_mom, self.end_mom, self.mom_vType, self.extra_mom = self.decode(mom)\n",
    "        \n",
    "    def decode(self, vals):\n",
    "        if is_tuple(vals):\n",
    "            if len(vals) == 2: return vals + (VarType.LINEAR, None)\n",
    "            elif len(vals) == 3: return vals + (None,)\n",
    "            else: return vals\n",
    "        else: return vals, None, VarType.NO, None\n",
    "    \n",
    "    def get_scheds(self, opt, n_batch):\n",
    "        if self.beta is not None: set_val_in_groups(self.beta, opt.param_groups, 'beta')\n",
    "        if self.wds is not None: set_val_in_groups(self.wds, opt.param_groups, 'weight_decay')\n",
    "        return [Scheduler(opt, 'lr', n_batch * self.epochs, self.lr_vType, self.start_lr, self.end_lr, self.extra_lr),\n",
    "               Scheduler(opt, 'momentum', n_batch * self.epochs, self.mom_vType, self.start_mom, self.end_mom, self.extra_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 'lr'\n",
    "MOM = 'momentum'\n",
    "WD = 'weight_decay'\n",
    "BETA = 'beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPhase1():\n",
    "    \n",
    "    def __init__(self, epochs, opt_fn, **kwargs):\n",
    "        self.epochs,self.opt_fn = epochs,opt_fn\n",
    "        assert 'LR' in kwargs, 'You need to set a learning rate.'\n",
    "        self.hp_behavior = {self.rename(n):self.decode(v) for n,v in kwargs.items()}\n",
    "        self.start_lr = self.hp_behavior[LR][0]\n",
    "        \n",
    "    def decode(self, vals):\n",
    "        if is_tuple(vals):\n",
    "            if len(vals) == 2: return vals + (annealing_linear, None)\n",
    "            elif len(vals) == 3: return vals + (None,)\n",
    "            else: return vals\n",
    "        else: return vals, None, annealing_no, None\n",
    "        \n",
    "    def rename(self, name):\n",
    "        match = {'LR':LR, 'MOM':MOM, 'WD':WD, 'BETA':BETA}\n",
    "        return match[name] if name in match else name\n",
    "    \n",
    "    def get_scheds(self, opt, n_batch):\n",
    "        return [HyperParamScheduler(opt, n, int(n_batch * self.epochs), v[2], v[0], v[1], v[3]) for n,v in self.hp_behavior.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = TrainingPhase1(2, optim.SGD, LR=0.1, MOM=0.9, WD=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class that handles the phases and make sure we go from one to the next is another Callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhaseScheduler(LossRecorder):\n",
    "    \n",
    "    def __init__(self, model, phases, n_batches):\n",
    "        self.model, self.phases, self.n_batches = model, phases, n_batches\n",
    "        super().__init__()\n",
    "        self.phase = 0\n",
    "        self.opt = None\n",
    "        self.init_phase()\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        super().on_train_begin()\n",
    "        self.hp_vals = collections.defaultdict(list)\n",
    "    \n",
    "    def init_phase(self):\n",
    "        cur_phase = self.phases[self.phase]\n",
    "        #Updates the optimizer\n",
    "        #TODO: Replacing self.model.parameters() by a function that returns the trainable parameters will handle freezing.\n",
    "        new_opt = cur_phase.opt_fn(self.model.parameters(), lr=cur_phase.start_lr)\n",
    "        if self.opt is None or type(self.opt) != type(new_opt): self.opt = new_opt\n",
    "        #Creates the various schedulers\n",
    "        self.scheds = cur_phase.get_scheds(self.opt, self.n_batches)\n",
    "        \n",
    "    def on_batch_end(self, raw_loss):\n",
    "        super().on_batch_end(raw_loss)\n",
    "        finished = False\n",
    "        for sched in self.scheds: finished = finished or sched.step()\n",
    "        if finished:\n",
    "            for sched in self.scheds: self.hp_vals[sched.hyper_param] += sched.vals\n",
    "            self.phase += 1\n",
    "            if self.phase == len(self.phases): return True\n",
    "            self.init_phase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting with phases is then super easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_phases(phases, model, loss_fn, train_dl, valid_dl=None, callbacks=[]):\n",
    "    epochs = int(np.ceil(sum([p.epochs for p in phases])))\n",
    "    cb = PhaseScheduler(model, phases, len(train_dl))\n",
    "    fit(epochs, model, loss_fn, cb.opt, train_dl, valid_dl, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of what a helper function could be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdr_phases(lr, opt_fn, n_cycle, cyc_len, cyc_mul, mom=0.9, wds=None):\n",
    "    phases = [TrainingPhase(0.05, opt_fn, lr/100, mom, wds)]\n",
    "    for i in range(n_cycle):\n",
    "        epochs = cyc_len - 0.05 if i==0 else cyc_len * (cyc_mul ** i)\n",
    "        phases.append(TrainingPhase(epochs, opt_fn, (lr,0,VarType.COSINE), mom, wds))\n",
    "    return phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdr_phases1(lr, opt_fn, n_cycle, cyc_len, cyc_mul, mom=0.9, wds=None):\n",
    "    phases = [TrainingPhase1(0.05, opt_fn, LR=lr/100, MOM=mom, WD=wds)]\n",
    "    for i in range(n_cycle):\n",
    "        epochs = cyc_len - 0.05 if i==0 else cyc_len * (cyc_mul ** i)\n",
    "        phases.append(TrainingPhase1(epochs, opt_fn, LR=(lr,0,annealing_cos), MOM=mom, WD=wds))\n",
    "    return phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, data, model):\n",
    "        self.data,self.model = data,model.to(data.device)\n",
    "        self.loss_fn = F.nll_loss\n",
    "\n",
    "    def fit(self, epochs, lr, opt_fn=optim.SGD):\n",
    "        opt = opt_fn(self.model.parameters(), lr=lr)\n",
    "        fit(epochs, self.model, self.loss_fn, opt, self.data.train_dl, self.data.valid_dl)\n",
    "    \n",
    "    def fit_phases(self, phases, model, loss_fn, train_dl, valid_dl=None, callbacks=[]):\n",
    "        epochs = int(np.ceil(sum([p.epochs for p in phases])))\n",
    "        cb = PhaseScheduler(model, phases, len(train_dl))\n",
    "        fit(epochs, model, loss_fn, cb.opt, train_dl, valid_dl, callbacks=[cb])\n",
    "        self.hp_vals = cb.hp_vals\n",
    "        \n",
    "    def lr_find(self, opt_fn=optim.SGD, n_iter=200, start_lr=1e-5, end_lr=10):\n",
    "        opt = opt_fn(self.model.parameters(), lr=start_lr)\n",
    "        save_model(self.model, MODEL_PATH/'tmp.pt')\n",
    "    \n",
    "        cb = LRFinderCallback(opt, n_iter, start_lr, end_lr)\n",
    "        epochs = int(np.ceil(n_iter/len(train_dl)))\n",
    "        fit(epochs, self.model, self.loss_fn, opt, self.data.train_dl, callbacks=[cb])\n",
    "    \n",
    "        load_model(self.model, MODEL_PATH/'tmp.pt')\n",
    "        return cb.sched.vals, cb.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.from_arrays(x_train,y_train,x_valid,y_valid, x_tfms=mnist2image)\n",
    "model = Simple_CNN([1,16,16,10], [3,3,3], [2,2,2])\n",
    "learner = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit_phases(sgdr_phases1(0.5, optim.SGD, 3, 1, 2, wds=1e-4), model, loss_fn, data.train_dl, data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, val_dl):\n",
    "    n, c = 0, 0\n",
    "    for xb,yb in val_dl:\n",
    "        probs = model(xb)\n",
    "        preds = probs.argmax(1)\n",
    "        n += (preds==yb).long().sum().item()\n",
    "        c += yb.size(0)\n",
    "    return n / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(learner.model, learner.data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner.hp_vals[LR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner.hp_vals[MOM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learner.hp_vals[WD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sylvain's transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class TfmType(IntEnum):\n",
    "    NO = 1\n",
    "    PIXEL = 2\n",
    "    COORD = 3\n",
    "    CLASS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "class Transform():\n",
    "    \n",
    "    def __init__(self, tfm_y=TfmType.NO, p=1, batch_lvl = False):\n",
    "        self.tfm_y,self.p,self.batch_lvl = tfm_y,p,batch_lvl\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        x,y = ((self.transform(x),y) if self.tfm_y==TfmType.NO\n",
    "                else self.transform(x,y) if self.tfm_y in (TfmType.PIXEL, TfmType.CLASS)\n",
    "                else self.transform_coord(x,y))\n",
    "        return x, y\n",
    "    \n",
    "    def set_device(self, device):\n",
    "        if not self.batch_lvl: self.device = device\n",
    "    \n",
    "    def transform_coord(self, x, y):\n",
    "        if self.p == 1 or np.random.rand < self.p:\n",
    "            return self.transform(x),y\n",
    "\n",
    "    def transform(self, x, y=None):\n",
    "        if self.p == 1 or np.random.rand < self.p:\n",
    "            x = self.do_transform(x,False)\n",
    "            return (x, self.do_transform(y,True)) if y is not None else x\n",
    "        else: return x,y\n",
    "    \n",
    "    @abstractmethod\n",
    "    def do_transform(self, x, is_y): raise NotImplementedError\n",
    "    #In do_transform we can save a value (angle of a random rotation for instance) in self.save_for_y that will be used\n",
    "    #if is_y is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelOrder(Transform):\n",
    "    #If we use PIL for data augmentation, maybe the conversion to a numpy array should be handled here?\n",
    "    def __init__(self, tfm_y=TfmType.NO):\n",
    "        super().__init__(tfm_y=tfm_y)\n",
    "    \n",
    "    def do_transform(self, x, is_y):\n",
    "        if not is_y or self.tfm_y == TfmType.PIXEL: x = np.rollaxis(x, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(Transform):\n",
    "    \n",
    "    def __init__(self, means, stds, tfm_y=TfmType.NO):\n",
    "        self.means,self.stds = means,stds\n",
    "        super().__init__(tfm_y=tfm_y, batch_lvl=True)\n",
    "    \n",
    "    def set_device(self, device):\n",
    "        super().set_device(device)\n",
    "        if type(self.means) != torch.Tensor or not self.means.device == device:\n",
    "            self.means,self.stds = map(lambda x:torch.Tensor(x).to(device), (self.means, self.stds))\n",
    "    \n",
    "    def do_transform(self, x, is_y):\n",
    "        if not is_y or self.tfm_y == TfmType.PIXEL:\n",
    "            m, s = self.means[None,:,None,None].type_as(x), self.stds[None,:,None,None].type_as(x)\n",
    "            x = (x - m) / s\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(tfms, x, y):\n",
    "    for tfm in tfms: x,y = tfm(x,y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_one_tfms(tfms):\n",
    "    ds_tfms = [tfm for tfm in tfms if not tfm.batch_lvl]\n",
    "    dl_tfms = [tfm for tfm in tfms if tfm.batch_lvl]\n",
    "    return ds_tfms,dl_tfms\n",
    "    \n",
    "def split_tfms(trn_tfms, val_tfms):\n",
    "    trn_ds_tfms, trn_dl_tfms = split_one_tfms(trn_tfms)\n",
    "    val_ds_tfms, val_dl_tfms = split_one_tfms(val_tfms)\n",
    "    return trn_ds_tfms, val_ds_tfms, trn_dl_tfms, val_dl_tfms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
