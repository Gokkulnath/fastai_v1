
        #################################################
        ### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
        #################################################

from nb_004 import *

def to_half(b):  return [b[0].half(), b[1]]

#For some reason DeviceDataLoader doesn't get overriden with no change of name, need to clean this up later.
@dataclass
class DeviceDataLoader1():
    dl: DataLoader
    device: torch.device
    progress_func: Callable
    half: bool = False
        
    def __len__(self): return len(self.dl)
    def __iter__(self):
        self.gen = (to_device(self.device,o) for o in self.dl)
        if self.half: self.gen = (to_half(o) for o in self.gen)
        if self.progress_func is not None:
            self.gen = self.progress_func(self.gen, total=len(self.dl), leave=False)
        return iter(self.gen)

    @classmethod
    def create(cls, *args, device=default_device, progress_func=tqdm, **kwargs):
        return cls(DataLoader(*args, **kwargs), device=device, progress_func=progress_func, half=False)

#Same for Databunch
class DataBunch1():
    def __init__(self, train_ds, valid_ds, bs=64, device=None, num_workers=4):
        self.device = default_device if device is None else device
        self.train_dl = DeviceDataLoader1.create(train_ds, bs, shuffle=True, num_workers=num_workers)
        self.valid_dl = DeviceDataLoader1.create(valid_ds, bs*2, shuffle=False, num_workers=num_workers)

    @classmethod
    def create(cls, train_ds, valid_ds, bs=64, train_tfm=None, valid_tfm=None, **kwargs):
        return cls(TfmDataset(train_ds, train_tfm), TfmDataset(valid_ds, valid_tfm), **kwargs)
        
    @property
    def train_ds(self): return self.train_dl.dl.dataset
    @property
    def valid_ds(self): return self.valid_dl.dl.dataset

def bn2float(module):
    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module.float()
    for child in module.children(): bn2float(child)
    return module

def model2half(model):
    """
    Converts the model to half precision except the batchnorm layers.
    """
    model = model.half()
    return bn2float(model)

from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors

def get_master(model, flat_master=False):
    """
    Returns two lists, one for the model parameters in FP16 and one for the master parameters in FP32
    """
    model_params = [param for param in model.parameters() if param.requires_grad]
    if flat_master:
        #Flattens all the parameters in one big tensor.
        master_params = _flatten_dense_tensors([param.data.float() for param in model_params])
        master_params = torch.nn.Parameter(master_params)
        master_params.requires_grad = True
        if master_params.grad is None: master_params.grad = master_params.new(*master_params.size())
        return model_params, [master_params]
    else:
        master_params = [param.clone().float().detach() for param in model_params]
        for param in master_params: param.requires_grad = True
        return model_params, master_params

def model_g2master_g(model_params, master_params, flat_master=False):
    """
    Copies the model gradients to the master parameters for the optimizer step.
    """
    if flat_master:
        master_params[0].grad.data.copy_(_flatten_dense_tensors([p.grad.data.float() for p in model_params]))
    else:
        for model, master in zip(model_params, master_params):
            if model.grad is not None:
                if master.grad is None: master.grad = master.data.new(*master.data.size())
                master.grad.data.copy_(model.grad.data)
            else: master.grad = None

def master2model(model_params, master_params, flat_master=False):
    """
    Copy master parameters to model parameters.
    """
    if flat_master:
        for model, master in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)):
            model.data.copy_(master)
    else:
        for model, master in zip(model_params, master_params):
            model.data.copy_(master.data)

class MixedPrecision(Callback):
    
    def __init__(self, learn, loss_scale=512, flat_master=False):
        self.learn,self.loss_scale,self.flat_master = learn,loss_scale, flat_master
    
    def on_train_begin(self, **kwargs):
        #Insures the dataloaders are in half precision.
        self.learn.data.train_dl.half = True
        if hasattr(self.learn.data, 'valid_dl') and self.learn.data.valid_dl is not None:
            self.learn.data.valid_dl.half = True
        #Get a copy of the model params in FP32
        self.model_params, self.master_params = get_master(self.learn.model, self.flat_master)
        #Changes the optimizer so that the optimization step is done in FP32.
        opt = self.learn.opt
        mom,wd,beta = opt.mom,opt.wd,opt.beta
        self.learn.opt.opt = self.learn.opt_fn(self.master_params, self.learn.opt.lr)
        opt.mom,opt.wd,opt.beta = mom,wd,beta
    
    def on_loss_begin(self, last_output, **kwargs):
        #It's better to compute the loss in FP32, to avoid reduction overflow.
        return last_output.float()
    
    def on_backward_begin(self, last_loss, **kwargs):
        #To avoid gradient underflow, we scale the gradients
        return last_loss * self.loss_scale
    
    def on_backward_end(self, **kwargs):
        #Convert the gradients back to FP32 and divide them by the scale.
        model_g2master_g(self.model_params, self.master_params, self.flat_master)
        for param in self.master_params: param.grad.div_(self.loss_scale)
        #Zeros the gradients of the model since the optimizer is disconnected.
        self.learn.model.zero_grad()
    
    def on_step_end(self, **kwargs):
        #Update the params from master to model.
        master2model(self.model_params, self.master_params, self.flat_master)