{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_007a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been prepared in csv files at the beginning 007a, we will use it know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/aclImdb/')\n",
    "CLAS_PATH = PATH/'clas'\n",
    "LM_PATH = PATH/'lm'\n",
    "MODEL_PATH = PATH/'models'\n",
    "os.makedirs(CLAS_PATH, exist_ok=True)\n",
    "os.makedirs(LM_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(rules=rules, special_cases=[BOS, FLD, UNK, PAD])\n",
    "train_ds, valid_ds = TextDataset.from_csv(LM_PATH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 100,70\n",
    "train_dl = LanguageModelLoader(np.concatenate(train_ds.ids), bs, bptt)\n",
    "valid_dl = LanguageModelLoader(np.concatenate(valid_ds.ids), bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt the pre-trained weights to the new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pretrained model and the corresponding itos dictionary here and put them in the MODEL_PATH folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(itos, tok1, tok2):\n",
    "    itos[itos.index(tok1)] = tok2\n",
    "    return itos\n",
    "\n",
    "def apply_new_flags():\n",
    "    \"Temporary function to change the old special tokens by the new ones\"\n",
    "    itos_wt = pickle.load(open(MODEL_PATH/'itos.pkl', 'rb'))\n",
    "    olds = ['_unk_', '_pad_', 'xbos', 'xfld', 'u_n', 't_up', 'tk_rep', 'tk_wrep']\n",
    "    news = [UNK, PAD, BOS, FLD, UNK, TOK_UP, TK_REP, TK_WREP]\n",
    "    for tok1,tok2 in zip(olds, news):\n",
    "        itos_wt = replace(itos_wt, tok1, tok2)\n",
    "    pickle.dump(itos_wt, open(MODEL_PATH/'itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply_new_flags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_wt = pickle.load(open(MODEL_PATH/'itos.pkl', 'rb'))\n",
    "stoi_wt = {v:k for k,v in enumerate(itos_wt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weights(wgts, stoi_wgts, itos_new):\n",
    "    dec_bias, enc_wgts = wgts['1.decoder.bias'], wgts['0.encoder.weight']\n",
    "    bias_m, wgts_m = dec_bias.mean(0), enc_wgts.mean(0)\n",
    "    new_w = enc_wgts.new_zeros((len(itos_new),enc_wgts.size(1))).zero_()\n",
    "    new_b = dec_bias.new_zeros((len(itos_new),)).zero_()\n",
    "    for i,w in enumerate(itos_new):\n",
    "        r = stoi_wgts[w] if w in stoi_wgts else -1\n",
    "        new_w[i] = enc_wgts[r] if r>=0 else wgts_m\n",
    "        new_b[i] = dec_bias[r] if r>=0 else bias_m\n",
    "    wgts['0.encoder.weight'] = new_w\n",
    "    wgts['0.encoder_dp.emb.weight'] = new_w.clone()\n",
    "    wgts['1.decoder.weight'] = new_w.clone()\n",
    "    wgts['1.decoder.bias'] = new_b\n",
    "    return wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = torch.load(MODEL_PATH/'lstm.pth', map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['1.decoder.bias'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos_wt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = convert_weights(wgts, stoi_wt, train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts['1.decoder.bias'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_data.itos)\n",
    "emb_sz,nh,nl = 400,1150,3\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_language_model(vocab_size, emb_sz, nh, nl, 0, input_p=dps[0], output_p=dps[1], weight_p=dps[2], \n",
    "                           embed_p=dps[3], hidden_p=dps[4])\n",
    "model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation in different groups for discriminitative lr and gradual unfreezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)] \n",
    "groups.append(nn.Sequential(model[0].encoder, model[0].encoder_dp, model[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model)\n",
    "learn.layer_groups = groups\n",
    "learn.callbacks.append(RNNTrainer(learn, bptt, alpha=2, beta=1))\n",
    "learn.metrics = [accuracy]\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7), wd=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_head')\n",
    "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7), wd=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
