{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_006 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carvana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/carvana')\n",
    "PATH_PNG = PATH/'train_masks_png'\n",
    "PATH_X_FULL = PATH/'train'\n",
    "PATH_X_128 = PATH/'train-128'\n",
    "PATH_Y_FULL = PATH_PNG\n",
    "PATH_Y_128 = PATH/'train_masks-128'\n",
    "\n",
    "# start with the 128x128 images\n",
    "PATH_X = PATH_X_128\n",
    "PATH_Y = PATH_Y_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_fn(x_fn): return PATH_Y/f'{x_fn.name[:-4]}_mask.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(path):\n",
    "    x_fns = [o for o in path.iterdir() if o.is_file()]\n",
    "    y_fns = [get_y_fn(o) for o in x_fns]\n",
    "    val_idxs = list(range(1008))\n",
    "    ((val_x,trn_x),(val_y,trn_y)) = split_arrs(val_idxs, x_fns, y_fns)\n",
    "    return (MatchedFilesDataset(trn_x, trn_y),\n",
    "            MatchedFilesDataset(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = get_datasets(PATH_X_128)\n",
    "train_ds,valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfm_datasets(size):\n",
    "    datasets = get_datasets(PATH_X_128 if size<=128 else PATH_X_FULL)\n",
    "    tfms = get_transforms(do_flip=True, max_rotate=4, max_lighting=0.2)\n",
    "    return transform_datasets(train_ds, valid_ds, tfms, tfm_y=True, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_norm,default_denorm = normalize_funcs(*imagenet_stats)\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(size, bs):\n",
    "    return DataBunch.create(*get_tfm_datasets(size), bs=bs, tfms=default_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(size, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sfs_idxs(sfs, last=True):\n",
    "    if last:\n",
    "        feature_szs = [sfs_feats.features.size()[-1] for sfs_feats in sfs]\n",
    "        sfs_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n",
    "        if feature_szs[0] != feature_szs[1]: sfs_idxs = [0] + sfs_idxs\n",
    "    else: sfs_idxs = list(range(len(sfs)))\n",
    "    return sfs_idxs\n",
    "\n",
    "def conv_bn_relu(in_c, out_c, kernel_size, stride, padding):\n",
    "    return [\n",
    "        nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(out_c)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in_c, x_in_c):\n",
    "        super().__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(up_in_c, up_in_c // 2, 2, 2) # H, W -> 2H, 2W\n",
    "        self.conv1 = nn.Conv2d(x_in_c + up_in_c // 2, (x_in_c + up_in_c // 2) // 2, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d((x_in_c + up_in_c // 2) // 2, (x_in_c + up_in_c // 2) // 2, 3, 1, 1)\n",
    "        self.bn = nn.BatchNorm2d((x_in_c + up_in_c // 2) // 2)\n",
    "\n",
    "    def forward(self, up_in, x_in):\n",
    "        up_out = self.upconv(up_in)\n",
    "        cat_x = torch.cat([up_out, x_in], dim=1)\n",
    "        x = F.relu(self.conv1(cat_x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return self.bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    \"\"\" Extract pretrained activations\"\"\"\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output.detach()\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicUnet(nn.Module):\n",
    "    def __init__(self, encoder, last=True, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.n_children = len(list(encoder.children()))\n",
    "        self.sfs = [SaveFeatures(encoder[i]) for i in range(self.n_children)]\n",
    "        self.last = last\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        dtype = x.type()\n",
    "        imsize = x.shape[-2:]\n",
    "        x = F.relu(self.encoder(x))\n",
    "\n",
    "        # initialize sfs_idxs, sfs_szs, middle_in_c and middle_conv only once\n",
    "        if not hasattr(self, 'middle_conv'):\n",
    "            self.sfs_szs = [sfs_feats.features.size() for sfs_feats in self.sfs]\n",
    "            self.sfs_idxs = get_sfs_idxs(self.sfs, self.last)\n",
    "            middle_in_c = self.sfs_szs[-1][1]\n",
    "            self.middle_conv = nn.Sequential(*conv_bn_relu(middle_in_c, middle_in_c * 2, 3, 1, 1),\n",
    "                *conv_bn_relu(middle_in_c * 2, middle_in_c, 3, 1, 1)).type(dtype)\n",
    "\n",
    "        x = self.middle_conv(x)\n",
    "\n",
    "        # initialize upmodel, extra_block and 1x1 final conv\n",
    "        if not hasattr(self, 'upmodel'):\n",
    "            x_copy = x.detach()\n",
    "            upmodel = []\n",
    "            for idx in self.sfs_idxs[::-1]:\n",
    "                up_in_c, x_in_c = int(x_copy.size()[1]), int(self.sfs_szs[idx][1])\n",
    "                unet_block = UnetBlock(up_in_c, x_in_c).type(dtype)\n",
    "                upmodel.append(unet_block)\n",
    "                x_copy = unet_block(x_copy, self.sfs[idx].features)\n",
    "                self.upmodel = nn.Sequential(*upmodel)\n",
    "\n",
    "            if imsize != self.sfs_szs[0][-2:]:\n",
    "                extra_in_c = self.upmodel[-1].conv2.out_channels\n",
    "                self.extra_block = nn.ConvTranspose2d(extra_in_c, extra_in_c, 2, 2).type(dtype)\n",
    "\n",
    "            final_in_c = self.upmodel[-1].conv2.out_channels\n",
    "            self.final_conv = nn.Conv2d(final_in_c, self.n_classes, 1).type(dtype)\n",
    "\n",
    "        # run upsample\n",
    "        for block, idx in zip(self.upmodel, self.sfs_idxs[::-1]):\n",
    "            x = block(x, self.sfs[idx].features)\n",
    "        if hasattr(self, 'extra_block'): x = self.extra_block(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicUnet(nn.Module):\n",
    "    def __init__(self, encoder, last=True, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.n_children = len(list(encoder.children()))\n",
    "        self.sfs = [SaveFeatures(encoder[i]) for i in range(self.n_children)]\n",
    "        self.last = last\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        x = torch.FloatTensor()\n",
    "        sfs_szs = [sfs_feats.features.size() for sfs_feats in self.sfs]\n",
    "        self.sfs_idxs = get_sfs_idxs(self.sfs, self.last)\n",
    "        middle_in_c = sfs_szs[-1][1]\n",
    "        self.middle_conv = nn.Sequential(*conv_bn_relu(middle_in_c, middle_in_c * 2, 3, 1, 1),\n",
    "            *conv_bn_relu(middle_in_c * 2, middle_in_c, 3, 1, 1)).type(dtype)\n",
    "\n",
    "        x_copy = x.detach()\n",
    "        upmodel = []\n",
    "        for idx in self.sfs_idxs[::-1]:\n",
    "            up_in_c, x_in_c = int(x_copy.size()[1]), int(self.sfs_szs[idx][1])\n",
    "            unet_block = UnetBlock(up_in_c, x_in_c).type(dtype)\n",
    "            upmodel.append(unet_block)\n",
    "            x_copy = unet_block(x_copy, self.sfs[idx].features)\n",
    "            self.upmodel = nn.Sequential(*upmodel)\n",
    "\n",
    "        if imsize != self.sfs_szs[0][-2:]:\n",
    "            extra_in_c = self.upmodel[-1].conv2.out_channels\n",
    "            self.extra_block = nn.ConvTranspose2d(extra_in_c, extra_in_c, 2, 2).type(dtype)\n",
    "\n",
    "        final_in_c = self.upmodel[-1].conv2.out_channels\n",
    "        self.final_conv = nn.Conv2d(final_in_c, self.n_classes, 1).type(dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dtype = x.type()\n",
    "        imsize = x.shape[-2:]\n",
    "        x = F.relu(self.encoder(x))\n",
    "\n",
    "        x = self.middle_conv(x)\n",
    "\n",
    "        # run upsample\n",
    "        for block, idx in zip(self.upmodel, self.sfs_idxs[::-1]):\n",
    "            x = block(x, self.sfs[idx].features)\n",
    "        if hasattr(self, 'extra_block'): x = self.extra_block(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=list(body.children())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_details(m):\n",
    "    for l in flatten_model(m):\n",
    "        if hasattr(l, 'weight'): return l.weight.type(),l.weight.shape[1]\n",
    "    raise Exception('No weight layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_in,ch_in = in_details(body)\n",
    "ch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = FloatTensor(1,ch_in,256,256).type(type_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[accuracy_thresh,dice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = create_body(tvm.resnet34(True), 2)\n",
    "model = DynamicUnet(body, n_classes=1).cuda()\n",
    "learn = Learner(data, model, metrics=metrics,\n",
    "                loss_fn=F.binary_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.valid_dl))\n",
    "t = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,py = learn.pred_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ax in enumerate(plt.subplots(4,4,figsize=(10,10))[1].flat):\n",
    "    show_image(default_denorm(x[i].cpu()), py[i]>0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=512\n",
    "bs = 8\n",
    "learn.data = get_data(size, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
