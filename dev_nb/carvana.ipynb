{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, PIL.Image, matplotlib.pyplot as plt, itertools, math, random, collections, torch, inspect\n",
    "from functools import wraps, partial\n",
    "from dataclasses import dataclass, field\n",
    "import inspect\n",
    "import random\n",
    "from copy import copy, deepcopy\n",
    "from typing import Dict, Any, AnyStr, List, Sequence, TypeVar, Tuple, Optional, Union, Iterable, cast, Collection, Callable\n",
    "from numpy import cos, sin, tan, tanh, log, exp\n",
    "from pdb import set_trace\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, Tensor, FloatTensor, LongTensor, ByteTensor, DoubleTensor, HalfTensor, ShortTensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "import scipy.stats, scipy.special\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import numpy as np\n",
    "from torchvision.models import resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm = tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_args(func):\n",
    "    return {k: v.default\n",
    "            for k, v in inspect.signature(func).parameters.items()\n",
    "            if v.default is not inspect.Parameter.empty}\n",
    "\n",
    "def get_arg_names(func):\n",
    "    return list(inspect.signature(func).parameters)\n",
    "\n",
    "def get_dynamic_var_args(func):\n",
    "    return {k: v.default\n",
    "            for k, v in inspect.signature(func).parameters.items()}\n",
    "\n",
    "def bind_args(func, v):\n",
    "    arg_names = get_arg_names(func)\n",
    "    bound_args = { arg_names[i]: vi for i, vi in enumerate(v)}\n",
    "    return bound_args\n",
    "        \n",
    "\n",
    "def resolve_fun_annotations(func, kwargs):\n",
    "    params = copy(func.__annotations__)\n",
    "    resolved = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if k in params and not isinstance(v, Dynamic):\n",
    "            rand_func = params[k]\n",
    "            if isinstance(v, Dict): resolved[k] = rand_func(**v)\n",
    "            elif isinstance(v, Iterable):\n",
    "                arg_names = get_arg_names(rand_func)\n",
    "                bound_args = bind_args(rand_func, v)\n",
    "                resolved[k] = rand_func(**bound_args)\n",
    "            else: \n",
    "                # gross hack\n",
    "                if rand_func == Bool and k == 'p':\n",
    "                    resolved[k] = rand_func(v)\n",
    "                else:\n",
    "                    resolved[k] = v\n",
    "        else: resolved[k] = v\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def dynamic_resolve(a):\n",
    "    if isinstance(a, Dynamic): return a.resolve()\n",
    "    else: return a\n",
    "\n",
    "def dynamic_release(a):\n",
    "    if isinstance(a, Dynamic): return a.release()\n",
    "    elif isinstance(a, list):\n",
    "        for ai in a: dynamic_release(ai)\n",
    "    elif isinstance(a, dict):\n",
    "        for vi in a.values(): dynamic_release(vi)\n",
    "            \n",
    "class Dynamic(object):\n",
    "    def __init__(self, func=None, args=None, kwargs=None):\n",
    "        self.func = func       \n",
    "        self.args = args\n",
    "        self.kwargs = resolve_fun_annotations(self.func, kwargs) if kwargs else {}\n",
    "        self.def_args = get_default_args(self.func) \n",
    "        self.init_state()\n",
    "        \n",
    "    def init_state(self):\n",
    "        self.value_ = None\n",
    "        self.bound = False\n",
    "        self.bound_args = {}\n",
    "        self.bound_kwargs = {}\n",
    "\n",
    "    def clone(self, **kwargs):\n",
    "        copy_kwargs = {k:copy(v) for k,v in self.kwargs.items()}\n",
    "        copy_args = copy(self.args)\n",
    "        kwargs = {**copy_kwargs, **kwargs}\n",
    "        dtype = type(self)\n",
    "        new_dynamic = dtype(func=self.func, args=copy_args, kwargs=kwargs)\n",
    "        new_dynamic.__dict__.update({k:v for k,v in self.__dict__.items()\n",
    "                                     if k not in new_dynamic.__dict__})\n",
    "        return new_dynamic\n",
    "    \n",
    "    def __copy__(self):\n",
    "        return self.clone()\n",
    "    \n",
    "    def bind(self, **kwargs):\n",
    "        if not self.bound:\n",
    "            kwargs = resolve_fun_annotations(self.func, kwargs) \n",
    "            self.bound_args = bind_args(self.func, self.args) if self.args else {}\n",
    "            kwargs = {**self.def_args, **self.kwargs, **self.bound_args, **kwargs}\n",
    "            kwargs = {k:dynamic_resolve(v) for k,v in kwargs.items()}\n",
    "            self.bound = True\n",
    "            self.bound_kwargs = kwargs\n",
    "            self.value = None\n",
    "        elif kwargs:\n",
    "            kwargs = resolve_fun_annotations(self.func, kwargs)\n",
    "            kwargs = {k:dynamic_resolve(v) for k,v in kwargs.items()}\n",
    "            self.bound_kwargs = {**self.bound_kwargs, **self.bound_args, **kwargs}\n",
    "            self.value = None\n",
    "                   \n",
    "    def release(self):\n",
    "        dynamic_release(self.kwargs)\n",
    "        dynamic_release(self.bound_args)\n",
    "        dynamic_release(self.def_args)\n",
    "        dynamic_release(self.func)\n",
    "        self.init_state()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.func.__name__}:{self.kwargs}:{self.value_}'\n",
    "    \n",
    "                \n",
    "class DynamicVar(Dynamic):      \n",
    "    def __init__(self, func=None, args=None, kwargs=None):\n",
    "        super().__init__(func=func, args=args, kwargs=kwargs)\n",
    "        \n",
    "    def resolve(self):\n",
    "        self.bind()\n",
    "        if self.value_ is None:\n",
    "            self.value_ = self.func(**self.bound_kwargs)\n",
    "        return self.value_\n",
    "\n",
    "\n",
    "class DynamicFunc(Dynamic):      \n",
    "    def __init__(self, func=None, args=None, kwargs=None):\n",
    "        assert(args is None)\n",
    "        super().__init__(func=func, args=args, kwargs=kwargs)\n",
    "        \n",
    "    def resolve(self):\n",
    "        self.bind()\n",
    "        return self\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.bind(**kwargs)\n",
    "        return self.func(*args, **self.bound_kwargs)  \n",
    "\n",
    "\n",
    "def dynamic_var(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return DynamicVar(func=func, args=args, kwargs=kwargs) \n",
    "    return wrapper\n",
    "\n",
    "def dynamic_func(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        f = DynamicFunc(func=func, kwargs=kwargs)\n",
    "        if args:return f(*args)\n",
    "        else: return f\n",
    "    return wrapper\n",
    "\n",
    "def uniform(low, high, size=None):\n",
    "    return random.uniform(low,high) if size is None else torch.FloatTensor(size).uniform_(low,high)\n",
    "\n",
    "@dynamic_var\n",
    "def Uniform(low, high, size=None):\n",
    "    value = uniform(low, high, size=size)\n",
    "    return value\n",
    "\n",
    "@dynamic_var\n",
    "def LogUniform(low, high, size=None):\n",
    "    res = uniform(log(low), log(high), size)\n",
    "    value = exp(res) if size is None else res.exp_()\n",
    "    return value\n",
    "\n",
    "@dynamic_var\n",
    "def Bool(p, size=None):\n",
    "    return uniform(0, 1, size=size)<p\n",
    "\n",
    "@dynamic_var\n",
    "def Const(c):\n",
    "    return c\n",
    "\n",
    "@dynamic_var\n",
    "def Choice(choices):\n",
    "    return random.choice(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dynamic_func\n",
    "def maybe(*args,f=None, p:Bool=False, **kwargs):\n",
    "    #print('maybe',p,f)\n",
    "    if p: return f(*args, **kwargs)\n",
    "    else: return args[0]\n",
    "    \n",
    "\n",
    "@dynamic_func\n",
    "def img_wrap(img, f=None, wrap=None):\n",
    "    return getattr(img, wrap)(f)\n",
    "    \n",
    "# sticks a p variable on a function\n",
    "def Transform(mfunc, order=0, wrap=None):\n",
    "    dmfunc = dynamic_func(mfunc)\n",
    "    tfm_type = wrap if wrap else 'unknown'\n",
    "    @wraps(dmfunc)\n",
    "    def make_trans(*args,p:Bool=True, **kwargs):\n",
    "        m = dmfunc(**kwargs)\n",
    "        if wrap: m = img_wrap(f=m, wrap=wrap)\n",
    "        res = maybe(*args, f=m, p=p)\n",
    "        res.order = order\n",
    "        res.tfm_type = tfm_type\n",
    "        return res\n",
    "    make_trans.order = order\n",
    "    make_trans.tfm_type = tfm_type\n",
    "    return make_trans\n",
    "\n",
    "TfmLighting = partial(Transform, order=9, wrap='lighting')\n",
    "TfmResize = partial(Transform, order=3)\n",
    "TfmAffine = partial(Transform, order=5, wrap='affine')\n",
    "TfmPixel = partial(Transform, order=10, wrap='pixel')\n",
    "TfmCoord = partial(Transform, order=4, wrap='coord')\n",
    "TfmPad = partial(Transform, order=-10, wrap='pixel')\n",
    "TfmCrop = partial(Transform, order=99)\n",
    "\n",
    "def is_lighting(tfm): tfm.tfm_type == 'lighting'\n",
    "def is_affine(tfm): tfm.tfm_type == 'affine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def mm(x,by:Uniform=2,mode:Bool=True):\n",
    "    print('mode', mode)\n",
    "    return x * by\n",
    "\n",
    "m1 = mm(by=(-2,2),p=0.5)\n",
    "m1.foobar = 'fred'\n",
    "m2 = partial(m1, mode=False) \n",
    "\n",
    "for i in range(10):\n",
    "    #if (m1(2) != m2(2,mode=False)):\n",
    "    print(m1(2), m2(2,mode=False))\n",
    "    m1.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "def pil2tensor(image, as_mask=False):\n",
    "    arr = torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n",
    "    arr = arr.view(image.size[1], image.size[0], -1)\n",
    "    arr = arr.permute(2,0,1).float()\n",
    "    return arr if as_mask else arr.div_(255)\n",
    "\n",
    "def open_image(fn, as_mask=False):\n",
    "    x = PIL.Image.open(fn)\n",
    "    if not as_mask: x = x.convert('RGB')\n",
    "    return pil2tensor(x, as_mask=as_mask)\n",
    "\n",
    "def image2np(image):\n",
    "    res = image.cpu().permute(1,2,0).numpy()\n",
    "    return res[...,0] if res.shape[2]==1 else res\n",
    "\n",
    "\n",
    "\n",
    "def show_image(img, ax=None, figsize=(3,3), hide_axis=True, alpha=None):\n",
    "    if ax is None: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(image2np(img),alpha=alpha)\n",
    "    if hide_axis: ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def show_xy_image(xim, yim, ax=None, figsize=(3,3), alpha=0.5, hide_axis=True):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax1 = show_image(xim, ax=ax, hide_axis=hide_axis)\n",
    "    show_image(yim, ax=ax1, alpha=alpha,hide_axis=hide_axis)\n",
    "    if hide_axis: ax.axis('off')\n",
    "\n",
    "def show_image_batch(dl, classes, rows=None, figsize=(12,15)):\n",
    "    x,y = next(iter(dl))\n",
    "    if rows is None: rows = int(math.sqrt(len(x)))\n",
    "    show_images(x[:rows*rows],y[:rows*rows],rows, classes)\n",
    "\n",
    "def show_images(x,y,rows,figsize=(9,9)):\n",
    "    fig, axs = plt.subplots(rows,rows,figsize=figsize)\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        show_xy_image(x[i], y[i], ax)\n",
    "        #show_image(x[i], ax)\n",
    "        #ax.set_title(classes[y[i]])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "class Image():\n",
    "    def __init__(self, px):\n",
    "        self._px = px\n",
    "        self._logit_px=None\n",
    "        self._flow=None\n",
    "        self._affine_mat=None\n",
    "        self.sample_kwargs = {}\n",
    "\n",
    "    @property\n",
    "    def shape(self): return self._px.shape\n",
    "    \n",
    "    def __repr__(self): return f'{self.__class__.__name__} ({self.px.shape})'\n",
    "\n",
    "    def refresh(self):\n",
    "        if self._logit_px is not None:\n",
    "            self._px = self._logit_px.sigmoid_()\n",
    "            self._logit_px = None\n",
    "        if self._affine_mat is not None or self._flow is not None:\n",
    "            #print('sample', self.sample_kwargs)\n",
    "            self._px = grid_sample(self._px, self.flow, **self.sample_kwargs)\n",
    "            self.sample_kwargs = {}\n",
    "            self._flow = None\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def px(self):\n",
    "        self.refresh()\n",
    "        return self._px\n",
    "    @px.setter\n",
    "    def px(self,v): self._px=v\n",
    "\n",
    "    @property\n",
    "    def flow(self):\n",
    "        if self._flow is None:\n",
    "            self._flow = affine_grid(self.shape)\n",
    "        if self._affine_mat is not None:\n",
    "            self._flow = affine_mult(self._flow,self._affine_mat)\n",
    "            self._affine_mat = None\n",
    "        return self._flow\n",
    "    @flow.setter\n",
    "    def flow(self,v): self._flow=v\n",
    "\n",
    "    def lighting(self, func, *args, **kwargs):\n",
    "        self.logit_px = func(self.logit_px, *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def pixel(self, func, *args, **kwargs):\n",
    "        self.px = func(self.px, *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def coord(self, func, *args, **kwargs):\n",
    "        self.flow = func(self.flow, self.shape, *args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def affine(self, func, *args, **kwargs):\n",
    "        m = func(*args, **kwargs)\n",
    "        self.affine_mat = self.affine_mat @ self._px.new(m)\n",
    "        return self\n",
    "\n",
    "    def set_sample(self, **kwargs):\n",
    "        self.sample_kwargs = kwargs\n",
    "        return self\n",
    "\n",
    "    def resize(self, size):\n",
    "        assert self._flow is None\n",
    "        if isinstance(size, int): size=(self.shape[0], size, size)\n",
    "        self.flow = affine_grid(size)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def affine_mat(self):\n",
    "        if self._affine_mat is None: self._affine_mat = self._px.new(torch.eye(3))\n",
    "        return self._affine_mat\n",
    "    @affine_mat.setter\n",
    "    def affine_mat(self,v): self._affine_mat=v\n",
    "\n",
    "    @property\n",
    "    def logit_px(self):\n",
    "        if self._logit_px is None: self._logit_px = logit_(self.px)\n",
    "        return self._logit_px\n",
    "    @logit_px.setter\n",
    "    def logit_px(self,v): self._logit_px=v\n",
    "    \n",
    "    def show(self, ax=None, **kwargs): show_image(self.px, ax=ax, **kwargs)\n",
    "    def clone(self): return self.__class__(self.px.clone())\n",
    "    \n",
    "    \n",
    "def grid_sample_nearest(input, coords, padding_mode='zeros'):\n",
    "    if padding_mode=='border': coords.clamp(-1,1)\n",
    "    bs,ch,h,w = input.size()\n",
    "    sz = torch.tensor([w,h]).float()[None,None]\n",
    "    coords.add_(1).mul_(sz/2)\n",
    "    coords = coords[0].round_().long()\n",
    "    if padding_mode=='zeros':\n",
    "        mask = (coords[...,0] < 0) + (coords[...,1] < 0) + (coords[...,0] >= w) + (coords[...,1] >= h)\n",
    "        mask.clamp_(0,1)\n",
    "    coords[...,0].clamp_(0,w-1)\n",
    "    coords[...,1].clamp_(0,h-1)\n",
    "    result = input[...,coords[...,1],coords[...,0]]\n",
    "    if padding_mode=='zeros': result[...,mask] = result[...,mask].zero_()\n",
    "    return result\n",
    "\n",
    "def grid_sample(x, coords, mode='bilinear', padding_mode='reflect'):\n",
    "    if padding_mode=='reflect': padding_mode='reflection'\n",
    "    if mode=='nearest': return grid_sample_nearest(x[None], coords, padding_mode)[0]\n",
    "    return F.grid_sample(x[None], coords, mode=mode, padding_mode=padding_mode)[0]\n",
    "\n",
    "def affine_grid(size):\n",
    "    size = ((1,)+size)\n",
    "    N, C, H, W = size\n",
    "    grid = FloatTensor(N, H, W, 2)\n",
    "    linear_points = torch.linspace(-1, 1, W) if W > 1 else torch.Tensor([-1])\n",
    "    grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, :, 0])\n",
    "    linear_points = torch.linspace(-1, 1, H) if H > 1 else torch.Tensor([-1])\n",
    "    grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, :, 1])\n",
    "    return grid\n",
    "\n",
    "def affine_mult(c,m):\n",
    "    if m is None: return c\n",
    "    size = c.size()\n",
    "    c = c.view(-1,2)\n",
    "    c = torch.addmm(m[:2,2], c,  m[:2,:2].t()) \n",
    "    return c.view(size)\n",
    "\n",
    "\n",
    "def normalize(x, mean,std):   return (x-mean[...,None,None]) / std[...,None,None]\n",
    "def denormalize(x, mean,std): return x*std[...,None,None] + mean[...,None,None]\n",
    "\n",
    "def normalize_batch(b, mean, std, do_y=False):\n",
    "    x,y = b\n",
    "    x = normalize(x,mean,std)\n",
    "    if do_y: y = normalize(y,mean,std)\n",
    "    return x,y\n",
    "\n",
    "def normalize_funcs(mean, std, do_y=False, device=None):\n",
    "    if device is None: device=default_device\n",
    "    return (partial(normalize_batch, mean=mean.to(device),std=std.to(device), do_y=do_y),\n",
    "            partial(denormalize,     mean=mean,           std=std))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(p=None, q=None):\n",
    "    if p is None: p=[]\n",
    "    elif not isinstance(p, Iterable): p=[p]\n",
    "    n = q if type(q)==int else 1 if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    return p\n",
    "\n",
    "def logit(x):  return -(1/x-1).log()\n",
    "def logit_(x): return (x.reciprocal_().sub_(1)).log_().neg_()\n",
    "\n",
    "@TfmLighting\n",
    "def brightness(x, change:Uniform=1.0): return x.add_(scipy.special.logit(change))\n",
    "\n",
    "@TfmLighting\n",
    "def contrast(x, scale:LogUniform=1.0): return x.mul_(scale)\n",
    "\n",
    "@TfmAffine\n",
    "def rotate(degrees:Uniform):\n",
    "    angle = degrees * math.pi / 180\n",
    "    return [[cos(angle), -sin(angle), 0.],\n",
    "            [sin(angle),  cos(angle), 0.],\n",
    "            [0.        ,  0.        , 1.]]\n",
    "\n",
    "def get_zoom_mat(sw, sh, c, r):\n",
    "    return [[sw, 0,  c],\n",
    "            [0, sh,  r],\n",
    "            [0,  0, 1.]]\n",
    "\n",
    "@TfmAffine\n",
    "def zoom(scale:Uniform=1.0, row_pct:Uniform=0.5, col_pct:Uniform=0.5):\n",
    "    s = 1-1/scale\n",
    "    col_c = s * (2*col_pct - 1)\n",
    "    row_c = s * (2*row_pct - 1)\n",
    "    return get_zoom_mat(1/scale, 1/scale, col_c, row_c)\n",
    "\n",
    "@TfmAffine\n",
    "def squish(scale:Uniform=1.0, row_pct:Uniform=0.5, col_pct:Uniform=0.5):\n",
    "    if scale <= 1: \n",
    "        col_c = (1-scale) * (2*col_pct - 1)\n",
    "        return get_zoom_mat(scale, 1, col_c, 0.)\n",
    "    else:          \n",
    "        row_c = (1-1/scale) * (2*row_pct - 1)\n",
    "        return get_zoom_mat(1, 1/scale, 0., row_c)\n",
    "    \n",
    "@TfmCoord\n",
    "def jitter(c, size, magnitude:Uniform):\n",
    "    return c.add_((torch.rand_like(c)-0.5)*magnitude*2)\n",
    "\n",
    "@TfmPixel\n",
    "def flip_lr(x): \n",
    "    print('do flip')\n",
    "    return x.flip(2)\n",
    "\n",
    "@TfmPad\n",
    "def pad(x, padding, mode='reflect'):\n",
    "    return F.pad(x[None], (padding,)*4, mode=mode)[0]\n",
    "\n",
    "@TfmPixel\n",
    "def crop(x, size, row_pct:Uniform=0.5, col_pct:Uniform=0.5):\n",
    "    size = listify(size,2)\n",
    "    rows,cols = size\n",
    "    row = int((x.size(1)-rows+1) * row_pct)\n",
    "    col = int((x.size(2)-cols+1) * col_pct)\n",
    "    return x[:, row:row+rows, col:col+cols].contiguous()\n",
    "\n",
    "\n",
    "def compute_zs_mat(sz, scale, squish, invert, row_pct, col_pct):\n",
    "    orig_ratio = math.sqrt(sz[2]/sz[1])\n",
    "    for s,r,i in zip(scale,squish, invert):\n",
    "        s,r = math.sqrt(s),math.sqrt(r)\n",
    "        if s * r <= 1 and s / r <= 1: #Test if we are completely inside the picture\n",
    "            w,h = (s/r, s*r) if i else (s*r,s/r)\n",
    "            w /= orig_ratio\n",
    "            h *= orig_ratio\n",
    "            col_c = (1-w) * (2*col_pct - 1)\n",
    "            row_c = (1-h) * (2*row_pct - 1)\n",
    "            return get_zoom_mat(w, h, col_c, row_c)\n",
    "        \n",
    "    #Fallback, hack to emulate a center crop without cropping anything yet.\n",
    "    if orig_ratio > 1: return get_zoom_mat(1/orig_ratio**2, 1, 0, 0.)\n",
    "    else:              return get_zoom_mat(1, orig_ratio**2, 0, 0.)\n",
    "\n",
    "@TfmCoord\n",
    "def zoom_squish(c, size, scale:Uniform=1.0, squish:Uniform=1.0, invert:Bool=False, \n",
    "                row_pct:Uniform=0.5, col_pct:Uniform=0.5):\n",
    "    #This is intended for scale, squish and invert to be of size 10 (or whatever) so that the transform\n",
    "    #can try a few zoom/squishes before falling back to center crop (like torchvision.RandomResizedCrop)\n",
    "    m = compute_zs_mat(size, scale, squish, invert, row_pct, col_pct)\n",
    "    return affine_mult(c, FloatTensor(m))\n",
    "\n",
    "\n",
    "def round_multiple(x, mult): return (int(x/mult+0.5)*mult)\n",
    "\n",
    "def get_crop_target(target_px, target_aspect=None, mult=32):\n",
    "    target_px = listify(target_px, 2)\n",
    "    target_r,target_c = target_px\n",
    "    if target_aspect:\n",
    "        target_r = math.sqrt(target_r*target_c/target_aspect)\n",
    "        target_c = target_r*target_aspect\n",
    "    return round_multiple(target_r,mult),round_multiple(target_c,mult)\n",
    "\n",
    "def get_resize_target(img, crop_target, do_crop=False):\n",
    "    if crop_target is None: return None\n",
    "    ch,r,c = img.shape\n",
    "    target_r,target_c = crop_target\n",
    "    ratio = (min if do_crop else max)(r/target_r, c/target_c)\n",
    "    return ch,round(r/ratio),round(c/ratio)\n",
    "\n",
    "\n",
    "@TfmResize\n",
    "def resize_image(x, *args, **kwargs): return x.resize(*args, **kwargs)\n",
    "\n",
    "def _resize(self, size=None, do_crop=False, mult=32):\n",
    "    assert self._flow is None\n",
    "    if not size and hasattr(self, 'size'): size = self.size\n",
    "    aspect = self.aspect if hasattr(self, 'aspect') else None\n",
    "    crop_target = get_crop_target(size, aspect, mult=mult)\n",
    "    target = get_resize_target(self, crop_target, do_crop)\n",
    "    self.flow = affine_grid(target)\n",
    "    return self\n",
    "Image.resize=_resize\n",
    "\n",
    "\n",
    "@TfmCrop\n",
    "def crop_pad(img, size=None, mult=32, padding_mode=None,\n",
    "             row_pct:uniform = 0.5, col_pct:uniform = 0.5):\n",
    "    aspect = img.aspect if hasattr(img, 'aspect') else 1.\n",
    "    if not size and hasattr(img, 'size'): size = img.size\n",
    "    if not padding_mode:\n",
    "        if hasattr(img, 'sample_kwargs') and ('padding_mode' in img.sample_kwargs):\n",
    "            padding_mode = img.sample_kwargs['padding_mode']\n",
    "        else: padding_mode='reflect'\n",
    "    if padding_mode=='zeros': padding_mode='constant'\n",
    "\n",
    "    rows,cols = get_crop_target(size, aspect, mult=mult)\n",
    "    x = img.px\n",
    "    if x.size(1)<rows or x.size(2)<cols:\n",
    "        row_pad = max((rows-x.size(1)+1)//2, 0)\n",
    "        col_pad = max((cols-x.size(2)+1)//2, 0)\n",
    "        x = F.pad(x[None], (col_pad,col_pad,row_pad,row_pad), mode=padding_mode)[0]\n",
    "    row = int((x.size(1)-rows+1)*row_pct)\n",
    "    col = int((x.size(2)-cols+1)*col_pct)\n",
    "\n",
    "    x = x[:, row:row+rows, col:col+cols]\n",
    "    img.px = x.contiguous() # without this, get NaN later - don't know why\n",
    "    return img\n",
    "\n",
    "def resize_crop(size=None, do_crop=False, mult=32, rand_crop=False):\n",
    "    crop_kw = {'row_pct':(0,1.),'col_pct':(0,1.)} if rand_crop else {}\n",
    "    return [resize_image(size=size, do_crop=do_crop, mult=mult),\n",
    "            crop_pad(size=size, mult=mult, **crop_kw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_device = torch.device('cuda')\n",
    "def to_device(device, b): return [o.to(device) for o in b]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DeviceDataLoader():\n",
    "    dl: DataLoader\n",
    "    device: torch.device\n",
    "    progress_func:Callable=None\n",
    "    tfms: List[Callable]=None\n",
    "    half: bool = False\n",
    "\n",
    "    def __len__(self): return len(self.dl)\n",
    "\n",
    "    def proc_batch(self,b):\n",
    "        b = to_device(self.device,b)\n",
    "        if self.half: b = to_half(b)\n",
    "        return b if self.tfms is None else self.tfms(b)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.gen = map(self.proc_batch, self.dl)\n",
    "        if self.progress_func is not None:\n",
    "            self.gen = self.progress_func(self.gen, total=len(self.dl), leave=False)\n",
    "        return iter(self.gen)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *args, device=default_device, progress_func=tqdm, tfms=tfms, **kwargs):\n",
    "        return cls(DataLoader(*args, **kwargs), device=device, progress_func=progress_func, tfms=tfms, half=False)\n",
    "\n",
    "\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, device=None, **kwargs):\n",
    "        self.device = default_device if device is None else device\n",
    "        self.train_dl = DeviceDataLoader(train_dl, device=self.device, **kwargs)\n",
    "        self.valid_dl = DeviceDataLoader(valid_dl, device=self.device, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, bs=64, device=None, num_workers=4, progress_func=tqdm,\n",
    "               train_tfm=None, valid_tfm=None, sample_func=None, dl_tfms=None, **kwargs):\n",
    "        if train_tfm is not None: train_tfm = DatasetTfm(train_ds, train_tfm, **kwargs)\n",
    "        if valid_tfm is not None: valid_tfm = DatasetTfm(valid_ds, valid_tfm, **kwargs)\n",
    "        if sample_func is None:\n",
    "            train_dl = DataLoader(train_ds, bs,   shuffle=True,  num_workers=num_workers)\n",
    "            valid_dl = DataLoader(valid_ds, bs*2, shuffle=False, num_workers=num_workers)\n",
    "        else:\n",
    "            train_samp = sample_func(train_ds, bs, True)\n",
    "            valid_samp = sample_func(valid_ds, bs*2, False)\n",
    "            train_dl = DataLoader(train_ds, num_workers=num_workers, batch_sampler=train_samp)\n",
    "            valid_dl = DataLoader(valid_ds, num_workers=num_workers, batch_sampler=valid_samp)\n",
    "        return cls(train_dl, valid_dl, device, tfms=dl_tfms, progress_func=progress_func)\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dl.dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/carvana')\n",
    "PATH_PNG = PATH/'train_masks_png'\n",
    "PATH_X = PATH/'train-128'\n",
    "PATH_Y = PATH/'train_masks-128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = next(PATH_X.iterdir())\n",
    "x = open_image(img_f)\n",
    "show_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_fn(x_fn): return PATH_Y/f'{x_fn.name[:-4]}_mask.png'\n",
    "\n",
    "\n",
    "img_y_f = get_y_fn(img_f)\n",
    "y = open_image(img_y_f, as_mask=True)\n",
    "show_image(y)\n",
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_xy_image(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MatchedFilesDataset(Dataset):\n",
    "    x_fns:List[Path]; y_fns:List[Path]\n",
    "    def __post_init__(self): assert len(self.x_fns)==len(self.y_fns)\n",
    "    def __repr__(self): return f'{type(self).__name__} of len {len(self.x_fns)}'\n",
    "    def __len__(self): return len(self.x_fns)\n",
    "    def __getitem__(self, i): return open_image(self.x_fns[i]), open_image(self.y_fns[i],as_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segmentation_tfms(tfms):\n",
    "    seg_tfms = []\n",
    "    for tfm in tfms:\n",
    "        if tfm.tfm_type != 'lighting':\n",
    "            #overrides = {'mode': 'nearest'} if tfm.tfm_type == 'affine' else {}\n",
    "            #overrides = {}\n",
    "            seg_tfms.append(tfm)\n",
    "    return seg_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "train_tfms = [\n",
    "    rotate(degrees=(-20,20.)),\n",
    "    zoom(scale=(1.,1.5)),\n",
    "    contrast(scale=(0.6,1.4)),\n",
    "    brightness(change=(0.3,0.7)),\n",
    "    *resize_crop(size=size, rand_crop=True, do_crop=True)\n",
    "]\n",
    "valid_tfms = [\n",
    "    *resize_crop(size=size, rand_crop=False, do_crop=True)\n",
    "]\n",
    "\n",
    "x_train_tfms = train_tfms\n",
    "y_train_tfms = make_segmentation_tfms(x_train_tfms)\n",
    "x_valid_tfms = train_tfms\n",
    "y_valid_tfms = make_segmentation_tfms(x_valid_tfms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_tfms(tfms): \n",
    "    for f in listify(tfms):\n",
    "        f.release()\n",
    "        \n",
    "def is_listy(x)->bool: return isinstance(x, (tuple,list))\n",
    "\n",
    "def apply_tfms(tfms, x, do_resolve=True, aspect=None, size=None,\n",
    "               padding_mode='reflect', **kwargs):\n",
    "    if not tfms: return x\n",
    "    tfms = sorted(listify(tfms), key=lambda o: o.order)\n",
    "    if do_resolve: resolve_tfms(tfms)\n",
    "    x = Image(x.clone())\n",
    "    x.set_sample(padding_mode=padding_mode, **kwargs)\n",
    "    x.aspect = aspect\n",
    "    x.size = size\n",
    "    \n",
    "    for tfm in tfms:\n",
    "        x = tfm(x)\n",
    "            \n",
    "    return x.px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_idxs(seq, idxs):\n",
    "    '''A generator that returns sequence pieces, seperated by indexes specified in idxs. '''\n",
    "    last = 0\n",
    "    for idx in idxs:\n",
    "        if not (-len(seq) <= idx < len(seq)):\n",
    "            raise KeyError(f'Idx {idx} is out-of-bounds')\n",
    "        yield seq[last:idx]\n",
    "        last = idx\n",
    "    yield seq[last:]\n",
    "\n",
    "def split_by_idx(idxs, *a):\n",
    "    \"\"\"\n",
    "    Split each array passed as *a, to a pair of arrays like this (elements selected by idxs,  the remaining elements)\n",
    "    This can be used to split multiple arrays containing training data to validation and training set.\n",
    "    :param idxs [int]: list of indexes selected\n",
    "    :param a list: list of np.array, each array should have same amount of elements in the first dimension\n",
    "    :return: list of tuples, each containing a split of corresponding array from *a.\n",
    "            First element of each tuple is an array composed from elements selected by idxs,\n",
    "            second element is an array of remaining elements.\n",
    "    \"\"\"\n",
    "    mask = np.zeros(len(a[0]),dtype=bool)\n",
    "    mask[np.array(idxs)] = True\n",
    "    return [(o[mask],o[~mask]) for o in a]\n",
    "\n",
    "\n",
    "x_fns = [o for o in PATH_X.iterdir() if o.is_file()]\n",
    "y_fns = [get_y_fn(o) for o in x_fns]\n",
    "val_idxs = list(range(1008))\n",
    "((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, np.array(x_fns), np.array(y_fns))\n",
    "train_ds = MatchedFilesDataset(trn_x, trn_y)\n",
    "val_ds = MatchedFilesDataset(val_x, val_y)\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_ds))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4,figsize=(12,3))\n",
    "for i in range(4):\n",
    "    imgx,imgy = train_ds[i]\n",
    "    show_xy_image(imgx, imgy, ax=axes[i])\n",
    "    #show_image(imgy, axes[1][i], ax=axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(8):\n",
    "    imgx,imgy = apply_tfms(x_train_tfms, x, do_resolve=True), apply_tfms(y_train_tfms, y, do_resolve=False, mode='nearest')\n",
    "    #imgx,imgy = x, y\n",
    "    show_xy_image(imgx, imgy, axes[i//4][i%4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetTfm(Dataset):\n",
    "    def __init__(self, ds:Dataset, x_tfms:Collection[Callable]=None, y_tfms:Collection[Callable]=None):\n",
    "        self.ds,self.x_tfms,self.y_tfms = ds,x_tfms,y_tfms\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.ds[idx]\n",
    "        if self.x_tfms is not None: x = apply_tfms(self.x_tfms, x, do_resolve=True)\n",
    "        if self.y_tfms is not None: y = apply_tfms(self.y_tfms, y, do_resolve=False, mode='nearest')\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tds = DatasetTfm(train_ds, x_tfms=x_train_tfms, y_tfms=y_train_tfms)\n",
    "valid_tds = DatasetTfm(val_ds, x_tfms=x_valid_tfms, y_tfms=y_valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(train_tds))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,9))\n",
    "for ax in axes.flat: show_image(train_tds[1][0], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagenet\n",
    "default_mean, default_std = Tensor([0.485, 0.456, 0.406]), Tensor([0.229, 0.224, 0.225])\n",
    "default_norm,default_denorm = normalize_funcs(default_mean,default_std)\n",
    "\n",
    "bs = 64\n",
    "data = DataBunch.create(train_tds, valid_tds, bs=bs, dl_tfms=default_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))\n",
    "x = x.cpu()\n",
    "y = y.cpu()\n",
    "print(x.min(),x.max(),x.mean(),x.std())\n",
    "x = default_denorm(x)\n",
    "#y = default_denorm(y)\n",
    "show_images(x,y,6, figsize=(9,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, xb, yb, loss_fn, opt=None, cb_handler=None, metrics=None):\n",
    "    if cb_handler is None: cb_handler = CallbackHandler([])\n",
    "    out = model(xb)\n",
    "    out = cb_handler.on_loss_begin(out)\n",
    "    loss = loss_fn(out, yb)\n",
    "    mets = [f(out,yb).item() for f in metrics] if metrics is not None else []\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss = cb_handler.on_backward_begin(loss)\n",
    "        loss.backward()\n",
    "        cb_handler.on_backward_end()\n",
    "        opt.step()\n",
    "        cb_handler.on_step_end()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return (loss.item(),) + tuple(mets) + (len(xb),)\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_fn, opt, data, callbacks=None, metrics=None):\n",
    "    cb_handler = CallbackHandler(callbacks)\n",
    "    cb_handler.on_train_begin()\n",
    "    \n",
    "    for epoch in tnrange(epochs):\n",
    "        model.train()\n",
    "        cb_handler.on_epoch_begin()\n",
    "        \n",
    "        for xb,yb in data.train_dl:\n",
    "            xb, yb = cb_handler.on_batch_begin(xb, yb)\n",
    "            loss,_ = loss_batch(model, xb, yb, loss_fn, opt, cb_handler)\n",
    "            if cb_handler.on_batch_end(loss): break\n",
    "        \n",
    "        if hasattr(data,'valid_dl') and data.valid_dl is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                *val_metrics,nums = zip(*[loss_batch(model, xb, yb, loss_fn, cb_handler=cb_handler, metrics=metrics)\n",
    "                                for xb,yb in data.valid_dl])\n",
    "            val_metrics = [np.sum(np.multiply(val,nums)) / np.sum(nums) for val in val_metrics]\n",
    "            \n",
    "        else: val_metrics=None\n",
    "        if cb_handler.on_epoch_end(val_metrics): break\n",
    "        \n",
    "    cb_handler.on_train_end()\n",
    "\n",
    "class Callback():\n",
    "    def on_train_begin(self, **kwargs): pass         \n",
    "        #To initiliaze constants in the callback.\n",
    "    def on_epoch_begin(self, **kwargs): pass\n",
    "        #At the beginning of each epoch\n",
    "    def on_batch_begin(self, **kwargs): pass \n",
    "        #To set HP before the step is done. \n",
    "        #Returns xb, yb (which can allow us to modify the input at that step if needed)\n",
    "    def on_loss_begin(self, **kwargs): pass\n",
    "        #Called after the forward pass but before the loss has been computed.\n",
    "        #Returns the output (which can allow us to modify it)\n",
    "    def on_backward_begin(self, **kwargs): pass\n",
    "        #Called after the forward pass and the loss has been computed, but before the back propagation.\n",
    "        #Returns the loss (which can allow us to modify it, for instance for reg functions)\n",
    "    def on_backward_end(self, **kwargs): pass\n",
    "        #Called after the back propagation had been done (and the gradients computed) but before the step of the optimizer.\n",
    "        #Useful for true weight decay in AdamW\n",
    "    def on_step_end(self, **kwargs): pass\n",
    "        #Called after the step of the optimizer but before the gradients are zeroed (not sure this one is useful)\n",
    "    def on_batch_end(self, **kwargs): pass\n",
    "        #Called at the end of the batch\n",
    "    def on_epoch_end(self, **kwargs): pass\n",
    "        #Called at the end of an epoch\n",
    "    def on_train_end(self, **kwargs): pass\n",
    "        #Useful for cleaning up things and saving files/models\n",
    "        \n",
    "Floats = Union[float, Collection[float]]\n",
    "\n",
    "@dataclass\n",
    "class Learner():\n",
    "    \"Object that wraps together some data, a model, a loss function and an optimizer\"\n",
    "    \n",
    "    data:DataBunch\n",
    "    model:nn.Module\n",
    "    opt_fn:Callable=optim.SGD\n",
    "    loss_fn:Callable=F.cross_entropy\n",
    "    metrics:Collection[Callable]=None\n",
    "    true_wd:bool=False\n",
    "    layer_groups:Collection[nn.Module]=None\n",
    "    def __post_init__(self): \n",
    "        self.model = self.model.to(self.data.device)\n",
    "        self.callbacks = []\n",
    "\n",
    "    def fit(self, epochs:int, lr:Floats, wd:Floats=0., callbacks:Collection[Callback]=None):\n",
    "        if not hasattr(self, 'opt'): self.create_opt(lr, wd)\n",
    "        else: self.opt.wd = wd\n",
    "        if callbacks is None: callbacks = []\n",
    "        callbacks = self.callbacks + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics)\n",
    "    \n",
    "    def create_opt(self, lr:Floats, wd:Floats=0.):\n",
    "        if self.layer_groups is None: self.layer_groups = [self.model]\n",
    "        lrs = listify(lr, self.layer_groups)\n",
    "        opt = self.opt_fn([{'params':l.parameters(), 'lr':lr} for l,lr in zip(self.layer_groups, lrs)])\n",
    "        self.opt = OptimWrapper(opt, wd=wd, true_wd=self.true_wd)\n",
    "        self.recorder = Recorder(self.opt, self.data.train_dl)\n",
    "        self.callbacks = [self.recorder] + self.callbacks\n",
    "\n",
    "def is_tuple(x): return isinstance(x, tuple)\n",
    "\n",
    "class Stepper():\n",
    "    def __init__(self, vals, num_it, ft=None):\n",
    "        self.start,self.end = (vals[0],vals[1]) if is_tuple(vals) else (vals,0)\n",
    "        self.num_it = num_it\n",
    "        if ft is None: self.ft = annealing_linear if is_tuple(vals) else annealing_no\n",
    "        else:          self.ft = ft\n",
    "        self.n = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        return self.ft(self.start, self.end, self.n/self.num_it)\n",
    "    \n",
    "    @property\n",
    "    def is_done(self):  return self.n >= self.num_it\n",
    "    \n",
    "class OneCycleScheduler(Callback):\n",
    "    def __init__(self, learn, lr_max, epochs, moms=(0.95,0.85), div_factor=10, pct_end=0.1):\n",
    "        self.learn = learn\n",
    "        a = int(len(learn.data.train_dl) * epochs * (1 - pct_end) / 2)\n",
    "        b = len(learn.data.train_dl) * epochs - 2*a\n",
    "        self.lr_scheds = [Stepper((lr_max/div_factor, lr_max), a),\n",
    "                          Stepper((lr_max, lr_max/div_factor), a),\n",
    "                          Stepper((lr_max/div_factor, lr_max/(div_factor*100)), b)]\n",
    "        self.mom_scheds = [Stepper(moms, a), Stepper((moms[1], moms[0]), a), Stepper(moms[0], b)]\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.opt = self.learn.opt\n",
    "        self.opt.lr, self.opt.mom = self.lr_scheds[0].start, self.mom_scheds[0].start\n",
    "        self.idx_s = 0\n",
    "    \n",
    "    def on_batch_end(self, **kwargs):\n",
    "        if self.idx_s >= len(self.lr_scheds): return True\n",
    "        self.opt.lr = self.lr_scheds[self.idx_s].step()\n",
    "        self.opt.mom = self.mom_scheds[self.idx_s].step()\n",
    "        if self.lr_scheds[self.idx_s].is_done:\n",
    "            self.idx_s += 1\n",
    "\n",
    "def annealing_no(start, end, pct): return start\n",
    "def annealing_linear(start, end, pct): return start + pct * (end-start)\n",
    "def annealing_exp(start, end, pct): return start * (end/start) ** pct\n",
    "def annealing_cos(start, end, pct):\n",
    "    cos_out = np.cos(np.pi * pct) + 1\n",
    "    return end + (start-end)/2 * cos_out\n",
    "    \n",
    "def do_annealing_poly(start, end, pct, degree): return end + (start-end) * (1-pct)**degree\n",
    "def annealing_poly(degree): return functools.partial(do_annealing_poly, degree=degree)\n",
    "\n",
    "def _get_init_state(): return {'epoch':0, 'iteration':0, 'num_batch':0}\n",
    "\n",
    "@dataclass\n",
    "class CallbackHandler():\n",
    "    callbacks:Collection[Callable]\n",
    "    beta:float=0.98\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        self.smoothener = SmoothenValue(self.beta)\n",
    "        self.state_dict:Dict[str,Union[int,float,Tensor]]=_get_init_state()\n",
    "    \n",
    "    def __call__(self, cb_name):\n",
    "        return [getattr(cb, f'on_{cb_name}')(**self.state_dict) for cb in self.callbacks]\n",
    "    \n",
    "    def on_train_begin(self): \n",
    "        self.state_dict = _get_init_state()\n",
    "        self('train_begin')\n",
    "        \n",
    "    def on_epoch_begin(self): \n",
    "        self.state_dict['num_batch'] = 0\n",
    "        self('epoch_begin')\n",
    "        \n",
    "    def on_batch_begin(self, xb, yb):\n",
    "        self.state_dict['last_input'], self.state_dict['last_target'] = xb, yb\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_batch_begin(**self.state_dict)\n",
    "            if a is not None: self.state_dict['last_input'], self.state_dict['last_target'] = a\n",
    "        return self.state_dict['last_input'], self.state_dict['last_target']\n",
    "    \n",
    "    def on_loss_begin(self, out):\n",
    "        self.state_dict['last_output'] = out\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_loss_begin(**self.state_dict)\n",
    "            if a is not None: self.state_dict['last_output'] = a\n",
    "        return self.state_dict['last_output']\n",
    "    \n",
    "    def on_backward_begin(self, loss):\n",
    "        self.smoothener.add_value(loss.item())\n",
    "        self.state_dict['last_loss'], self.state_dict['smooth_loss'] = loss, self.smoothener.smooth\n",
    "        for cb in self.callbacks:\n",
    "            a = cb.on_backward_begin(**self.state_dict)\n",
    "            if a is not None: self.state_dict['last_loss'] = a\n",
    "        return self.state_dict['last_loss']\n",
    "    \n",
    "    def on_backward_end(self):        self('backward_end')\n",
    "    def on_step_end(self):            self('step_end')\n",
    "        \n",
    "    def on_batch_end(self, loss):     \n",
    "        self.state_dict['last_loss'] = loss\n",
    "        stop = np.any(self('batch_end'))\n",
    "        self.state_dict['iteration'] += 1\n",
    "        self.state_dict['num_batch'] += 1\n",
    "        return stop\n",
    "    \n",
    "    def on_epoch_end(self, val_metrics):\n",
    "        self.state_dict['last_metrics'] = val_metrics\n",
    "        stop = np.any(self('epoch_end'))\n",
    "        self.state_dict['epoch'] += 1\n",
    "        return stop\n",
    "    \n",
    "    def on_train_end(self): self('train_end')\n",
    "        \n",
    "class OptimWrapper():\n",
    "    def __init__(self, opt, wd=0., true_wd=False):\n",
    "        self.opt,self.true_wd = opt,true_wd\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "        self._wd = wd\n",
    "    \n",
    "    #Pytorch optimizer methods\n",
    "    def step(self):\n",
    "        # weight decay outside of optimizer step (AdamW)\n",
    "        if self.true_wd:\n",
    "            for pg in self.opt.param_groups:\n",
    "                for p in pg['params']: p.data.mul_(1 - self._wd*pg['lr'])\n",
    "            self.set_val('weight_decay', 0)\n",
    "        self.opt.step()\n",
    "    \n",
    "    def zero_grad(self): self.opt.zero_grad()\n",
    "    \n",
    "    #Hyperparameters as properties\n",
    "    @property\n",
    "    def lr(self): return self._lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, val): self._lr = self.set_val('lr', val)\n",
    "    \n",
    "    @property\n",
    "    def mom(self): return self._mom\n",
    "\n",
    "    @mom.setter\n",
    "    def mom(self, val):\n",
    "        if 'momentum' in self.opt_keys: self.set_val('momentum', val)\n",
    "        elif 'betas' in self.opt_keys:  self.set_val('betas', (val, self._beta))\n",
    "        self._mom = val\n",
    "    \n",
    "    @property\n",
    "    def beta(self): return self._beta\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val):\n",
    "        if 'betas' in self.opt_keys:    self.set_val('betas', (self._mom,val))\n",
    "        elif 'alpha' in self.opt_keys:  self.set_val('alpha', val)\n",
    "        self._beta = val\n",
    "    \n",
    "    @property\n",
    "    def wd(self): return self._wd\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val):\n",
    "        if not self.true_wd: self.set_val('weight_decay', val)\n",
    "        self._wd = val\n",
    "    \n",
    "    #Helper functions\n",
    "    def read_defaults(self):\n",
    "        self._beta = None\n",
    "        if 'lr' in self.opt_keys: self._lr = self.opt.param_groups[0]['lr']\n",
    "        if 'momentum' in self.opt_keys: self._mom = self.opt.param_groups[0]['momentum']\n",
    "        if 'alpha' in self.opt_keys: self._beta = self.opt.param_groups[0]['alpha']\n",
    "        if 'betas' in self.opt_keys: self._mom,self._beta = self.opt.param_groups[0]['betas']\n",
    "        if 'weight_decay' in self.opt_keys: self._wd = self.opt.param_groups[0]['weight_decay']\n",
    "    \n",
    "    def set_val(self, key, val):\n",
    "        for pg in self.opt.param_groups: pg[key] = val\n",
    "        return val\n",
    "\n",
    "@dataclass\n",
    "class Recorder(Callback):\n",
    "    opt: torch.optim\n",
    "    train_dl: DeviceDataLoader = None\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.losses,self.val_losses,self.lrs,self.moms,self.metrics,self.nb_batches = [],[],[],[],[],[]\n",
    "    \n",
    "    def on_batch_begin(self, **kwargs):\n",
    "        self.lrs.append(self.opt.lr)\n",
    "        self.moms.append(self.opt.mom)\n",
    "    \n",
    "    def on_backward_begin(self, smooth_loss, **kwargs):\n",
    "        #We record the loss here before any other callback has a chance to modify it.\n",
    "        self.losses.append(smooth_loss)\n",
    "        if self.train_dl is not None and self.train_dl.progress_func is not None: \n",
    "            self.train_dl.gen.set_postfix_str(smooth_loss)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, num_batch, smooth_loss, last_metrics, **kwargs):\n",
    "        self.nb_batches.append(num_batch)\n",
    "        if last_metrics is not None:\n",
    "            self.val_losses.append(last_metrics[0])\n",
    "            if len(last_metrics) > 1: self.metrics.append(last_metrics[1:])\n",
    "            print(epoch, smooth_loss, *last_metrics)\n",
    "        else:  print(epoch, smooth_loss)\n",
    "    \n",
    "    def plot_lr(self, show_moms=False):\n",
    "        iterations = list(range(len(self.lrs)))\n",
    "        if show_moms:\n",
    "            _, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "            axs[0].plot(iterations, self.lrs)\n",
    "            axs[1].plot(iterations, self.moms)\n",
    "        else: plt.plot(iterations, self.lrs)\n",
    "    \n",
    "    def plot(self, skip_start=10, skip_end=5):\n",
    "        lrs = self.lrs[skip_start:-skip_end] if skip_end > 0 else self.lrs[skip_start:]\n",
    "        losses = self.losses[skip_start:-skip_end] if skip_end > 0 else self.losses[skip_start:]\n",
    "        _, ax = plt.subplots(1,1)\n",
    "        ax.plot(lrs, losses)\n",
    "        ax.set_xscale('log')\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        _, ax = plt.subplots(1,1)\n",
    "        iterations = list(range(len(self.losses)))\n",
    "        ax.plot(iterations, self.losses)\n",
    "        val_iter = self.nb_batches\n",
    "        val_iter = np.cumsum(val_iter)\n",
    "        ax.plot(val_iter, self.val_losses)\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        assert len(self.metrics) != 0, \"There is no metrics to plot.\"\n",
    "        _, axes = plt.subplots(len(self.metrics[0]),1,figsize=(6, 4*len(self.metrics[0])))\n",
    "        val_iter = self.nb_batches\n",
    "        val_iter = np.cumsum(val_iter)\n",
    "        axes = axes.flatten() if len(self.metrics[0]) != 1 else [axes]\n",
    "        for i, ax in enumerate(axes):\n",
    "            values = [met[i] for met in self.metrics]\n",
    "            ax.plot(val_iter, values)\n",
    "            \n",
    "class SmoothenValue():\n",
    "    def __init__(self, beta):\n",
    "        self.beta,self.n,self.mov_avg = beta,0,0\n",
    "    \n",
    "    def add_value(self, val):\n",
    "        self.n += 1\n",
    "        self.mov_avg = self.beta * self.mov_avg + (1 - self.beta) * val\n",
    "        self.smooth = self.mov_avg / (1 - self.beta ** self.n)\n",
    "        \n",
    "class LRFinder(Callback):\n",
    "    def __init__(self, opt, data, start_lr=1e-5, end_lr=10, num_it=200):\n",
    "        self.opt,self.data = opt,data\n",
    "        self.sched = Stepper((start_lr, end_lr), num_it, annealing_exp)\n",
    "        #To avoid validating if the train_dl has less than num_it batches, we put aside the valid_dl and remove it\n",
    "        #during the call to fit.\n",
    "        self.valid_dl = data.valid_dl\n",
    "        self.data.valid_dl = None\n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.opt.lr = self.sched.start\n",
    "        self.stop,self.best_loss = False,0.\n",
    "    \n",
    "    def on_batch_end(self, iteration, smooth_loss, **kwargs):\n",
    "        if iteration==0 or smooth_loss < self.best_loss: self.best_loss = smooth_loss\n",
    "        self.opt.lr = self.sched.step()\n",
    "        if self.sched.is_done or smooth_loss > 4*self.best_loss:\n",
    "            #We use the smoothed loss to decide on the stopping since it's less shaky.\n",
    "            self.stop=True\n",
    "            return True\n",
    "    \n",
    "    def on_epoch_end(self, **kwargs): return self.stop\n",
    "    \n",
    "    def on_train_end(self, **kwargs):\n",
    "        #Clean up and put back the valid_dl in its place.\n",
    "        self.data.valid_dl = self.valid_dl\n",
    "        \n",
    "        \n",
    "def lr_find(learn, start_lr=1e-5, end_lr=10, num_it=100):\n",
    "    #TODO: add model.save and model.load.\n",
    "    learn.create_opt(start_lr)\n",
    "    cb = LRFinder(learn.opt, learn.data, start_lr, end_lr, num_it)\n",
    "    a = int(np.ceil(num_it/len(learn.data.train_dl)))\n",
    "    learn.fit(a, start_lr, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet34\n",
    "\n",
    "model_meta = {\n",
    "    resnet34:[8,6]\n",
    "}\n",
    "\n",
    "f = resnet34\n",
    "cut,lr_cut = model_meta[f]\n",
    "\n",
    "def cut_model(m, cut):\n",
    "    return list(m.children())[:cut] if cut else m\n",
    "\n",
    "def get_base():\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def dice(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    return 2. * (pred*targs).sum() / (pred+targs).sum()\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    preds = torch.max(out, dim=1)[1]\n",
    "    return (preds==yb).float().mean()\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "def to_gpu(x, *args, **kwargs):\n",
    "    '''puts pytorch variable to gpu, if cuda is available and USE_GPU is set to true. '''\n",
    "    return x.cuda(*args, **kwargs) if USE_GPU else x\n",
    "\n",
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()\n",
    "        \n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))\n",
    "    \n",
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,256,256)\n",
    "        self.up2 = UnetBlock(256,128,256)\n",
    "        self.up3 = UnetBlock(256,64,256)\n",
    "        self.up4 = UnetBlock(256,64,256)\n",
    "        self.up5 = UnetBlock(256,3,16)\n",
    "        self.up6 = nn.ConvTranspose2d(16, 1, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        x = F.relu(self.rn(x))\n",
    "        x = self.up1(x, self.sfs[3].features)\n",
    "        x = self.up2(x, self.sfs[2].features)\n",
    "        x = self.up3(x, self.sfs[1].features)\n",
    "        x = self.up4(x, self.sfs[0].features)\n",
    "        x = self.up5(x, inp)\n",
    "        x = self.up6(x)\n",
    "        return x #[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()\n",
    "\n",
    "            \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]\n",
    "    \n",
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_base = get_base()\n",
    "model = to_gpu(Unet34(m_base))\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [dice]\n",
    "learn.loss_fn = nn.BCEWithLogitsLoss()\n",
    "sched = OneCycleScheduler(learn, 0.1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(20, 0.5, callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.valid_dl))\n",
    "py = learn.model(x)\n",
    "py = py.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(y[0]), show_image(py[0]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(y[3]), show_image(py[3]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(y[5]), show_image(py[5]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
