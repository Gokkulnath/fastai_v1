{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_002 import *\n",
    "\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "# PATH = DATA_PATH/'cifar10_dog_air'\n",
    "PATH = DATA_PATH/'cifar10'\n",
    "\n",
    "train_ds = FilesDataset(PATH/'train')\n",
    "valid_ds = FilesDataset(PATH/'test', train_ds.classes)\n",
    "\n",
    "x = train_ds[1][0]\n",
    "bs=256\n",
    "c = len(train_ds.classes)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@reg_transform\n",
    "def pad(x, padding, mode='reflect') -> TfmType.Start:\n",
    "    return F.pad(x[None], (padding,)*4, mode=mode)[0]\n",
    "\n",
    "@reg_transform\n",
    "def crop(x, size, row_pct:uniform=0.5, col_pct:uniform=0.5) -> TfmType.Pixel:\n",
    "    size = listify(size,2)\n",
    "    rows,cols = size\n",
    "    row = int((x.size(1)-rows+1) * row_pct)\n",
    "    col = int((x.size(2)-cols+1) * col_pct)\n",
    "    return x[:, row:row+rows, col:col+cols].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(crop(pad(x, 4, 'constant'), 32, 0.25, 0.75), hide_axis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(crop(pad(x, 4), 32, 0.25, 0.75), hide_axis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5),\n",
    "        pad_tfm(padding=4),\n",
    "        crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TfmDataset(Dataset):\n",
    "    def __init__(self, ds: Dataset, tfms: Collection[Callable] = None, **kwargs):\n",
    "        self.ds,self.tfms,self.kwargs = ds,tfms,kwargs\n",
    "        \n",
    "    def __len__(self): return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.ds[idx]\n",
    "        if self.tfms is not None: x = apply_tfms(self.tfms)(x, **self.kwargs)\n",
    "        return x,y\n",
    "\n",
    "class DataBunch():\n",
    "    def __init__(self, train_ds, valid_ds, bs=64, device=None, num_workers=4):\n",
    "        self.device = default_device if device is None else device\n",
    "        self.train_dl = DeviceDataLoader.create(train_ds, bs, shuffle=True, num_workers=num_workers)\n",
    "        self.valid_dl = DeviceDataLoader.create(valid_ds, bs*2, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, train_tfm=None, valid_tfm=None, **kwargs):\n",
    "        return cls(TfmDataset(train_ds, train_tfm), TfmDataset(valid_ds, valid_tfm))\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, train_tfm=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,9))\n",
    "for ax in axes.flat: show_image(data.train_ds[1][0], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 stats looked up on google \n",
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@reg_transform\n",
    "def normalize(x, mean,std) -> TfmType.Pixel: return (x-mean[...,None,None]) / std[...,None,None]\n",
    "\n",
    "def denormalize(x, mean,std): return x*std[...,None,None] + mean[...,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_norm = normalize_tfm(mean=data_mean,std=data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=tfms+[cifar_norm], valid_tfm=[cifar_norm], num_workers=12)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))\n",
    "x = x.cpu()\n",
    "x.min(),x.max(),x.mean(),x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = denormalize(x, data_mean, data_std).clamp(0,1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(x,y,6,train_ds.classes, figsize=(9,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, simple_cnn([3,16,16,c], [3,3,3], [2,2,2]))\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, 0.1, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_layer(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_layer(ni//2, ni, ks=3)\n",
    "        \n",
    "    def forward(self, x): return x + self.conv2(self.conv1(x))\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def make_group_layer(self, ch_in, num_blocks, stride=1):\n",
    "        return [conv_layer(ch_in, ch_in*2,stride=stride)\n",
    "               ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes, nf=32):\n",
    "        super().__init__()\n",
    "        layers = [conv_layer(3, nf, ks=3, stride=1)]\n",
    "        for i,nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, num_classes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 4, 4, 2], num_classes=c, nf=16)\n",
    "# model = Darknet([1, 2, 4, 6, 3], num_classes=c, nf=32)\n",
    "learner = Learner(data, model)\n",
    "opt_fn = partial(optim.SGD, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1, 0.1, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2, 0.01, opt_fn=opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for lr in (0.1,0.2,0.4,0.8,0.1,0.01):\n",
    "#     momentum = 0.95 if lr<0.1 else 0.85 if lr>0.5 else 0.9\n",
    "#     learner.fit(2, lr, opt_fn=partial(optim.SGD, momentum=momentum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
