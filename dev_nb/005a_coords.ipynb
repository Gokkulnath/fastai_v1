{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_005 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset [here](https://download.pytorch.org/tutorial/faces.zip) from the pytoch tutorial on transforms. Unzip it in the data directory, so that data/faces/ contains the images and the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/faces/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fns = get_image_files(PATH)\n",
    "img_fns = [fn for fn in img_fns if str(fn)[-3:] == 'jpg']\n",
    "len(img_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "poses = pd.read_csv(PATH/'face_landmarks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the coordinates. We adopt pytorch convention in grid_sampler where the coordinates are normalized between -1 and 1. (-1,-1) is the top left corner, (1,1) the bottom right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_pnts = []\n",
    "for i, fname in enumerate(img_fns):\n",
    "    w,h = Image.open(fname).size\n",
    "    coords = np.array(poses[poses['image_name'] == fname.name].iloc[0][1:], dtype=np.float32)\n",
    "    coords = torch.tensor(coords).view(-1,2)\n",
    "    coords.div_(torch.tensor([w/2,h/2])[None]).add_(-1)\n",
    "    pose_pnts.append(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pose_pnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pose(img, pnts, ax=None):\n",
    "    if ax is None: _,ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    w,h = img.size\n",
    "    pnts = (pnts.numpy() + 1) * np.array([w/2,h/2])[None]\n",
    "    ax.scatter(pnts[:, 0], pnts[:, 1], s=10, marker='.', c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(img_fns[0])\n",
    "show_pose(img, pose_pnts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we change the picture, the points must be changed accordingly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CoordTargetDataset(Dataset):\n",
    "    x_fns:List[Path]; coords:List[List[float]]\n",
    "    def __post_init__(self): assert len(self.x_fns)==len(self.coords)\n",
    "    def __repr__(self): return f'{type(self).__name__} of len {len(self.x_fns)}'\n",
    "    def __len__(self): return len(self.x_fns)\n",
    "    def __getitem__(self, i): return open_image(self.x_fns[i]), self.coords[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ds = CoordTargetDataset(img_fns, pose_pnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine, coords, crop transform first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_coords(c, padding_mode):\n",
    "    \"Apply the padding mode to the coords\"\n",
    "    if padding_mode=='zeros' or padding_mode=='border':\n",
    "        mask = (c[:,0] >=-1) * (c[:,0] <=1) * (c[:,1] >=-1) * (c[:,1] <=1)\n",
    "        return c[mask]\n",
    "    elif padding_mode=='reflect':\n",
    "        c[c < -1] = -2 - c[c < -1]\n",
    "        c[c > 1] = 2  - c[c > 1]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_inv_mult(c,m):\n",
    "    \"Applies the inverse affine transform described in m\"\n",
    "    size = c.size()\n",
    "    c = c.view(-1,2)\n",
    "    a = torch.inverse(m[:2,:2].t()) \n",
    "    c = torch.addmm(-torch.mv(a,m[:2,2]), c, a) \n",
    "    return c.view(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Problem in affine: we must apply the inverse tranformation (affine_inv_mult function)\n",
    "- Problem in coord: we must also apply the inverse transformation (flag invert)\n",
    "- Probem in crop (and other transforms and the beginning): we need the size of the image where as if the object pass is a list of points, it doesn't have it. The TfmDatase adds at the beginning the size of the corresponding image as a first element, then we update it as it changes (with resize, pad or crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_affine(img, size=None, padding_mode='reflect', do_crop=False, aspect=None, mult=32,\n",
    "                  mats=None, func=None, crop_func=None, is_coord=False, **kwargs):\n",
    "    if size is not None and not is_listy(size):\n",
    "        size = listify(size,2) if aspect is None else get_crop_target(size, aspect, mult)\n",
    "    if (not mats) and func is None and size is None: return img\n",
    "    im_size = torch.Size((1,int(img[0][0]),int(img[0][1]))) if is_coord else img.size()\n",
    "    resize_target = get_resize_target(im_size, size, do_crop=do_crop)\n",
    "    c = img[1:] if is_coord else affine_grid(img, torch.eye(3), size=resize_target)\n",
    "    if func is not None: c = func(c, im_size, invert=is_coord)\n",
    "    if mats:\n",
    "        m = affines_mat(mats)\n",
    "        c = affine_mult(c, img.new_tensor(m)) if not is_coord else affine_inv_mult(c, img.new_tensor(m))\n",
    "    if is_coord: \n",
    "        res = pad_coords(c, padding_mode=padding_mode)\n",
    "        if resize_target is None: resize_target = im_size\n",
    "        res = torch.cat([torch.Tensor([resize_target[1],resize_target[2]]).float()[None], res],0)\n",
    "    else: \n",
    "        res = grid_sample(img, c, padding_mode=padding_mode, **kwargs)\n",
    "        if padding_mode=='zeros': padding_mode='constant'\n",
    "    if crop_func is not None: res = crop_func(res, size=size, padding_mode=padding_mode, is_coord=is_coord)\n",
    "    return res\n",
    "\n",
    "def apply_affine(mats=None, func=None, crop_func=None):\n",
    "    return partial(_apply_affine, mats=mats, func=func, crop_func=crop_func)\n",
    "\n",
    "nb_003a.apply_affine = apply_affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfmY = IntEnum('TfmY', 'No Pixel Mask Coord BBox')\n",
    "\n",
    "class TfmDataset(Dataset):\n",
    "    def __init__(self, ds:Dataset, tfms:Collection[Callable]=None, tfm_y:TfmY=TfmY.No, **kwargs):\n",
    "        self.ds,self.tfms,self.tfm_y,self.kwargs = ds,tfms,tfm_y,kwargs\n",
    "\n",
    "    def __len__(self): return len(self.ds)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x,y = self.ds[idx]\n",
    "        if self.tfms is not None:\n",
    "            tfm = apply_tfms(self.tfms)\n",
    "            if self.tfm_y == TfmY.Coord: \n",
    "                y = torch.cat([torch.Tensor([x.size(1),x.size(2)]).float()[None], y],0)\n",
    "            x = tfm(x, **self.kwargs)\n",
    "            if self.tfm_y != TfmY.No and y is not None:\n",
    "                coord,seg = (self.tfm_y == TfmY.Coord),(self.tfm_y == TfmY.Mask)\n",
    "                y = tfm(y, segmentation=seg, is_coord=coord, **self.kwargs)[1:]\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [rotate_tfm(degrees=(-30,30))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord, padding_mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pose(img, pnts, ax=None):\n",
    "    if ax is None: _,ax = plt.subplots()\n",
    "    ax.imshow(img.numpy().transpose(1,2,0))\n",
    "    w,h = img.size(2),img.size(1)\n",
    "    if len(pnts) != 0:\n",
    "        pnts = (pnts.numpy() + 1) * np.array([w/2,h/2])[None]\n",
    "        ax.scatter(pnts[:, 0], pnts[:, 1], s=10, marker='.', c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [zoom_tfm(scale=(1,2),row_pct=(0,1),col_pct=(0,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord, padding_mode='zeros', size=100, do_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole pipeline: we pass is_coord as an argument to every transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_tfm_func(pixel_func,lighting_func,affine_func,start_func, x, segmentation=False, is_coord=False, **kwargs):\n",
    "    if not np.any([pixel_func,lighting_func,affine_func,start_func]): return x\n",
    "    x = x.clone()\n",
    "    if start_func is not None:  x = start_func(x, is_coord=is_coord)\n",
    "    if affine_func is not None: x = affine_func(x, is_coord=is_coord, **kwargs)\n",
    "    if lighting_func is not None and not segmentation and not is_coord: x = lighting_func(x)\n",
    "    if pixel_func is not None: x = pixel_func(x, is_coord=is_coord)\n",
    "    return x\n",
    "\n",
    "def apply_tfms(tfms):\n",
    "    resolve_tfms(tfms)\n",
    "    grouped_tfms = dict_groupby(listify(tfms), lambda o: o.tfm_type)\n",
    "    start_tfms,affine_tfms,coord_tfms,pixel_tfms,lighting_tfms,crop_tfms = [\n",
    "        (grouped_tfms.get(o)) for o in TfmType]\n",
    "    lighting_func = apply_lighting(compose(lighting_tfms))\n",
    "    mats = [o() for o in listify(affine_tfms)]\n",
    "    affine_func = apply_affine(mats, func=compose(coord_tfms), crop_func=compose(crop_tfms))\n",
    "    return partial(_apply_tfm_func,\n",
    "        compose(pixel_tfms),lighting_func,affine_func,compose(start_tfms))\n",
    "\n",
    "nb_003a.apply_tfms = apply_tfms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_transform\n",
    "def pad(x, padding, mode='reflect', is_coord=False) -> TfmType.Start:\n",
    "    if is_coord:\n",
    "        pad = torch.Tensor([x[0][1]/(x[0][1] + 2*padding), x[0][0]/(x[0][0] + 2*padding)])\n",
    "        x[1:] = x[1:].mul_(pad[None])\n",
    "        x[0] += 2*padding\n",
    "        return x\n",
    "    else: return F.pad(x[None], (padding,)*4, mode=mode)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [pad_tfm(padding=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not implemented: the points aren't reflected (is it important?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_coords(c, row, col, rows, cols):\n",
    "    h,w = int(c[0][0]),int(c[0][1])\n",
    "    c = c[1:]\n",
    "    c.mul_(torch.Tensor([w/cols,h/rows])[None])\n",
    "    c.add_(-1 + torch.Tensor([w/cols-2*col/cols,h/rows-2*row/rows])[None])\n",
    "    c = pad_coords(c, padding_mode='zeros')\n",
    "    return torch.cat([torch.Tensor([rows,cols]).float()[None], c],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_transform\n",
    "def crop(x, size, is_coord=False, row_pct:uniform=0.5, col_pct:uniform=0.5) -> TfmType.Pixel:\n",
    "    size = listify(size,2)\n",
    "    rows,cols = size\n",
    "    img_size = torch.Size((1,int(x[0][0]),int(x[0][1]))) if is_coord else x.size()\n",
    "    row = int((img_size[1]-rows+1) * row_pct)\n",
    "    col = int((img_size[2]-cols+1) * col_pct)\n",
    "    if is_coord: return crop_coords(x,row,col,rows,cols)\n",
    "    return x[:, row:row+rows, col:col+cols].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [crop_tfm(size=200,row_pct=(0,1),col_pct=(0,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop pad transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_transform\n",
    "def crop_pad(x, size, padding_mode='reflect', is_coord=False,\n",
    "             row_pct:uniform = 0.5, col_pct:uniform = 0.5) -> TfmType.Crop:\n",
    "    size = listify(size,2)\n",
    "    rows,cols = size\n",
    "    img_size = torch.Size((1,int(x[0][0]),int(x[0][1]))) if is_coord else x.size()\n",
    "    if img_size[1]<rows or img_size[2]<cols:\n",
    "        row_pad = max((rows-img_size[1]+1)//2, 0)\n",
    "        col_pad = max((cols-img_size[2]+1)//2, 0)\n",
    "        new_size = torch.Size(img_size[0], img_size[1] + 2*row_pad, img_size[2] + 2*col_pad) \n",
    "        if is_coord: \n",
    "            pad = torch.Tensor([img_size[1]/new_size[1], img_size[2]/new_size[2]])\n",
    "            x[1:] = x[1:].mul_(pad[None])\n",
    "            x[0] += torch.Tensor([2*row_pad,2*col_pad])\n",
    "        else: x = F.pad(x[None], (col_pad,col_pad,row_pad,row_pad), mode=padding_mode)[0]\n",
    "        img_size = new_size\n",
    "    \n",
    "    row = int((img_size[1]-rows+1)*row_pct)\n",
    "    col = int((img_size[2]-cols+1)*col_pct)\n",
    "    if is_coord: return crop_coords(x,row,col,rows,cols)\n",
    "    x = x[:, row:row+rows, col:col+cols]\n",
    "    return x.contiguous() # without this, get NaN later - don't know why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [crop_pad_tfm(row_pct=(0,1),col_pct=(0,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord, padding_mode='zeros', do_crop=True, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coord transform (where we see the need for the invert transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_transform\n",
    "def tilt(c, img_size, invert=False, direction:rand_int=0, magnitude:uniform=0) -> TfmType.Coord:\n",
    "    orig_pts = [[-1,-1], [-1,1], [1,-1], [1,1]]\n",
    "    if direction == 0:   targ_pts = [[-1,-1], [-1,1], [1,-1-magnitude], [1,1+magnitude]]\n",
    "    elif direction == 1: targ_pts = [[-1,-1-magnitude], [-1,1+magnitude], [1,-1], [1,1]]\n",
    "    elif direction == 2: targ_pts = [[-1,-1], [-1-magnitude,1], [1,-1], [1+magnitude,1]]\n",
    "    elif direction == 3: targ_pts = [[-1-magnitude,-1], [-1,1], [1+magnitude,-1], [1,1]]  \n",
    "    coeffs = find_coeffs(targ_pts, orig_pts) if invert else find_coeffs(orig_pts, targ_pts)\n",
    "    return apply_perspective(c, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [tilt_tfm(direction=(0,3), magnitude=(-0.4,0.4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord, padding_mode='zeros', do_crop=True, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@reg_transform\n",
    "def flip_lr(x, is_coord=False) -> TfmType.Pixel: \n",
    "    if is_coord:\n",
    "        x[1:,0] = -x[1:,0]\n",
    "        return x\n",
    "    else: return x.flip(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_ds = TfmDataset(img_ds, tfms, TfmY.Coord, padding_mode='zeros', do_crop=True, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = plt.subplots(1,4, figsize=(12,6))\n",
    "for i,ax in enumerate(axs): show_pose(*tfm_ds[0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
