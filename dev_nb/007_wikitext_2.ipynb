{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_005 import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikitext 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset [here](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip) and unzip it so it's in the folder wikitext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'\n",
    "PATH=Path('data/wikitext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small helper function to read the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    tokens = []\n",
    "    with open(PATH/filename, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tokens.append(line.split() + [EOS])\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = read_file('wiki.train.tokens')\n",
    "valid_tok = read_file('wiki.valid.tokens')\n",
    "test_tok = read_file('wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760, 4358)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tok), len(valid_tok), len(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_tok[4][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 113161),\n",
       " (',', 99913),\n",
       " ('.', 73388),\n",
       " ('of', 56889),\n",
       " ('<unk>', 54625),\n",
       " ('and', 50603),\n",
       " ('in', 39453),\n",
       " ('to', 39190),\n",
       " ('<eos>', 36718),\n",
       " ('a', 34237)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = Counter(word for sent in train_tok for word in sent)\n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an id to each token and add the pad token (just in case we need it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in cnt.most_common()]\n",
    "itos.insert(0,'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos); vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the mapping from token to id then numericalizing our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda : 5, {w:i for i,w in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array([([stoi[w] for w in s]) for s in train_tok])\n",
    "valid_ids = np.array([([stoi[w] for w in s]) for s in valid_tok])\n",
    "test_ids = np.array([([stoi[w] for w in s]) for s in test_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataLoader():\n",
    "    \"A dataloader that iterates through batches while changing slightly the bptt.\"\n",
    "    def __init__(self, nums, bs, bptt, backwards=False):\n",
    "        self.bs,self.bptt,self.backwards = bs,bptt,backwards\n",
    "        self.data = self.batchify(nums)\n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            if self.i == 0: seq_len = self.bptt + 5 * 5\n",
    "            else:\n",
    "                bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "                seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "            res = self.get_batch(self.i, seq_len)\n",
    "            self.i += seq_len\n",
    "            self.iter += 1\n",
    "            yield res\n",
    "\n",
    "    def __len__(self): return self.n // self.bptt - 1\n",
    "\n",
    "    def batchify(self, data):\n",
    "        nb = data.shape[0] // self.bs\n",
    "        data = np.array(data[:nb*self.bs])\n",
    "        data = data.reshape(self.bs, -1).T\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return LongTensor(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source = self.data\n",
    "        seq_len = min(seq_len, len(source) - 1 - i)\n",
    "        return source[i:i+seq_len], source[i+1:i+1+seq_len].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 20,10\n",
    "train_dl = LMDataLoader(np.concatenate(train_ids), bs, bptt)\n",
    "valid_dl = LMDataLoader(np.concatenate(valid_ids), bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, device=None, tfms=None):\n",
    "        self.device = default_device if device is None else device\n",
    "        if not isinstance(train_dl, DeviceDataLoader): \n",
    "            train_dl = DeviceDataLoader(train_dl, self.device, progress_func=tqdm, tfms=tfms)\n",
    "        if not isinstance(valid_dl, DeviceDataLoader): \n",
    "            valid_dl = DeviceDataLoader(valid_dl, self.device, progress_func=tqdm, tfms=tfms)\n",
    "        self.train_dl,self.valid_dl = train_dl,valid_dl\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, train_tfm=None, valid_tfm=None, dl_tfms=None, bs=64, **kwargs):\n",
    "        train_dl = DeviceDataLoader.create(DatasetTfm(train_ds, train_tfm), bs, shuffle=True, \n",
    "                                           tfms=dl_tfms, **kwargs)\n",
    "        valid_dl = DeviceDataLoader.create(DatasetTfm(valid_ds, valid_tfm), bs*2, shuffle=False, \n",
    "                                           tfms=dl_tfms, **kwargs)\n",
    "        return cls(train_dl, valid_dl)\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(train_dl, DeviceDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, valid_dl, default_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the AWD-LSTM from [Stephen Merity](https://arxiv.org/abs/1708.02182). First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the averga of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor `1/(1-p)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    \"Returns a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\n",
    "    return x.new(*sz).bernoulli_(1-p)/(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 0., 2., 0., 0., 0., 2., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 2., 2., 0., 2., 2.],\n",
       "        [0., 2., 2., 0., 0., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 2., 2., 0., 2., 0., 2., 2., 0.],\n",
       "        [2., 2., 2., 2., 2., 0., 2., 2., 2., 2.],\n",
       "        [2., 2., 0., 2., 0., 2., 0., 0., 0., 0.],\n",
       "        [0., 2., 2., 0., 0., 2., 0., 2., 0., 0.],\n",
       "        [0., 2., 2., 0., 0., 0., 0., 0., 0., 2.],\n",
       "        [0., 2., 0., 0., 0., 2., 2., 2., 2., 0.],\n",
       "        [0., 0., 0., 0., 0., 2., 2., 2., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,10)\n",
    "dropout_mask(x, (10,10), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once with have a dropout mask `m`, applying the dropout to `x` is simply done by `x = x * m`. We create our own dropout mask and don't rely on pytorch dropout because we want to nullify the coefficients on the batch dimension but not the token dimension (aka the same coefficients are replaced by zero for each word in the sentence). \n",
    "\n",
    "Inside a RNN, a tensor x will have three dimensions: seq_len, bs, vocab_size, so we create a dropout mask for the last two dimensions and broadcast it to the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.p: return x\n",
    "        m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)\n",
    "        return m * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7618, -0.6116,  0.3955,  1.3657,  1.4364, -2.2120,  0.2474,\n",
       "           -0.3893,  0.4495,  1.2473],\n",
       "          [ 0.2402, -0.4327, -1.4691,  0.4653,  0.4280,  0.3873,  0.9973,\n",
       "            0.3174,  0.9244, -1.3500],\n",
       "          [-1.3865,  0.7202, -0.8552,  1.9565,  1.0824, -0.4621, -0.7159,\n",
       "            0.1479,  0.6398,  0.9142],\n",
       "          [ 0.4448, -0.9450,  0.0832,  0.0767,  0.9118,  2.2280,  1.0178,\n",
       "           -0.6625,  0.2360, -0.2408],\n",
       "          [ 0.3941, -1.6472, -0.5152,  0.4187, -0.1898,  0.8900, -0.7480,\n",
       "           -0.9121, -1.7869,  1.0283]],\n",
       " \n",
       "         [[ 0.5170,  0.6714, -0.0522, -0.3161,  1.2824, -0.0983, -1.5031,\n",
       "           -1.8706, -0.1500,  0.9089],\n",
       "          [ 1.3129,  0.7559, -0.2603,  1.1480, -0.0479, -0.7193, -1.2821,\n",
       "           -0.7878,  0.5105,  0.9481],\n",
       "          [ 1.1758, -0.4721, -2.1607, -0.8924,  0.6982,  0.4315, -0.2785,\n",
       "           -0.1542, -1.1626,  0.0448],\n",
       "          [ 2.0093, -0.0582, -1.2177,  0.3014,  0.8134,  0.2001,  0.1971,\n",
       "            0.7769, -0.3051,  0.4290],\n",
       "          [ 0.1914,  0.0329, -1.3884,  1.2543, -0.8739,  0.4197,  2.1740,\n",
       "           -0.4185, -1.1502, -0.1902]]]),\n",
       " tensor([[[-1.5235, -1.2233,  0.7911,  0.0000,  2.8729, -4.4240,  0.4948,\n",
       "           -0.0000,  0.8989,  2.4946],\n",
       "          [ 0.4804, -0.0000, -0.0000,  0.9306,  0.8560,  0.0000,  1.9946,\n",
       "            0.6348,  0.0000, -0.0000],\n",
       "          [-0.0000,  1.4404, -1.7104,  0.0000,  2.1648, -0.0000, -0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.8895, -0.0000,  0.1664,  0.1533,  1.8237,  0.0000,  0.0000,\n",
       "           -1.3251,  0.0000, -0.4817],\n",
       "          [ 0.0000, -0.0000, -1.0304,  0.0000, -0.3796,  1.7799, -1.4960,\n",
       "           -1.8241, -0.0000,  2.0566]],\n",
       " \n",
       "         [[ 1.0339,  1.3429, -0.1045, -0.0000,  2.5648, -0.1965, -3.0063,\n",
       "           -0.0000, -0.3000,  1.8179],\n",
       "          [ 2.6258,  0.0000, -0.0000,  2.2959, -0.0958, -0.0000, -2.5642,\n",
       "           -1.5755,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.9442, -4.3213, -0.0000,  1.3964,  0.0000, -0.0000,\n",
       "           -0.0000, -0.0000,  0.0000],\n",
       "          [ 4.0186, -0.0000, -2.4354,  0.6027,  1.6269,  0.0000,  0.0000,\n",
       "            1.5538, -0.0000,  0.8581],\n",
       "          [ 0.0000,  0.0000, -2.7769,  0.0000, -1.7478,  0.8395,  4.3481,\n",
       "           -0.8370, -0.0000, -0.3805]]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_test = RNNDropout(0.5)\n",
    "x = torch.randn(2,5,10)\n",
    "x, dp_test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDropout(nn.Module):\n",
    "    \"A module that warps another layer in which some weights will be replaced by 0 during training.\"\n",
    "    \n",
    "    def __init__(self, module, dropout, layer_names=['weight_hh_l0']):\n",
    "        super().__init__()\n",
    "        self.module,self.dropout,self.layer_names = module,dropout,layer_names\n",
    "    \n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            w1 = F.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "            module._parameters[layer] = w1\n",
    "            \n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightDropout(\n",
       "  (module): LSTM(20, 20)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(20, 20)\n",
    "dp_module = WeightDropout(module, 0.5)\n",
    "dp_module.reset()\n",
    "opt = optim.SGD(dp_module.parameters(), 10)\n",
    "dp_module.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,5,20)\n",
    "x.requires_grad_(requires_grad=True)\n",
    "h = (torch.zeros(1,5,20), torch.zeros(1,5,20))\n",
    "for _ in range(5): x,h = dp_module(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2956, -0.0000,  0.0000,  ...,  0.0000,  0.3535,  0.4001],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0790,  0.0856, -0.0000],\n",
       "         [-0.0000, -0.0000, -0.0000,  ..., -0.3327,  0.1975, -0.0000],\n",
       "         ...,\n",
       "         [ 0.0000, -0.1207,  0.0833,  ...,  0.4233,  0.0000, -0.4075],\n",
       "         [-0.0000,  0.2056, -0.2639,  ..., -0.1691,  0.0000,  0.0000],\n",
       "         [-0.0298,  0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.2821]],\n",
       "        grad_fn=<MulBackward0>), Parameter containing:\n",
       " tensor([[ 0.1478, -0.0745,  0.1796,  ...,  0.0533,  0.1767,  0.2001],\n",
       "         [ 0.2082,  0.0146,  0.0523,  ...,  0.0395,  0.0428, -0.1395],\n",
       "         [-0.0663, -0.1542, -0.0367,  ..., -0.1663,  0.0988, -0.0174],\n",
       "         ...,\n",
       "         [ 0.0941, -0.0604,  0.0417,  ...,  0.2117,  0.2185, -0.2037],\n",
       "         [-0.0441,  0.1028, -0.1320,  ..., -0.0846,  0.1436,  0.1333],\n",
       "         [-0.0149,  0.0645,  0.0770,  ..., -0.1577,  0.0262,  0.1411]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, 'weight_hh_l0'),getattr(dp_module,'weight_hh_l0_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(0,20,(10,)).long()\n",
    "loss = F.nll_loss(x.view(-1,20), target)\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, tensor([[ 0.0000, -0.0001,  0.0006,  ...,  0.0002,  0.0001,  0.0003],\n",
       "         [ 0.0002,  0.0002, -0.0000,  ...,  0.0000,  0.0002, -0.0002],\n",
       "         [-0.0001, -0.0001,  0.0002,  ...,  0.0001,  0.0003,  0.0001],\n",
       "         ...,\n",
       "         [ 0.0001,  0.0001, -0.0000,  ..., -0.0001,  0.0002, -0.0001],\n",
       "         [-0.0003, -0.0005,  0.0008,  ...,  0.0000, -0.0003,  0.0004],\n",
       "         [ 0.0001,  0.0000, -0.0000,  ...,  0.0000,  0.0000,  0.0002]]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w_raw = getattr(dp_module.module, 'weight_hh_l0'),getattr(dp_module,'weight_hh_l0_raw')\n",
    "w.grad, w_raw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0966,  0.1848,  0.0000,  ...,  0.0870,  0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.3470,  ..., -0.4012,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.2064,  ..., -0.0000,  0.1284, -0.1940],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0622,  0.2181,  ..., -0.1798, -0.0411,  0.3447],\n",
       "         [ 0.2740,  0.0000, -0.1230,  ...,  0.0000, -0.0000, -0.3689],\n",
       "         [-0.0000, -0.0000,  0.0082,  ..., -0.2975,  0.0668, -0.1271]],\n",
       "        grad_fn=<MulBackward0>), Parameter containing:\n",
       " tensor([[-0.0471,  0.0914,  0.0246,  ...,  0.0449,  0.1147,  0.0845],\n",
       "         [-0.0043, -0.0756, -0.1734,  ..., -0.2046,  0.0081, -0.1604],\n",
       "         [ 0.0182,  0.2126, -0.1032,  ..., -0.1206,  0.0640, -0.0966],\n",
       "         ...,\n",
       "         [ 0.2082, -0.0453,  0.1092,  ..., -0.0819, -0.0221,  0.1757],\n",
       "         [ 0.1418,  0.0751, -0.0614,  ...,  0.0478, -0.0686, -0.1859],\n",
       "         [-0.1811, -0.2028,  0.0045,  ..., -0.1474,  0.0329, -0.0629]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, 'weight_hh_l0'),getattr(dp_module,'weight_hh_l0_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb):\n",
    "        super().__init__()\n",
    "        self.emb = emb\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, dropout=0.1, scale=None):\n",
    "        if dropout:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, dropout)\n",
    "            masked_emb_weight = mask * self.emb.weight\n",
    "        else: masked_emb_weight = self.emb.weight\n",
    "        if scale: masked_emb_weight = scale * masked_emb_weight\n",
    "        return F.embedding(words, masked_emb_weight, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = nn.Embedding(100,20, padding_idx=0)\n",
    "enc_dp = EmbeddingDropout(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,100,(25,)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "         -0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [-3.0746,  0.0532, -0.9969, -1.5401, -2.2785, -3.1142, -5.2356, -1.9817,\n",
       "         -0.4740,  0.7455, -0.4058,  1.4690,  0.7676,  0.4037, -0.0132, -4.1723,\n",
       "          0.8027, -1.0026, -1.9151,  2.5202],\n",
       "        [-2.6915, -1.3094,  2.5535, -2.0829,  1.0566, -2.4929,  0.1980,  2.1521,\n",
       "         -1.9781,  0.4168, -1.6336,  1.8876,  1.8933,  2.5815,  1.3373,  0.4616,\n",
       "         -1.5860,  2.5099,  2.6191,  0.9438],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.2953,  1.9750,  3.3688,  2.1726, -1.6868,  0.7382,  0.7309,  2.3460,\n",
       "         -3.7565, -0.3633, -0.3705,  1.0253,  2.0195,  0.2571, -1.9339, -2.8782,\n",
       "          3.3939,  2.9073,  0.4097,  1.9072],\n",
       "        [ 0.5697, -0.8794, -1.1051,  0.7381,  3.2064,  1.1869,  0.6103,  4.0163,\n",
       "         -2.8680,  0.2569, -0.1953,  1.7844, -0.6983, -3.0204,  0.1995,  4.6198,\n",
       "         -2.4291,  0.9621,  2.2416,  0.8460],\n",
       "        [ 1.1072, -0.7850, -1.2569, -1.8064, -0.2758,  1.5919,  2.1156,  0.8011,\n",
       "          3.1572,  0.0501, -1.6282,  2.1985, -0.1718,  1.1339, -1.5130,  2.3161,\n",
       "          5.8747,  2.9110,  4.0537,  0.1406],\n",
       "        [-0.6216, -2.1223,  0.2563, -1.2750, -2.4240, -0.9206, -0.4679,  1.0163,\n",
       "          0.4720, -2.3387,  0.6266, -1.5123,  2.9876,  2.5822,  0.8812,  0.4616,\n",
       "          2.7637,  3.2318,  0.5398, -0.5174],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "         -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.2953,  1.9750,  3.3688,  2.1726, -1.6868,  0.7382,  0.7309,  2.3460,\n",
       "         -3.7565, -0.3633, -0.3705,  1.0253,  2.0195,  0.2571, -1.9339, -2.8782,\n",
       "          3.3939,  2.9073,  0.4097,  1.9072],\n",
       "        [ 0.3960, -0.7506,  2.3170,  1.6889,  2.0988,  1.9712, -1.8320,  5.1785,\n",
       "         -0.7022, -0.7580, -2.3049, -0.6902,  2.4481, -0.8291, -0.9982,  0.9262,\n",
       "         -1.8147,  2.5985,  2.0517,  1.0900],\n",
       "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [-0.3157,  0.0871,  1.0549,  1.5682, -0.6026, -0.4858,  2.3007, -0.3649,\n",
       "          4.2785, -1.6199,  0.1949, -1.9335, -2.0696,  0.4727, -1.9461, -1.0550,\n",
       "         -0.5563, -2.6314, -0.6478, -1.6041],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "         -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "         -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "          0.0000, -0.0000,  0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000,  0.0000],\n",
       "        [-1.4331,  1.2885, -1.8112, -3.6487,  1.5365,  2.2097, -0.1210, -4.0178,\n",
       "         -0.9067,  0.2416,  1.7297,  1.0364,  0.1152,  2.4813,  0.8581,  0.7856,\n",
       "         -0.6928, -0.6553,  0.0081,  0.1336],\n",
       "        [-0.3157,  0.0871,  1.0549,  1.5682, -0.6026, -0.4858,  2.3007, -0.3649,\n",
       "          4.2785, -1.6199,  0.1949, -1.9335, -2.0696,  0.4727, -1.9461, -1.0550,\n",
       "         -0.5563, -2.6314, -0.6478, -1.6041],\n",
       "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "          0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [-1.6983, -2.0710, -2.8568, -0.2215, -2.4582,  1.1068,  3.1352, -2.2469,\n",
       "         -4.7432,  2.0595,  0.4809,  1.3533, -3.3434, -0.4067,  1.3954, -2.2860,\n",
       "          0.2896, -2.1592,  0.3023,  0.2292],\n",
       "        [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.2118, -1.7264,  0.1159, -0.5105, -1.1614, -1.7116, -0.4231,  0.1977,\n",
       "          0.1689, -3.6297,  4.3669, -0.0019, -2.1002,  0.5867,  1.3009, -2.6246,\n",
       "          1.1866,  0.1172,  1.0763, -2.1137],\n",
       "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0000, -0.0000,  0.0000]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dp(x, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"Detaches h from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCore(nn.Module):\n",
    "    \"AWD-LSTM/QRNN inspired by https://arxiv.org/abs/1708.02182\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bs,self.qrnn,self.ndir = 1, qrnn,(2 if bidir else 1)\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.dropoute = emb_sz,n_hid,n_layers,dropoute\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.dp_encoder = EmbeddingDropout(self.encoder)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .torchqrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(n_layers)]\n",
    "            if wdrop:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDropout(rnn.linear, wdrop, layer_names=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if wdrop: self.rnns = [WeightDropout(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropouti = RNNDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([RNNDropout(dropouth) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        emb = self.dp_encoder(input, dropout=self.dropoute if self.training else 0)\n",
    "        emb = self.dropouti(emb)\n",
    "        raw_output = emb\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "            #with warnings.catch_warnings():\n",
    "            #    warnings.simplefilter(\"ignore\")\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = drop(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNN_Core module\"\n",
    "    \n",
    "    initrange=0.1\n",
    "    \n",
    "    def __init__(self, n_out, n_hid, dropout, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropout = RNNDropout(dropout)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialRNN(nn.Sequential):\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, tie_weights=True, qrnn=False, bias=True,\n",
    "                 dropout=0.4, dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "    \"To create a full AWD-LSTM\"\n",
    "    rnn_enc = RNNCore(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, qrnn=qrnn,\n",
    "                 dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, dropout, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = get_language_model(500, 20, 100, 2, 0, wdrop=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 500, (10,5)).long()\n",
    "z = tst_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Callbacks to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RNNTrainer(Callback):\n",
    "    learn:Learner\n",
    "    bptt:int\n",
    "    alpha:float=0.\n",
    "    beta:float=0.\n",
    "    \n",
    "    def on_loss_begin(self, last_output, **kwargs):\n",
    "        #Save the extra outputs for later and only returns the true output.\n",
    "        self.raw_out,self.out = last_output[1],last_output[2]\n",
    "        return last_output[0]\n",
    "    \n",
    "    def on_backward_begin(self, last_loss, last_input, last_output, **kwargs):\n",
    "        #Adjusts the lr to the bptt selected\n",
    "        self.learn.opt.lr *= last_input.size(0) / self.bptt\n",
    "        #AR and TAR\n",
    "        if self.alpha != 0.:  last_loss += (self.alpha * self.out[-1].pow(2).mean()).sum()\n",
    "        if self.beta != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h)>1: last_loss += (self.beta * (h[1:] - h[:-1]).pow(2).mean()).sum()\n",
    "        return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WeightDrop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-9932332f0aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_language_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-4c0e068515ff>\u001b[0m in \u001b[0;36mget_language_model\u001b[1;34m(vocab_sz, emb_sz, n_hid, n_layers, pad_token, tie_weights, qrnn, bias, dropout, dropouth, dropouti, dropoute, wdrop)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"To create a full AWD-LSTM\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     rnn_enc = RNNCore(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, qrnn=qrnn,\n\u001b[1;32m----> 5\u001b[1;33m                  dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtie_weights\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mSequentialRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtie_encoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-2eb3b9311abe>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, bidir, dropouth, dropouti, dropoute, wdrop, qrnn)\u001b[0m\n\u001b[0;32m     23\u001b[0m             self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n\u001b[0;32m     24\u001b[0m                 1, bidirectional=bidir) for l in range(n_layers)]\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mwdrop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mWeightDrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwdrop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitrange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-2eb3b9311abe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m             self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n\u001b[0;32m     24\u001b[0m                 1, bidirectional=bidir) for l in range(n_layers)]\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mwdrop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mWeightDrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwdrop\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitrange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WeightDrop' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_language_model(vocab_size, 20, 100, 2, 0, wdrop=0.5)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f0c2e477494f438a8777d7ea606f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50eecf5c50aa41b695be8ff1a844d17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(177)loss_batch()\n",
      "-> if cb_handler is None: cb_handler = CallbackHandler([])\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(178)loss_batch()\n",
      "-> out = model(xb)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(179)loss_batch()\n",
      "-> out = cb_handler.on_loss_begin(out)\n",
      "(Pdb) print(out)\n",
      "(tensor([[-0.0136,  0.0158,  0.0056,  ...,  0.0244, -0.0030, -0.0066],\n",
      "        [-0.0152,  0.0105,  0.0129,  ...,  0.0475, -0.0089, -0.0050],\n",
      "        [-0.0122,  0.0210,  0.0035,  ...,  0.0254,  0.0094, -0.0128],\n",
      "        ...,\n",
      "        [-0.0135,  0.0422, -0.0215,  ...,  0.0446, -0.0224,  0.0163],\n",
      "        [-0.0208,  0.0494,  0.0019,  ...,  0.0529, -0.0074, -0.0249],\n",
      "        [ 0.0011,  0.0051, -0.0012,  ...,  0.0315, -0.0532,  0.0060]],\n",
      "       device='cuda:0', grad_fn=<ThAddmmBackward>), [tensor([[[-0.0076, -0.0034,  0.0288,  ...,  0.0372, -0.0065, -0.0178],\n",
      "         [-0.0176, -0.0076,  0.0152,  ...,  0.0344, -0.0025, -0.0173],\n",
      "         [-0.0139, -0.0178,  0.0297,  ...,  0.0465,  0.0027, -0.0298],\n",
      "         ...,\n",
      "         [-0.0163, -0.0018,  0.0343,  ...,  0.0266, -0.0063,  0.0035],\n",
      "         [-0.0153, -0.0053,  0.0145,  ...,  0.0305,  0.0026, -0.0165],\n",
      "         [-0.0148, -0.0043,  0.0351,  ...,  0.0300, -0.0039, -0.0163]],\n",
      "\n",
      "        [[-0.0114, -0.0048,  0.0522,  ...,  0.0484, -0.0046, -0.0148],\n",
      "         [-0.0254, -0.0131,  0.0351,  ...,  0.0449,  0.0040, -0.0103],\n",
      "         [-0.0288, -0.0212,  0.0256,  ...,  0.0400,  0.0147, -0.0229],\n",
      "         ...,\n",
      "         [-0.0268, -0.0088,  0.0464,  ...,  0.0421,  0.0019, -0.0041],\n",
      "         [-0.0321, -0.0210,  0.0230,  ...,  0.0379,  0.0129, -0.0260],\n",
      "         [-0.0259, -0.0151,  0.0592,  ...,  0.0481, -0.0041, -0.0232]],\n",
      "\n",
      "        [[-0.0256, -0.0129,  0.0479,  ...,  0.0461,  0.0013, -0.0061],\n",
      "         [-0.0412, -0.0180,  0.0404,  ...,  0.0500,  0.0044, -0.0094],\n",
      "         [-0.0328, -0.0213,  0.0423,  ...,  0.0470,  0.0040, -0.0203],\n",
      "         ...,\n",
      "         [-0.0395, -0.0142,  0.0539,  ...,  0.0526,  0.0076, -0.0038],\n",
      "         [-0.0377, -0.0243,  0.0366,  ...,  0.0496,  0.0102, -0.0207],\n",
      "         [-0.0337, -0.0138,  0.0611,  ...,  0.0475, -0.0020, -0.0310]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0330, -0.0272,  0.0683,  ...,  0.0497,  0.0112,  0.0007],\n",
      "         [-0.0443, -0.0276,  0.0461,  ...,  0.0615,  0.0088,  0.0017],\n",
      "         [-0.0511, -0.0319,  0.0501,  ...,  0.0543,  0.0224, -0.0108],\n",
      "         ...,\n",
      "         [-0.0485, -0.0184,  0.0505,  ...,  0.0544,  0.0163, -0.0030],\n",
      "         [-0.0614, -0.0299,  0.0472,  ...,  0.0605,  0.0141, -0.0093],\n",
      "         [-0.0491, -0.0388,  0.0283,  ...,  0.0666,  0.0191, -0.0064]],\n",
      "\n",
      "        [[-0.0385, -0.0266,  0.0571,  ...,  0.0457,  0.0100,  0.0052],\n",
      "         [-0.0521, -0.0334,  0.0508,  ...,  0.0549,  0.0122,  0.0048],\n",
      "         [-0.0496, -0.0335,  0.0463,  ...,  0.0579,  0.0207, -0.0054],\n",
      "         ...,\n",
      "         [-0.0468, -0.0144,  0.0389,  ...,  0.0554,  0.0095, -0.0036],\n",
      "         [-0.0627, -0.0309,  0.0488,  ...,  0.0657,  0.0147, -0.0085],\n",
      "         [-0.0523, -0.0378,  0.0382,  ...,  0.0648,  0.0181, -0.0062]],\n",
      "\n",
      "        [[-0.0331, -0.0216,  0.0594,  ...,  0.0588,  0.0038, -0.0119],\n",
      "         [-0.0468, -0.0289,  0.0493,  ...,  0.0578,  0.0062,  0.0081],\n",
      "         [-0.0458, -0.0347,  0.0365,  ...,  0.0605,  0.0267, -0.0113],\n",
      "         ...,\n",
      "         [-0.0530, -0.0231,  0.0325,  ...,  0.0633,  0.0058, -0.0136],\n",
      "         [-0.0641, -0.0317,  0.0501,  ...,  0.0684,  0.0152, -0.0074],\n",
      "         [-0.0497, -0.0422,  0.0487,  ...,  0.0608,  0.0196, -0.0142]]],\n",
      "       device='cuda:0', grad_fn=<StackBackward>), tensor([[[-0.0191, -0.0070,  0.0308,  ...,  0.0326,  0.0121, -0.0446],\n",
      "         [-0.0227, -0.0074,  0.0337,  ...,  0.0118,  0.0157, -0.0481],\n",
      "         [-0.0218, -0.0025,  0.0349,  ...,  0.0195,  0.0090, -0.0456],\n",
      "         ...,\n",
      "         [-0.0263, -0.0151,  0.0237,  ...,  0.0190,  0.0192, -0.0455],\n",
      "         [-0.0221, -0.0110,  0.0267,  ...,  0.0368,  0.0115, -0.0452],\n",
      "         [-0.0229, -0.0223,  0.0259,  ...,  0.0259,  0.0001, -0.0486]],\n",
      "\n",
      "        [[-0.0234, -0.0082,  0.0418,  ...,  0.0466,  0.0209, -0.0797],\n",
      "         [-0.0238, -0.0060,  0.0451,  ...,  0.0092,  0.0262, -0.0851],\n",
      "         [-0.0288, -0.0016,  0.0493,  ...,  0.0233,  0.0191, -0.0823],\n",
      "         ...,\n",
      "         [-0.0377, -0.0182,  0.0340,  ...,  0.0201,  0.0339, -0.0815],\n",
      "         [-0.0296, -0.0217,  0.0336,  ...,  0.0508,  0.0209, -0.0835],\n",
      "         [-0.0330, -0.0297,  0.0343,  ...,  0.0359,  0.0031, -0.0895]],\n",
      "\n",
      "        [[-0.0266, -0.0083,  0.0435,  ...,  0.0543,  0.0278, -0.1019],\n",
      "         [-0.0252, -0.0058,  0.0499,  ...,  0.0010,  0.0326, -0.1101],\n",
      "         [-0.0310,  0.0011,  0.0606,  ...,  0.0230,  0.0202, -0.1040],\n",
      "         ...,\n",
      "         [-0.0473, -0.0215,  0.0349,  ...,  0.0166,  0.0408, -0.1064],\n",
      "         [-0.0327, -0.0300,  0.0323,  ...,  0.0596,  0.0243, -0.1069],\n",
      "         [-0.0358, -0.0277,  0.0385,  ...,  0.0404,  0.0021, -0.1196]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0237,  0.0079,  0.0557,  ...,  0.0561,  0.0378, -0.1346],\n",
      "         [-0.0215,  0.0193,  0.0562,  ..., -0.0131,  0.0367, -0.1408],\n",
      "         [-0.0300,  0.0096,  0.0612,  ...,  0.0347,  0.0262, -0.1409],\n",
      "         ...,\n",
      "         [-0.0475, -0.0029,  0.0359,  ...,  0.0467,  0.0519, -0.1513],\n",
      "         [-0.0475, -0.0344,  0.0201,  ...,  0.0712,  0.0368, -0.1420],\n",
      "         [-0.0421, -0.0255,  0.0258,  ...,  0.0510,  0.0130, -0.1538]],\n",
      "\n",
      "        [[-0.0253,  0.0073,  0.0524,  ...,  0.0604,  0.0377, -0.1353],\n",
      "         [-0.0235,  0.0122,  0.0537,  ..., -0.0164,  0.0339, -0.1446],\n",
      "         [-0.0322,  0.0063,  0.0608,  ...,  0.0250,  0.0255, -0.1406],\n",
      "         ...,\n",
      "         [-0.0466, -0.0050,  0.0351,  ...,  0.0478,  0.0494, -0.1514],\n",
      "         [-0.0492, -0.0312,  0.0204,  ...,  0.0738,  0.0342, -0.1415],\n",
      "         [-0.0427, -0.0276,  0.0267,  ...,  0.0458,  0.0100, -0.1519]],\n",
      "\n",
      "        [[-0.0286,  0.0110,  0.0587,  ...,  0.0620,  0.0358, -0.1349],\n",
      "         [-0.0227,  0.0091,  0.0525,  ..., -0.0203,  0.0337, -0.1429],\n",
      "         [-0.0286,  0.0136,  0.0617,  ...,  0.0227,  0.0260, -0.1404],\n",
      "         ...,\n",
      "         [-0.0478, -0.0057,  0.0380,  ...,  0.0451,  0.0477, -0.1487],\n",
      "         [-0.0506, -0.0285,  0.0211,  ...,  0.0761,  0.0328, -0.1409],\n",
      "         [-0.0414, -0.0238,  0.0275,  ...,  0.0438,  0.0077, -0.1559]]],\n",
      "       device='cuda:0', grad_fn=<StackBackward>)], [tensor([[[-0.0109, -0.0000,  0.0412,  ...,  0.0000, -0.0000, -0.0254],\n",
      "         [-0.0251, -0.0000,  0.0218,  ...,  0.0491, -0.0000, -0.0247],\n",
      "         [-0.0000, -0.0255,  0.0000,  ...,  0.0664,  0.0039, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0380, -0.0000,  0.0049],\n",
      "         [-0.0218, -0.0075,  0.0000,  ...,  0.0000,  0.0037, -0.0235],\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0233]],\n",
      "\n",
      "        [[-0.0162, -0.0000,  0.0746,  ...,  0.0000, -0.0000, -0.0212],\n",
      "         [-0.0363, -0.0000,  0.0502,  ...,  0.0641,  0.0000, -0.0147],\n",
      "         [-0.0000, -0.0302,  0.0000,  ...,  0.0572,  0.0210, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0601,  0.0000, -0.0058],\n",
      "         [-0.0458, -0.0300,  0.0000,  ...,  0.0000,  0.0184, -0.0372],\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0331]],\n",
      "\n",
      "        [[-0.0365, -0.0000,  0.0684,  ...,  0.0000,  0.0000, -0.0087],\n",
      "         [-0.0588, -0.0000,  0.0577,  ...,  0.0714,  0.0000, -0.0135],\n",
      "         [-0.0000, -0.0304,  0.0000,  ...,  0.0672,  0.0057, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0751,  0.0000, -0.0055],\n",
      "         [-0.0539, -0.0347,  0.0000,  ...,  0.0000,  0.0146, -0.0296],\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000, -0.0000, -0.0443]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0471, -0.0000,  0.0975,  ...,  0.0000,  0.0000,  0.0010],\n",
      "         [-0.0633, -0.0000,  0.0659,  ...,  0.0879,  0.0000,  0.0024],\n",
      "         [-0.0000, -0.0455,  0.0000,  ...,  0.0776,  0.0320, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0778,  0.0000, -0.0043],\n",
      "         [-0.0877, -0.0428,  0.0000,  ...,  0.0000,  0.0201, -0.0133],\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0091]],\n",
      "\n",
      "        [[-0.0550, -0.0000,  0.0815,  ...,  0.0000,  0.0000,  0.0075],\n",
      "         [-0.0745, -0.0000,  0.0725,  ...,  0.0784,  0.0000,  0.0069],\n",
      "         [-0.0000, -0.0478,  0.0000,  ...,  0.0827,  0.0295, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0791,  0.0000, -0.0052],\n",
      "         [-0.0896, -0.0441,  0.0000,  ...,  0.0000,  0.0210, -0.0121],\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0088]],\n",
      "\n",
      "        [[-0.0473, -0.0000,  0.0849,  ...,  0.0000,  0.0000, -0.0171],\n",
      "         [-0.0668, -0.0000,  0.0704,  ...,  0.0826,  0.0000,  0.0115],\n",
      "         [-0.0000, -0.0496,  0.0000,  ...,  0.0864,  0.0381, -0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0905,  0.0000, -0.0194],\n",
      "         [-0.0916, -0.0453,  0.0000,  ...,  0.0000,  0.0217, -0.0106],\n",
      "         [-0.0000, -0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0202]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>), tensor([[[-0.0191, -0.0070,  0.0308,  ...,  0.0326,  0.0121, -0.0446],\n",
      "         [-0.0227, -0.0074,  0.0337,  ...,  0.0118,  0.0157, -0.0481],\n",
      "         [-0.0218, -0.0025,  0.0349,  ...,  0.0195,  0.0090, -0.0456],\n",
      "         ...,\n",
      "         [-0.0263, -0.0151,  0.0237,  ...,  0.0190,  0.0192, -0.0455],\n",
      "         [-0.0221, -0.0110,  0.0267,  ...,  0.0368,  0.0115, -0.0452],\n",
      "         [-0.0229, -0.0223,  0.0259,  ...,  0.0259,  0.0001, -0.0486]],\n",
      "\n",
      "        [[-0.0234, -0.0082,  0.0418,  ...,  0.0466,  0.0209, -0.0797],\n",
      "         [-0.0238, -0.0060,  0.0451,  ...,  0.0092,  0.0262, -0.0851],\n",
      "         [-0.0288, -0.0016,  0.0493,  ...,  0.0233,  0.0191, -0.0823],\n",
      "         ...,\n",
      "         [-0.0377, -0.0182,  0.0340,  ...,  0.0201,  0.0339, -0.0815],\n",
      "         [-0.0296, -0.0217,  0.0336,  ...,  0.0508,  0.0209, -0.0835],\n",
      "         [-0.0330, -0.0297,  0.0343,  ...,  0.0359,  0.0031, -0.0895]],\n",
      "\n",
      "        [[-0.0266, -0.0083,  0.0435,  ...,  0.0543,  0.0278, -0.1019],\n",
      "         [-0.0252, -0.0058,  0.0499,  ...,  0.0010,  0.0326, -0.1101],\n",
      "         [-0.0310,  0.0011,  0.0606,  ...,  0.0230,  0.0202, -0.1040],\n",
      "         ...,\n",
      "         [-0.0473, -0.0215,  0.0349,  ...,  0.0166,  0.0408, -0.1064],\n",
      "         [-0.0327, -0.0300,  0.0323,  ...,  0.0596,  0.0243, -0.1069],\n",
      "         [-0.0358, -0.0277,  0.0385,  ...,  0.0404,  0.0021, -0.1196]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0237,  0.0079,  0.0557,  ...,  0.0561,  0.0378, -0.1346],\n",
      "         [-0.0215,  0.0193,  0.0562,  ..., -0.0131,  0.0367, -0.1408],\n",
      "         [-0.0300,  0.0096,  0.0612,  ...,  0.0347,  0.0262, -0.1409],\n",
      "         ...,\n",
      "         [-0.0475, -0.0029,  0.0359,  ...,  0.0467,  0.0519, -0.1513],\n",
      "         [-0.0475, -0.0344,  0.0201,  ...,  0.0712,  0.0368, -0.1420],\n",
      "         [-0.0421, -0.0255,  0.0258,  ...,  0.0510,  0.0130, -0.1538]],\n",
      "\n",
      "        [[-0.0253,  0.0073,  0.0524,  ...,  0.0604,  0.0377, -0.1353],\n",
      "         [-0.0235,  0.0122,  0.0537,  ..., -0.0164,  0.0339, -0.1446],\n",
      "         [-0.0322,  0.0063,  0.0608,  ...,  0.0250,  0.0255, -0.1406],\n",
      "         ...,\n",
      "         [-0.0466, -0.0050,  0.0351,  ...,  0.0478,  0.0494, -0.1514],\n",
      "         [-0.0492, -0.0312,  0.0204,  ...,  0.0738,  0.0342, -0.1415],\n",
      "         [-0.0427, -0.0276,  0.0267,  ...,  0.0458,  0.0100, -0.1519]],\n",
      "\n",
      "        [[-0.0286,  0.0110,  0.0587,  ...,  0.0620,  0.0358, -0.1349],\n",
      "         [-0.0227,  0.0091,  0.0525,  ..., -0.0203,  0.0337, -0.1429],\n",
      "         [-0.0286,  0.0136,  0.0617,  ...,  0.0227,  0.0260, -0.1404],\n",
      "         ...,\n",
      "         [-0.0478, -0.0057,  0.0380,  ...,  0.0451,  0.0477, -0.1487],\n",
      "         [-0.0506, -0.0285,  0.0211,  ...,  0.0761,  0.0328, -0.1409],\n",
      "         [-0.0414, -0.0238,  0.0275,  ...,  0.0438,  0.0077, -0.1559]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<StackBackward>)])\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(180)loss_batch()\n",
      "-> loss = loss_fn(out, yb)\n",
      "(Pdb) print(out)\n",
      "tensor([[-0.0136,  0.0158,  0.0056,  ...,  0.0244, -0.0030, -0.0066],\n",
      "        [-0.0152,  0.0105,  0.0129,  ...,  0.0475, -0.0089, -0.0050],\n",
      "        [-0.0122,  0.0210,  0.0035,  ...,  0.0254,  0.0094, -0.0128],\n",
      "        ...,\n",
      "        [-0.0135,  0.0422, -0.0215,  ...,  0.0446, -0.0224,  0.0163],\n",
      "        [-0.0208,  0.0494,  0.0019,  ...,  0.0529, -0.0074, -0.0249],\n",
      "        [ 0.0011,  0.0051, -0.0012,  ...,  0.0315, -0.0532,  0.0060]],\n",
      "       device='cuda:0', grad_fn=<ThAddmmBackward>)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(181)loss_batch()\n",
      "-> mets = [f(out,yb).item() for f in metrics] if metrics is not None else []\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(183)loss_batch()\n",
      "-> if opt is not None:\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(184)loss_batch()\n",
      "-> loss = cb_handler.on_backward_begin(loss)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(185)loss_batch()\n",
      "-> loss.backward()\n",
      "(Pdb) print(loss)\n",
      "tensor(10.4286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "(Pdb) c\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(177)loss_batch()\n",
      "-> if cb_handler is None: cb_handler = CallbackHandler([])\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(178)loss_batch()\n",
      "-> out = model(xb)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(179)loss_batch()\n",
      "-> out = cb_handler.on_loss_begin(out)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(180)loss_batch()\n",
      "-> loss = loss_fn(out, yb)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(181)loss_batch()\n",
      "-> mets = [f(out,yb).item() for f in metrics] if metrics is not None else []\n",
      "(Pdb) print(loss)\n",
      "tensor(9.0275, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(183)loss_batch()\n",
      "-> if opt is not None:\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(184)loss_batch()\n",
      "-> loss = cb_handler.on_backward_begin(loss)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(185)loss_batch()\n",
      "-> loss.backward()\n",
      "(Pdb) print(loss)\n",
      "tensor(9.3069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(186)loss_batch()\n",
      "-> cb_handler.on_backward_end()\n",
      "(Pdb) n\n",
      "> d:\\work\\deeplearning\\fastai_v1\\dev_nb\\nb_004.py(187)loss_batch()\n",
      "-> opt.step()\n",
      "(Pdb) model.encoder\n",
      "*** AttributeError: 'SequentialRNN' object has no attribute 'encoder'\n",
      "(Pdb) model.enc\n",
      "*** AttributeError: 'SequentialRNN' object has no attribute 'enc'\n",
      "(Pdb) model[0]\n",
      "RNNCore(\n",
      "  (encoder): Embedding(33279, 20, padding_idx=0)\n",
      "  (dp_encoder): EmbeddingDropout(\n",
      "    (emb): Embedding(33279, 20, padding_idx=0)\n",
      "  )\n",
      "  (rnns): ModuleList(\n",
      "    (0): LSTM(20, 100)\n",
      "    (1): LSTM(100, 20)\n",
      "  )\n",
      "  (dropouti): RNNDropout()\n",
      "  (dropouths): ModuleList(\n",
      "    (0): RNNDropout()\n",
      "    (1): RNNDropout()\n",
      "  )\n",
      ")\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-f25d71b354e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004a.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_fn, opt, data, callbacks, metrics)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, xb, yb, loss_fn, opt, cb_handler, metrics)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, xb, yb, loss_fn, opt, cb_handler, metrics)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai1\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai1\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cb = RNNTrainer(learn, bptt, alpha=2, beta=1)\n",
    "learn.opt_fn = partial(optim.Adam)\n",
    "learn.fit(1, 0.1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
