{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed precision training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module allows the forward and backward passes of your neural net to be done in fp16 (also known as *half precision*). This is particularly important if you have an NVIDIA GPU with [tensor cores](https://www.nvidia.com/en-us/data-center/tensorcore/), since it can speed up your training by 200% or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from fastai.gen_doc.nbdoc import *\n",
    "from fastai.callbacks.fp16 import * \n",
    "from fastai.docs import *\n",
    "from fastai import *\n",
    "from fastai.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## <a id=MixedPrecision></a>`class` `MixedPrecision`\n",
       "> `MixedPrecision`(`learn`:[`Learner`](/basic_train.html#Learner), `loss_scale`:`float`=`512.0`, `flat_master`:`bool`=`False`) :: [`Callback`](/callback.html#Callback)\n",
       "\n",
       "\n",
       "Callback that handles mixed-precision training <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L56\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that accuracy is not impacted when using fp16, there are a few details that need to be handled - The [[`MixedPrecision`](/callbacks.fp16.html#MixedPrecision)](/callbacks.fp16.html#MixedPrecision) callback handles those details for you. See below for details.\n",
    "\n",
    "Instead of constructing this callback and passing it to a [`Learner`](/basic_train.html#Learner), it's generally easier to just call [`Learner.to_fp16`](/train.html#to_fp16), which modifies the existing object to add [[`MixedPrecision`](/callbacks.fp16.html#MixedPrecision)](/callbacks.fp16.html#MixedPrecision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=to_fp16></a>`to_fp16`\n",
       "> `to_fp16`(`learn`:[`Learner`](/basic_train.html#Learner), `loss_scale`:`float`=`512.0`, `flat_master`:`bool`=`False`) -> [`Learner`](/basic_train.html#Learner)\n",
       "\n",
       "\n",
       "Transforms the learner in FP16 precision <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L26\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Learner.to_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  vaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:05\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      0.104780    0.063669    0.977429  (00:05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn = ConvLearner(get_mnist(), tvm.resnet18, metrics=accuracy).to_fp16()\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about mixed precision training are available in [NVIDIA's documentation](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html). We will just summarize the basics here.\n",
    "\n",
    "The only parameter you may want to tweak is `loss_scale`. This is used to scale the loss up, so that it doesn't underflow fp16, leading to loss of accuracy (this is reversed for the final gradient calculation after converting back to fp32). Generally the default `512` works well, however. You can also enable or disable the flattening of the master parameter tensor with `flat_master=True`, however in our testing the different is negligible.\n",
    "\n",
    "Internally, the callback ensures that all model parameters (except batchnorm layers, which require fp32) are converted to fp16, and an fp32 copy is also saved. The fp32 copy (the `master` parameters) is what is used for actually updating with the optimizer; the fp16 parameters are used for calculating gradients. This helps avoid underflow with small learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback implementation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to call these yourself - they're called by the callback framework automatically. They're just documented here so you can see exactly what the callback is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_backward_begin></a>`on_backward_begin`\n",
       "> `on_backward_begin`(`last_loss`:`Rank0Tensor`, `kwargs`:`Any`) -> `Rank0Tensor`\n",
       "\n",
       "\n",
       "Scale gradients up by `loss_scale` to prevent underflow <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L90\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision.on_backward_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_backward_end></a>`on_backward_end`\n",
       "> `on_backward_end`(`kwargs`:`Any`)\n",
       "\n",
       "\n",
       "Convert the gradients back to FP32 and divide them by the scale. <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L95\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision.on_backward_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_loss_begin></a>`on_loss_begin`\n",
       "> `on_loss_begin`(`last_output`:`Tensor`, `kwargs`:`Any`) -> `Tensor`\n",
       "\n",
       "\n",
       "Converts half precision output to FP32 to avoid reduction overflow. <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L86\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision.on_loss_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_step_end></a>`on_step_end`\n",
       "> `on_step_end`(`kwargs`:`Any`)\n",
       "\n",
       "\n",
       "Update the params from master to model and zero grad <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L101\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision.on_step_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_train_begin></a>`on_train_begin`\n",
       "> `on_train_begin`(`kwargs`:`Any`)\n",
       "\n",
       "\n",
       "Ensures everything is in half precision mode <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L63\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision.on_train_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### <a id=on_train_end></a>`on_train_end`\n",
       "> `on_train_end`(`kwargs`:`Any`)\n",
       "\n",
       "\n",
       "Removes half precision transforms added at `on_train_begin` <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L80\">[source]</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(MixedPrecision.on_train_end)"
   ]
  }
 ],
 "metadata": {
  "jekyll": {
   "summary": "Callback support for half precision (fp16) training. Increases training speed.",
   "title": "callbacks.fp16"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
