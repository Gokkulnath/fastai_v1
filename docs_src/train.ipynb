{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Additional training functions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[`train`](/train.html#train) provides a number of extension methods that are added to [`Learner`](/basic_train.html#Learner) (see below for a list and details), along with three simple callbacks:\n\n- [`ShowGraph`](/train.html#ShowGraph)\n- [`GradientClipping`](/train.html#GradientClipping)\n- [`BnFreeze`](/train.html#BnFreeze)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": "from fastai.gen_doc.nbdoc import *\nfrom fastai.train import *\nfrom fastai.docs import *"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## [`Learner`](/basic_train.html#Learner) extension methods"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "These methods are automatically added to all [`Learner`](/basic_train.html#Learner) objects created after importing this module. They provide convenient access to a number of callbacks, without requiring them to be manually created."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=fit_one_cycle></a>`fit_one_cycle`\n`fit_one_cycle`(`learn`:[`Learner`](/basic_train.html#Learner), `cyc_len`:`int`, `max_lr`:`Union`\\[`float`, `Collection`\\[`float`\\], `slice`\\]=`slice(None, 0.003, None)`, `moms`:`Tuple`\\[`float`, `float`\\]=`(0.95, 0.85)`, `div_factor`:`float`=`25.0`, `pct_start`:`float`=`0.3`, `wd`:`float`=`None`, `kwargs`)\n\n\nFits a model following the 1cycle policy <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L11\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(fit_one_cycle)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Fit a model with 1cycle training. See [`OneCycleScheduler`](/callbacks.one_cycle.html#OneCycleScheduler) for details."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=lr_find></a>`lr_find`\n`lr_find`(`learn`:[`Learner`](/basic_train.html#Learner), `start_lr`:`float`=`1e-05`, `end_lr`:`float`=`10`, `num_it`:`int`=`100`, `kwargs`:`Any`)\n\n\nExplore lr from `start_lr` to `end_lr` over `num_it` iterations of `learn` <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L20\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(lr_find)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "See [`LRFinder`](/callbacks.lr_finder.html#LRFinder) for details."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=to_fp16></a>`to_fp16`\n`to_fp16`(`learn`:[`Learner`](/basic_train.html#Learner), `loss_scale`:`float`=`512.0`, `flat_master`:`bool`=`False`) -> [`Learner`](/basic_train.html#Learner)\n\n\nTransforms the learner in FP16 precision <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L26\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(to_fp16)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "See [`MixedPrecision`](/callbacks.fp16.html#MixedPrecision) for details."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "## <a id=ShowGraph></a>class `ShowGraph`\n`ShowGraph`(`learn`:[`Learner`](/basic_train.html#Learner)) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n\n\nUpdates a graph of learner stats and metrics after each epoch <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L44\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(ShowGraph)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "data = get_mnist()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy, callback_fns=ShowGraph)\nlearn.fit(3)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "![Training graph](imgs/train_graph.gif)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Internal callback implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_epoch_end></a>`on_epoch_end`\n`on_epoch_end`(`n_epochs`:`int`, `last_metrics`:`Collection`[`Union`[`Tensor`, `Number`]], `kwargs`) -> `bool`\n\n\nIf we have metrics plot them in our pbar graph <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L46\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(ShowGraph.on_epoch_end)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "## <a id=GradientClipping></a>class `GradientClipping`\n`GradientClipping`(`learn`:[`Learner`](/basic_train.html#Learner), `clip`:`float`) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n\n\nTo do gradient clipping during training. <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L64\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(GradientClipping)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Clips gradient at a maximum absolute value of `clip` during training. For instance:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  va…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total time: 00:05\nepoch  train loss  valid loss  accuracy\n0      0.080691    0.043826    0.986261  (00:05)\n\n"
    }
   ],
   "source": "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy,\n    callback_fns=partial(GradientClipping, clip=0.1))\nlearn.fit(1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Internal callback implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_backward_end></a>`on_backward_end`\n`on_backward_end`(`kwargs`)\n\n\nCalled after backprop but before optimizer step. Useful for true weight decay in AdamW <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L68\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(GradientClipping.on_backward_end)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "## <a id=BnFreeze></a>class `BnFreeze`\n`BnFreeze`(`learn`:[`Learner`](/basic_train.html#Learner)) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n\n\nFreezes moving average statistics in all batchnorm layers <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L57\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(BnFreeze)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For batchnorm layers where `requires_grad==False`, you generally don't want to update their moving average statistics, in order to avoid the model's statistics getting out of sync with its pre-trained weights. You can add this callback to automate this freezing of statistics (internally, it calls `eval` on these layers)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  va…"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total time: 00:05\nepoch  train loss  valid loss  accuracy\n0      0.085482    0.052628    0.984789  (00:05)\n\n"
    }
   ],
   "source": "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy, callback_fns=BnFreeze)\nlearn.fit(1)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Internal callback implementation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": "#### <a id=on_epoch_begin></a>`on_epoch_begin`\n`on_epoch_begin`(`kwargs`:`Any`)\n\n\nPut bn layers in eval mode on epoch_begin <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L59\">[source]</a>",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": "show_doc(BnFreeze.on_epoch_begin)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Undocumented Methods - Methods moved below this line will intentionally be hidden"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": "show_doc(one_cycle_scheduler)"
  }
 ],
 "metadata": {
  "jekyll": {
   "summary": "Provides advanced training extensions to `fastai.basic_train`. Includes half-precision, learning rate finder, mixup, and one-cycle",
   "title": "train"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
